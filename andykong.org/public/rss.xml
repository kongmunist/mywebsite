<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Andy Kong's Blog</title>
    <link>https://andykong.org</link>
    <description>Thoughts and Work</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 14 Mar 2025 07:32:19 +0000</lastBuildDate>
    <item>
      <title>First Post!</title>
      <link>blog/first</link>
      <description>WHAT'S GOING ON I'M MAKING A WEBSITE</description>
      <content:encoded>**WHAT'S GOING ON I'M MAKING A WEBSITE**

</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 24 Jun 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>The exploration of unexplorable search spaces</title>
      <link>blog/unexplorable</link>
      <description>Any hard problem can be solved by randomly guessing — if you're good enough at guessing.</description>
      <content:encoded>Any hard problem can be solved by randomly guessing — if you're good enough at guessing.

Take my website's aesthetic. The colors composing my website were not chosen at random. I went on a nice site called [colourlovers.com](https://www.colourlovers.com/) where people discover and post colors they think are beautiful and I found a curated selection of the best colors out of the 16.7 million RGB ones. Not only that, they were already matched in a palette of 4 complementary colors! Thank god for these color-space explorers, without them my site would be a garish combination of 4 colors I liked individually, but put together look like an alternative Mardi Gras parade.


&lt;img src={{ url_for('static', filename = 'dreammagnet.png')}} width="450" class="d-block mx-auto"/&gt;
&lt;p class="caption"&gt;[Dream Magnet](https://www.colourlovers.com/palette/482774/dream_magnet) has one of the prettiest cyans I have ever seen&lt;/p&gt;


That incident got me thinking. I think it is amazingly frustrating that many problems have solutions that can be guessed. In any field, almost all problems can be randomly, instantaneously solved to a greater extent than methodical approaches are currently solving them. This is because we know what type of thing we're guessing (integers from 1-60), just not what the right guesses are.

In the field of personal finance, I could correctly guess 4 or 5 numbers and win the lottery. The solution space for this one isn't even that big, only 60&lt;sup&gt;5&lt;/sup&gt;=777 million (I wonder if the lucky numbers are intentional), but the importance is several orders of magnitude higher than picking a color scheme for my website while only being 1 order of magnitude harder to find.  

Or take machine learning for instance. Nowadays, modern research labs spent millions training neural networks with their fancy computers. With the invention of [backpropagation](http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf) by Geoffrey Hinton in 1986, every school and company dropped anything computationally challenging they were working on (re: not webdev or IT) to figure out the fastest way to do matrix math really, really fast. The faster your model could multiply huge matrices of floating-point numbers together, the better/faster/stronger it could tell a [dog from a cat](https://www.kaggle.com/c/dogs-vs-cats) or [classify a 32x32 pixel image](http://www.image-net.org/challenges/LSVRC/) as a airplane, bird, etc.
 
Backpropagation left the machine learning community a method for tuning the matrices. As any high school/college kid interested in machine learning knows, the really hard part of machine learning that Geoffrey Hinton didn't solve is &lt;del&gt;automagically importing data&lt;/del&gt; twiddling the matrix numbers to perfection sometime before the heat death of the universe. THAT'S why my neighbor at my first-year college dorm had multiple high-end graphics cards, not to play the Overwatch in 4K at 144Hz while their model is training. My jealousy of their dual 144Hz monitor gaming setup aside, finding the minimum in higher dimensions is HARD. 

&lt;img src="https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png" class="mx-auto d-block" width = 300px style=""/&gt;

&lt;p class="caption"&gt;You think this is bad? Imagine how many bumps it has in the 500th dimension!!&lt;/p&gt;

Confronted with all this complexity from today's approach, I could just give up. Like an Amazon warehouse stocking inventory, I could just begin to guess values and randomly shove them into the matrices anywhere they fit. And, in one world out of many I will achieve a miracle: I will find the global minimum by raw chance. Now, there's no academic clout to be gained this way, and I wouldn't know how my matrices worked. But neither does any other ML researcher. 

Then take a look at medical research. To find new medicines today, we just add random functional groups to an existing, working drug. To check if doing something random made the medicine more effective somehow, we give it to a bunch of rats, and if it works for them, a bunch of monkeys, and if it works for them, the humans that need it. The process takes millions of dollars and lots of time, and in the end we still don't know how the new drug works until some PhD student figures out the mechanism of action 20 years later. 

Why does it matter? It's because I think it's sad that the lives of many experts today will be dedicated to finding the best way to guess at random numbers, with no need to understand *how it all works* once they've done it.</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 30 Jun 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>The miracle of brain-computer interfaces</title>
      <link>blog/extraarms</link>
      <description>How BCIs can give us more arms</description>
      <content:encoded>Bear with me. I'm going to try to explain something slightly abstract but incredibly interesting. 

## You, the brain reading this

Look at your hands. Admire your reflection in a mirror. It's you, right?

Wrong. That is not you. You are the brain inside that person. You are a chunk of meat piloting a bone-armored meatsuit, and that meatsuit's hands and feet are the only ways you can interact with the physical world. It is kinda weird to think of our limbs as tools. Our hands act as an extension of our brain into the physical world, a way for our abstract thoughts to influence the concrete world. And it's our best tool because it's so generalizable; I'm currently using the fingers on mine to hit my keyboard in specific combinations to write the words you're reading. 

However, their generality leads us to create more specialized tools to make certain tasks easier for us. Hammers allow us to piece together pieces of wood with bits of metal. Pianos make it easy to create specific sounds reliably. Keyboards enable precise electron changes on tiny, tiny silicon chips. We use our general tools to create and harness more specialized ones. 

But there's a problem. Whenever we use a tool, we are actually trading use of our hands or feet for use of a more specialized tool. Even if we know exactly how we want to use the tool, we have to figure out how to best interact with it. This is an added learning curve called muscle memory.

This might not seem like a big deal to most people, but that's because most people have working hands or feet. They have general tools with which to harness specialized ones. But what if you're already using both your hands and just need another one, like a surgeon in a 36 hour surgery who just needs a nurse to hold a piece of skin out of the way? What if you've lost the ability to control your limbs in bike accident, or have terribly trembling hands from Parkinson's as a result of a random mutation? These people can't harness any tools! They're out of luck in today's technology.

&lt;img src={{ url_for('static', filename = 'extraarmeeg.jpg')}} width="450" class="d-block mx-auto"/&gt;
&lt;p class="caption"&gt;&lt;/p&gt;


## Not just a better keyboard

This is where brain-computer interfaces completely dominate current, existing interfaces. While a joystick might allow us to replace our arms with a more powerful and steady robotic arm, a brain-computer interface allows us to use a robotic arm *in addition to the two incredibly versatile arms we already have*. BCIs aren't just better keyboards; they're the only tool that doesn't require us to give up one of our general tools to use a more specific one. 

I'm not saying BCIs will allow us all to be Doc Ock, or that they'll cure disability forever. There'll still be a learning curve, just like for learning to type. But inputs can now occur at the speed of thought, instead of being limited at the speed at which we can move our fingers. We can connect our brains and interact directly with the world around us, instead of having to go through our muscles as middlemen. 

## What it means for us

When computers first became commercially available in the 1960s, they revolutionized the way we did work. Calculations could be done instantly; large spreadsheets of data could be manipulated at scale. But today, despite all the advances we've made in computing, we do work at the same rate as a worker in the 1960s. We're limited fundamentally by the speed of our interface, the keyboard and mouse.

BCIs are currently still in the early stages of development, and won't be feasible for at least another 10 years. But with a working brain-computer interface, we'll open an entirely new field of ways to connect our brains to the world around us. It's time for a new interface. 


&lt;br&gt;&lt;br&gt;

in the bent get get

</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 28 Jul 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Parts arrived for Altered Perceptions</title>
      <link>blog/alteredperceptions0</link>
      <description>How would you like to see the world from a new pair of eyes?</description>
      <content:encoded>Hello. This is Andy. I'll tell you about my new project, and why I need your help coming up with ideas for it. 

Ever since I was young, I have wanted to be able to see the world a bit differently. I'm not sure if I had gotten bored of the way everything looked, or maybe because I knew that I wanted to see old things in a new light. Whether wishing I could change my reds for blues after hearing about "everyone sees colors differently" theory, or wishing I had a real life heads-up display to show me peoples names and subconscious emotions, I always wanted my eyes to be a bit more effective than they were. 

Soon, I hope to make this a reality. Today I received a few boxes in the mail, containing a PlayStation VR, Jetson Nano, Raspicam, and some assorted cables. Can you tell where this is going? I'm going to run the PSVR from the Nano, and wear the Raspicam on the front of the headset to have a POV livestream that I can intercept and play with before sending to the wearer's eyes. The latency might be bad, and the video stream may not trick me into believing its "live live", but it'll emulate a childhood desire of mine. To see things differently with the press of a button. 

We rely so heavily on our eyes for almost everything. Balance, trajectory planning, social interaction, building cool toys, playing video games. The list goes on and on. And soon, I will be able to have more than one pair. Say I wanted to up the contrast of the world, or overlay line detection over my view? Done in a few lines of code. What about the Minecraft nausea effect applied in real time, or even better, Deep Dreaming every frame I see? Easy enough given the level of control over the data that's available. For the first time, I will have granular control over how I see things. It will be glorious.

## If you find this project interesting and/or think that there's some neat way to edit the visual field that I've missed, don't hesitate to email me to let me know what your idea is, and I'll try to implement it (andykongresearch@gmail.com).
&lt;br&gt;&lt;br&gt;

&lt;ul&gt;Filters/new eyes I'm trying to implement
	&lt;li&gt;rainbow cycling the colors&lt;/li&gt;
	&lt;li&gt;inverting the world&lt;/li&gt;
	&lt;li&gt;deepdreaming, live (like this video):&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/DgPaCWJL7XI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 13 Nov 2019 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Sledding on a table</title>
      <link>blog/sleddingontable</link>
      <description>We had no sled, but we did have an extra folding table</description>
      <content:encoded>First sticking snow in Pittsburgh today! Me and Elio and Jona wanted to sled, but couldn't find a sled equivalent around the house. Cardboard too soggy and small, container lids too flimsy and weak, no detachable thing on those trashcans anymore :(. What to do? We looked around and found this folding table we had!

![Place your bets as to whether this table makes a good sled]({{ url_for('static', filename = 'tableforsledding.JPG')}})
&lt;p class="caption"&gt;Place your bets as to whether this table makes a good sled!&lt;/p&gt;

It works! Video on my instagram [here](https://www.instagram.com/p/CIR1Gn6nJ6u/). Not super well since it's so heavy, but you can lift the front and sort of glide down a hill over the ice instead of just plowing into it. Pretty good!


</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 01 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>The first generation of handheld barcode scanners had laser tubes</title>
      <link>blog/barcodehistory</link>
      <description>History of the LS1000 and MH290, and a demo of the 2nd!</description>
      <content:encoded>Hi! Today we have a history lesson! I'm going to tell you the history of the handheld barcode scanner, and how to use one if you have one lying around. 

## History
All but ubiquitous today, the first barcode scanners were stationary and built into the checkout counter. This made it difficult to scan large items, which couldn't easily be passed above the desk. Though laser diodes were invented shortly after the laser (1962, 1960), they were not mass produced until the late 1980s, so these early barcode scanners had to make do with laser tubes — big, fragile, bulky things that are usually around a foot long. So how did they make them handheld?

&lt;p class="caption"&gt;Original LS1000 barcode scanner versus modern day barcode scanners. The laser tube in the LS1000 is horizontal and takes up most of the length, while the laser diode powering the 2nd is smaller than your pinky.&lt;/p&gt;
![Original LS1000 barcode scanner versus modern day barcode scanners. The laser tube in the LS1000 is horizontal and takes up most of the length, while the laser diode powering the 2nd is smaller than your pinky.]({{ url_for('static', filename='barcodescannerls1000.jpg')}})

With Symbol Technologies, Dr. Jerry Swartz performed calculations in the late 70s and believed that the laser tubes could be miniaturized to 5-6 inches, which were short enough to fit into a handheld scanner. The first company he asked to manufacture them laughed him out of the room, but the second company, Uniphase, believed it could be done. Uniphase later overtook that first company, Spectra-Physics, as the largest manufacture of HeNe laser tubes. The barcode scanner that they created with the shortened laser tube was the LS1000, pictured above. 

Metrologic followed shortly after and released their handheld barcode scanner. They mounted the laser tube along the handle instead along the top shaft, and achieved a much more compact size. 

&lt;p class="caption"&gt;The Metrologic 290 handheld barcode scanner. The laser tube is hidden in the handle&lt;/p&gt;
![The Metrologic 290 handheld barcode scanner. The laser tube is hidden in the handle]({{ url_for('static', filename='mh290.jpg')}})

After some fundamental problems of room-temperature lasing and stable output were solved, Japan began mass-manufacture of compact laser diodes in the early 80s. Symbol began incorporating these diodes into their products, producing much smaller barcode scanners due to the lack of massive HeNe tube to produce the laser. These lead to the kinds of barcode scanners today, pictured above next to the LS1000.

## Where do you find them now?
Today, these early barcode scanners are hard to find and fairly expensive when you do find them. They're all pretty bulky due to the laser tube, and fragile because of it. Nobody manufactures them anymore, so you'll have to look on resale markets — usually you can find one for $40-$100 on eBay. You can tell it has a tube because of the size — smaller, compact models can't possibly house the 5 inch laser tube that the earlier barcode scanners housed. 

In the past, I found a bulk sale of 17 MH290s and purchased them, and am currently reselling in individual quantities on eBay [here](https://www.ebay.com/itm/Metrologic-MH290-Barcode-Scanner-HeNe-Laser-inside/363080135269). I think everyone who buys old barcode scanners also knows there's a laser tube inside them, because why else would you be buying such an old barcode scanner? It works the same as a new barcode scanner, just bigger and dirtier. 

## What do you do with them?
I guess you could open a vintage grocery if you really wanted, but I wanted the laser tube from within! While reading Sam's Laser FAQ, I found out about the MH290 for the first time and that's what started this whole obsession.

&lt;p class="caption"&gt;The laser tube from the MH290 lasing.&lt;/p&gt;
![The laser tube from the MH290 lasing.]({{ url_for('static', filename='barcodescannerlaseron.png')}})

Theoretically, [this guide](http://www.repairfaq.org/sam/sale/henemll1.htm) for the MH290 from Sam's Laser FAQ tells you everything you need to know. Practically there's a few considerations you need to think about, like the current draw being a few amp. A 15V wall adaptor will work, and only requires a bit of soldering to get a wire to trigger the PWM-on pin on the DIP IC that drives the laser. If you use an weak power supply or wall brick which can't supply enough current, you'll notice a slight flickering and buzzing to the laser which makes it look unstable but also *cool* at the same time.

This is the first HeNe laser I've owned, and it's probably useful for science. Let me know if you know of anything cool I can do with one!

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;hr&gt;
### References
[1] [Sam's Repair FAQ guide to powering on MH290](http://www.repairfaq.org/sam/sale/henemll1.htm)

[2] [Scholarpedia entry for bar code scanning](http://www.scholarpedia.org/article/Bar_code_scanning)

</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 14 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>How Trees Fall Apart</title>
      <link>blog/howtreesfallapart</link>
      <description>The differing decay patterns of trees during fall</description>
      <content:encoded>You only get to experience 1 fall a year, and by the time you read this you're probably older than 15. This means you probably have only 65 falls left in you — remember to do your best to enjoy each one. 

Anyway, this particular fall I took more of a look at the trees around me. I noticed that the green-yellow-red transition of leaves followed different patterns for different trees, and took photos of all the ones I could. Here follows the decay patterns of 4 different trees.

&lt;hr&gt;

# 1. Top down
&lt;p class="caption"&gt;The decay here happens from the highest leaves to the lowest. &lt;/p&gt;
![The decay here happens from the highest leaves to the lowest. My favorite tree this year]({{ url_for('static', filename = 'topdown2.jpg')}})

# 2. Inside out
&lt;p class="caption"&gt;Core leaves brown completely while the outer ones are still green&lt;/p&gt;
![Core leaves brown completely while the outer ones are still green]({{ url_for('static', filename = 'insideout.jpg')}})

# 3. Outside in
&lt;p class="caption"&gt;My favorite tree of 2019 and 2020. The outermost leaves redden before the core&lt;/p&gt;
![My favorite tree of 2019 and 2020. The outermost leaves redden before the core]({{ url_for('static', filename = 'outside_in1.jpg')}})



# 4. All at once
&lt;p class="caption"&gt;No regard for order! All leaves turn red at the same time, perfect example of communism&lt;/p&gt;
![No regard for order! All leaves turn red at the same time, perfect example of communism]({{ url_for('static', filename = 'allatonce.jpg')}})


Evolutionarily, I'm not sure the decaying has a rhyme or reason. Maybe it makes sense to keep the outer ones green since they receive the most sunlight, but maybe it doesn't matter because red leaves can collect energy as well. Whatever the reason, it makes a beautiful sight. 


</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 13 Nov 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Examples from the Taxonomy of Automation</title>
      <link>blog/taxonomyofautomation</link>
      <description>Summary and thoughts on the convenience of interfaces</description>
      <content:encoded>Hello y'all. I've been thinking about automation, specifically, intuitive interaction design's role in coddling people. 

My lab is called the Future Interfaces Group. We abuse hardware and software to make predictive interfaces and new interactions between users and technology. Every project needs a user study to verify usability or reliability — does the user like the interaction? How does the device compare to other ones like it? We think about these things to make devices that hopefully feel a bit magical, like they can tell what you're going to do before you do them.

I've been thinking about the usefulness of such research, especially since it doesn't feel like anyone's life is particularly impacted by the research we put out. While it's nice that the products we use are smooth and not frustrating, I sometimes feel that nobody would die or hate their lives if we took it all away. Do these things really build up into useful parts of someone's life? Or is it always just cruft, extra features and bloatware that nobody asked for? 

I'm getting off topic. Recently, I read a Tumblr blog called Crap Futures (I'm gonna abbreviate it as CF) which shared many of my thoughts about the apparent shittiness of the future we're creating. IoT devices everywhere that go down every week due to ransomware attacks or server outages does not sound like my idea of a good tech future. 

They wrote one particular post called [Scratch an itch: A taxonomy of automation](https://crapfutures.tumblr.com/post/180304398284/scratch-an-itch-a-taxonomy-of-automation) that really got me thinking about the degrees of automation in our lives. I'm going to summarize it here, but you should definitely give it a read too to see where I'm getting these ideas from. 

# Taxonomy of Automation
I see automation's actors as the human and the device/tech/automator, and I am going to explain CF's taxonomy of automation in terms of the actions: who does the sensing, and who does the action. And I liked CF's concrete example of scratching an itch, so I'll continue with that as my main focus

&lt;br&gt;

## Level 1: Human Sensing, Human Doing
On automation level 1, the human feels the itch and then reaches over to scratch it

## Level 2: Human Sensing, Device Doing
The human senses the itch, and uses a device to scratch it. This can be a stick or an ItchScratcher 3000.

## Level 3: Device Sensing, Device Doing
The device senses our itch (from imagery or something, use your imagination), then scratches it for us. 

## Level 4: Device Prediction
The device anticipates an itch, perhaps it occurs on a regular basis or shows a red spot before actually itching. The desire to scratch is circumvented entirely, through early anti-itch cream or pre-scratching. 

## Level 5: Device Omniscience
The device anticipates and even pre-supposes an itch. It can do this to sell repairs of itself, or to sell its own usefulness. Complete loss of desire control. 

&lt;br&gt;

# Examples
I've mentioned this to a few friends, and each one has thought of a few examples of devices at each level that we already use today. As we'll see, nearly everything sits on level 1 and 2.

At level 1, we do all the work that we've always done by hand. Scratching, massaging, feeding ourselves.

Level 2 contains all "dumb" tools - the shovel, the spatula, the TV remote. We sense a boring channel coming on and click the remote to change it. Most of our technology sits at level 2, including our phones and the Roomba.

Level 3 largely drops off and contains almost nothing because level 3 features a loss of autonomy. Most devices stop short of that. Most of these require user confirmation, but I'd say that's pretty close to just letting the device do it, we just don't trust them enough. 

- Our email client detects dates and offers to put them on a calendar, but it knows the limits of its own accuracy and doesn't create events on its own. 

- Auto-sharing wifi passwords and detecting lost devices are both features on this level. 

- GitHub Copilot also falls on this level, but it isn't quite good enough to be trusted.

And nothing falls beyond that. It's kind of sad that so few things live at Level 3, even given all our crazy machine learning advances in the past twenty years. Our devices stay tools, useful when we use them and not otherwise. 

&lt;br&gt;

# Cross-application to people
One thing that came up in my discussions is if anything sits past Level 4. I think our friends and family fall past level 3, since they're always looking out for us and can anticipate what we want pretty well (just think of your recent Christmas gifts!). And people besides them can sit anywhere on the spectrum. We can do a task ourselves (us sensing, us doing), tell someone what to do (us sensing, them doing), or tell someone what high-level thing to be working on (them sensing, them doing). Ideally they sit as high as possible on the levels. 

&lt;br&gt;

# Conclusion
I think we should be pushing robots as far up the automation hierarchy as possible. Each layer saves exponentially more time. The question is if the idea being automated is worth doing yourself, and if it is, why automate it? I close with a quote from the little prince

&lt;p class="caption"&gt;The Merchant passage from The Little Prince&lt;/p&gt;
![The Merchant passage from The Little Prince]({{ url_for('static', filename = 'tlpmerchant.png')}})

A question for later: Why do we value human level 3 much more than robot level 3? (I think it's related to the fact that we know humans have opportunity cost but don't really think about computers having the same). What if we reported robot operating cost whenever they did a task for us? 

Until then, cya around!




</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 03 Jan 2022 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Calendar Memories</title>
      <link>blog/calendarmemories</link>
      <description>Personalized holidays in my calendar</description>
      <content:encoded>When I first began my life, time was nothing to me. I did not notice it passing, nor did I care. I lived in the world of events, and as stuff happened, I noticed the stuff without caring what time it happened. I was incredible at reading analog clocks when I was in preschool, but my parents worried about the time on my behalf, delivering me to soccer practice or school whenever they started. 

As I got older, specific times took on meaning. Classes starting and ending times, lunchtime, and school letting out were all hard-coded in my head, and I would think about them every time I glanced at the time. Over time, it only got worse. In college, I started keeping a calendar to track my courses and their locations. If a clock is a machine that produces time, a calendar is a factory. It is the ultimate time-keeping tool, containing every minute of your life and your past, theoretically forever in both directions if its electronic. Isn't that incredible? 

You may not think so, but try this. One day while working on [Carnegie Calendar](../../projects/carnegiecalendar), I realized that you could put in ANY date you wanted for an event. 

I scheduled a meeting for April 9th, 2150 that I probably won't be attending. I scrolled on my calendar to make sure the event showed up, and it was there, sitting as innocently as any other calendar event. Then I navigated to my 150th birthday, and that event was there too. I probably won't make it to that either. 

{{ add_pic("cm_150th.png", "") }}

If that's not insane to you, just think about it for a bit. To me, it felt like knowing exactly where I will be buried and visiting the plot of land 50 years in advance.

&lt;hr&gt; 

# Sorry, that was a tangent
I'm actually here to talk to you about a way to create your own holidays.

You know how there are these preset holidays in your calendar that you didn't add, like "Presidents' Day" or "Thanksgiving Day"? We celebrate these events on certain days because these days were instrumental to the country's development, and that's cool. 

But to me, these events are kinda just taking up space — I didn't make them, and I might not celebrate them. What I find much more interesting to commemorate are the events that were instrumental to my life and my development. Stuff like "First time I got stitches" or "Failed midterm" that I can look back on and smile because they happened. 

{{ add_pic("cm_firststiches.png", "") }}

Now, whenever I feel like I've experienced something life-changing, I put it on my calendar and set it to repeat annually. I've started added the year too, just so I can gauge how long it's been. 

## In the far-future...
I know that if I continue this, my calendar will eventually be filled with ghosts — past iterations of myself experiencing my own history. My routine weekly work meetings will clamor with "First stiches" for space on my screen. Birthdays of people I don't remember will populate the top bar. But these are the holidays I have chosen for myself, and to me, they are worth remembering.

Give it a go!</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 09 Apr 2023 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>iCloud Cleanup</title>
      <link>blog/icloudconfusion</link>
      <description>Script for highlighting large videos in iCloud and a storage discrepancy</description>
      <content:encoded>Hello!

Recently I received an email from Apple letting me know that my iCloud storage was full. 

{{ add_pic("ic_appleemail.jpg", "Kind letter from Apple requesting I purchase more storage") }}

Since the cost of 200GB vs. 1TB jumps more than threefold, I embarked on a great exploration of alternatives. Since photos made up the majority of my storage (~127GB), I figured I could just go into my Photos album and click Sort By File Size.

...

Except this is not a feature offered by Apple. It's not even a feature in the iCloud browser. I guess it's a bit too technical for Apple Photos, and unfortunately it would also make it too easy to avoid paying for more iCloud storage. 

Whatever the reason, I still wanted to go through my photos by file size. Several apps exist for going through your photos and highlighting duplicates, or into your videos and showing you the file size. The only problem — photos that have been offloaded to iCloud do not show up in these apps, so they're not actually super useful. Also, would you really want random apps to scan through all your photos? 


{{ add_pic("ic_photosbysize.png", "Screenshot from a photo file size viewing app, with non-downloaded photos represented as 0B") }}

## Janky JS solution

Instead, I went to the iCloud website and thought about what was possible. I navigated to Photos-&gt;Media Types-&gt;Videos, and zoomed out as far as I could. 

{{ add_pic("ic_icloudphotos.png", "Videos anonymized by slow internet loading") }}

We can see that each video has an accompanying duration box — as long as it's an HTML element, we can use JS to search and filter them. I found that every runtime box has the class `video-text-badge`. From there is was a simple matter to find all of them in the page, sort by their duration, and highlight the ones that were past some threshold. Here's the code:

&lt;script src="https://gist.github.com/kongmunist/a598bcdd8c226c3a3159b1a918344977.js"&gt;&lt;/script&gt;

Because iCloud only loads the elements that are on the page, I've made this into a function that runs on a timer so new elements get highlighted as they get scrolled into. Here's what it looks like:

{{ add_pic("ic_icloudphotoshighlighted.png", "Videos bigger than 20s are surrounded by a red box, making them much easy to pick out") }}

To use it, just open the Javascript console (right click page -&gt; Inspect Element) and paste in the entire gist. Now you can easily select multiple big videos from iCloud and download them before deleting, moving them into longer-term storage: secret HDD under your mattress, other cloud storage, etc etc. 

&lt;hr&gt;

# The Mystery
So, I used this script to remove all my iCloud videos &gt;30s. The interesting thing is, after I had removed all the "big videos" and downloaded them, it cleared ~55GB from my iCloud *despite only downloading 7GB of videos.* Herein lies the mystery.

{{ add_pic("ic_dl1info.png", "All downloaded videos take up 8GB of disk space") }}

{{ add_pic("ic_dl1afterdelete.png", "iCloud storage reduces from 199GB to 143GB after downloading 7GB of videos") }}

Somehow, those 7GB of video took up way more space in the cloud than on my hard drive. Interesting...

# Experiment 1

I wanted to test this further. First, I uploaded a 4K video with a lot of motion. This took up 281 MB. My storage looked like this after uploading it:

{{ add_pic("ic_dl2storagebefore.png", "4K video uploaded, iCloud says 145.33 GB used") }}

Then I downloaded it and deleted it. The file was still 281 MB. Here is the storage afterwards:

{{ add_pic("ic_dl2storageafter.png", "4K video deleted, iCloud says 145.6 GB used") }}

Removing a 281MB video frees up ~270MB. This adds up, which is puzzling. What about the other, older videos? 

# Experiment 2
I thought that maybe older videos could have multiple copies saved in iCloud, so I searched through my videos to see if I could find a shorter one that takes up a lot of storage space. I found one with a lot of graphs, iCloud said it took up 128 MB.

{{ add_pic("ic_dl3icloudinfo.png", "Older big video that takes up 128 MB") }}

When I downloaded it, the file was only 47 MB!

{{ add_pic("ic_dl3download.png", "Downloaded video file is 47 MB") }}

And here is my iCloud storage before and after

{{ add_pic("ic_dl3storagebefore.png", "iCloud storage before deleting the old video, 145.29 GB used") }}
{{ add_pic("ic_dl3storageafter.png", "iCloud storage before deleting the old video, 145.12 GB used, reduction of 170MB") }}

So iCloud says the video is 128MB, I download it and the video is actually 48MB, and my free storage increases by ~170MB when I deleted it. Interesting!

# Conclusion
It's weird that my storage freed up more than 7x the removed files size, and weirder still that old, big videos appear to have a much larger storage footprint in iCloud than in real life. 

I am mildly interested in finding out why this happens, but I am not interested/bored enough to do it myself. If one of you fine people figure it out, please let me know by emailing me.

Anyway, I have freed up &gt;50GB to fill with more inane videos, and written a small script that allows me to do it again in the future. Hope this proves helpful to you, dear reader.

Cya later!</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 06 Jun 2023 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Average clip duration in MILF Manor</title>
      <link>blog/milfpy</link>
      <description>Gauging the audience attention span from TLC dating shows</description>
      <content:encoded>Whenever I'm watching unpretentious TV like sports or reality TV, I calculate the average clip length. I get my phone stopwatch out, and enter a lap whenever a cut happens. 

Do this maybe 20 times, and you usually get some idea of the distribution. Usually the cuts happen so quickly and fluidly that you may notice yourself _missing_ a few cuts, even though every pixel on the screen dramatically changes and it seems like something you'd notice. 

Recently I watched Milf Manor, a "reality" dating show where older women seeking younger men are paired with younger men seeking older women on a fancy island. The twist is that every young man is the child of one of the older women. Hilarity ensues (?). 

{{ add_pic("milf_cutfeature.jpg", "The complicated feature I used for detecting jump cuts") }}

Common decency aside, this show jump-cuts like nothing else. I did my stopwatch thing but didn't want to stop there — I really wanted to know all about the cut length distribution. I downloaded an episode and used the change in standard deviation of the absolute color difference of two adjacent frames. When this number crossed ~30, I called it a cut and recorded the frame number.

Then I calculated a few stats about it. Here they are:

{{ add_pic("milf_stats.png", "Avg clip length is 2.48 sec, with the median at 1.92 sec") }}

While the average clip length is 2.5 seconds, the median is only 2 seconds. I also find it interesting that the average human blink rate is 1 blink per 4-5 seconds, which means we can see around two clips between blinks. Since the average blink takes ~0.3 seconds, we also won't miss much of a clip while blinking.

{{ add_pic("milf_cutdistro.jpg", "Milf Manor clip duration histogram") }}

# Conclusion
The difficulty of identifying a cut in a video surprised me, but it makes sense if you consider all the possible transitions (fade to black, fade to another clip, slo-mo transition to real-time, etc.). There is even a library in Python called [scenedetect](https://pypi.org/project/scenedetect/) which identifies these for you. Next time I will definitely use that for cut detection instead of making my own feature. 

I have completely satisfied my curiousity about Milf Manor at this time. 
</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 11 Jun 2023 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Shipping from Europe to America from $2.30/kg</title>
      <link>blog/packingtips</link>
      <description>How to game cross-continental travel</description>
      <content:encoded>Hello! After my undergrad, I moved from the US to Europe, taking all my earthly possessions. A year later, I have moved back to the US, taking, again, all my earthly possessions. I am extremely susceptible to the [endowment effect](https://en.wikipedia.org/wiki/Endowment_effect) and I'm not rich yet, so I put in a lot of effort and moved back nearly everything I took there and then some without paying much extra — the process took quite a bit of repacking and rethinking, so I'd like to share my insights with you, dear reader. 

{{ add_pic("packingtips/0.png", "Interesting endowment effect factoid, usually people use the analogy that a mug you own is worth much more to you than the value of the mug. I did not manage to take my ThorLabs mug back to America with me :(") }}

This will be mostly targeted at people who carry around a lot of extraneous stuff related to a hobby or maybe a medical condition. I do a bit of electronics and the components and heavy equipment comprise approximately 30kg of my possessions. For instance, these two machines (oscilloscope, function generator + power supply) constitute a basic electronics lab totalling ~15kg, which I brought back from Europe. 

{{ add_pic("packingtips/1.jpg", "") }}

# Packing

Before you start, it helps to mentally sort your stuff by density. For example, clothes are low density, books wood and weirdly shaped knick-knacks are medium, and metal/stone/ceramic stuff is high density. I also knew I would have two checked bags (23kg x 2) and a carry on (18kg in non-American countries) with personal item (no limit), with the total weight &gt;64kg.

While you may think it makes sense to pack densely in the checked bags so you won't have to lug it around, you actually need to save the densest stuff for the carry-on and personal item. This is because they always check the weight of your checked bags but never your carry-on, so it doesn't matter if the carry-on is super heavy. Also, checked bags are bigger, and oftentimes you will have to pack the biggest, fluffiest stuff in there because they do not fit anywhere else.

{{ add_pic("packingtips/2.png", "Burgeoning is a good word for these bags") }}

Not all the clothes go into the checked bags. Anything potentially breakable should be padded with clothes, preferably fluffy items like sweaters or puffers. Think of the luggage like a sushi roll — the valuables are inside, guarded by many layers of cushy clothing. Valuables should also be insulated from knocking into each other within the luggage center.

If you're really tight on space, you'll have to do a bit more optimization, sorting the clothes by density to determine which luggage is padded with what. I packed all the low-medium density stuff in my large bags, padded with the lowest density clothing so it wouldn't go overweight. Higher density clothes like pants and shoes went into the carry-on. This means the carry-on is less protected, but you're not going to throw it around if you're handling it yourself, right?

{{ add_pic("packingtips/3.png", "Folded clothes DO NOT GET SMALLER") }}

When you pack clothes, ditch the rolling and the folding routine. Just think about it — clothes only waste space when they're folded. Flat fabric sheets waste zero volume, but you add empty volume in the crease once you fold it. Fold your clothes as little as possible. Ideally they go into a suitcase that is exactly their shape and lie totally flat, but this is an imperfect world.

# At the airport

Arrive early, your stuff is heavy and you'll move slower accordingly. Oh, and you will get stopped at the security checkpoint — they claimed they could not see through my bag and had to unpack everything to re-scan it, then I had to pack everything again at that little side counter where they make the suspicious people hang out.

{{ add_pic("packingtips/4.jpg", "A couple friends going to the airport") }}

You should also know your bag weight just so there's no head-scratching repacking in the check-in line. Try to bring a friend. A couple came with me to the airport, bringing a small sack in case I needed to shed some cruft. Luckily, both my bags werighed in at 22.8kg/23kg. The staff are usually a bit lenient to overweight bags by about +0.5kg, but I didn't want to rely on the kindness of the Swiss staff.

After checking my two big luggages, I had only my burgeoning backpack and carry-on. Usually nobody cares how big/heavy they are, but once I flew Swiss and a person actually stopped me after the check-in because they said my carry-on bag "looked heavy" — it was, and cost me 60 bucks to check it separately. Avoid people who look like they're patrolling. You must be careful in this crazy world.

# Getting rid of your carry-on

Something that never made sense to me is at the check-in, you can pay money to check your bag, but once you're in the airport and they've filled all their overhead storage, all of a sudden it's free! We can exploit this little glitch to ditch our large suitcase.

At your gate when they start calling out group numbers to start boarding, just don't move. You just keep reading/crocheting/twiddling your thumbs, whatever you were doing before. You have assigned seats anyway.

When the line of fellow passengers diminishes to nobody, go check in. We've now saved ~30 minutes of just standing in line, and usually those early riders will have used up all the storage. If this is the case. the staff will take one look at your hulking, totally-regulation carry-on and check it for free. Voila, now you just have to get you and your personal item to your destination — all your earthly possessions will follow along your whole journey without you needing to worry about a thing.

# Peace of mind

Ah, you don't trust the airport staff with all your earthly possessions? Yea, they do have a [tendency to misplace luggage ~1% of the time](https://thepointsguy.com/news/lost-baggage-report-2022/). I always keep an Airtag in my checked luggage just to make sure it's following along — also, if they lose it, we'll have the small comfort of knowing where it ended up.

# Total cost

My ticket had an included checked bag (75$), and I paid 100$ for the 2nd one. My two checked bags weighed 22.8kg each, carry-on weighed 20kg, and my backpack weighed ~10kg. 175$/75.6kg comes out to 2.31$/kg, beating out every shipping option I could find online. For reference, UPS charges approximately 9$/kg to ship a package (I'm using the weight of both my checked bags, since the carry-on is free anyway).

{{ add_pic("packingtips/5.png", "") }}

# Planned obsolescence

This blog post will be rendered unnecessary once an hour of my life spent repacking is worth more money than 9$/kg. I'll still think applying optimization to the feat of packing is an enduring skill though, which will serve me greatly in my other arenas of life. Also, fuck rolling your clothes.

# Conclusion

If you don't want to [remodel your whole life around travelling](https://vitalik.eth.limo/general/2022/06/20/backpack.html), keeping these lessons in mind is a nice compromise. Hope you enjoyed reading, and that my painful insights become your pro-tips.

Cya around!
</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 01 Jan 2024 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>My Vitalia Experience</title>
      <link>blog/vitalia1</link>
      <description>Seven days in a city trying to usurp death</description>
      <content:encoded>Hello. I just got back from Vitalia, a pop-up city in Prospera, a special economic zone (ZEDE) on Roatán, an island which fields a lot of cruise ship tourists and is part of the nation of Honduras, a country consisting of AT LEAST 99 islands. 

{{ add_pic("vitalia1/0.png", "") }}

I'm going to show some highlights and work through a "should I go to Vitalia-like thing in the future" scenario with you.

Generally, Vitalia felt like a vacation amongst like-minded people. I came during an off-week, so there wasn't a ton of networking or talks going on, but this gave me more time to have good conversations with people with mutual rare interests. I think it's worth going for an on-week, but off-week it's just a very casual gathering + vacation.

&lt;hr&gt;
# Highlights

## People

- It surprised me that many people were in their 30s and 40s — I suppose this is when the "I'm gonna die but I'm having too much fun rn" urge kicks in? Towards the end, a few undergraduate students flew in for the conference week, but I did not meet anyone near my age. 

- I came to Vitalia looking for fellow builders. Without counting business builders, these are a rare breed at Vitalia — I'd say only 4 out of the ~50 people I met. I broke in the soldering iron in the Augmentation Lab (first!!), and I came in the 6th week of Vitalia being open.

{{ add_pic("vitalia1/2.jpeg", "Underutilized electronics workshop at Vitalia") }}

- There are lots of Quantified Self people, nootropics people, and implant people. These fit my interests pretty well, so I enjoyed talking to these people — I've never found as dense a population as Vitalia. While I was there, the Biohacker DAO ran an intranasal insulin vs. cognition study and let anyone try their hand at analyzing the data. My roommate was into nootropics, and had purchased and tried many of the compounds I asked him about. And the Augmentation Lab, ran by Cassox of [Augmentation Limitless](https://augmentationlimitles.ipage.com/) always had a couple cyborgs hanging out that would entertain fun conversations on transhumanism. 

{{ add_pic("vitalia1/3.jpeg", "Cognitive enhancement / longevity substances table. Only legal stuff of course, and a ton of olive oil.") }}

- There were also many crypto people. During my stay, there was some kind of fundraiser on Gitcoin for Vitalia which used "quadratic funding" to match donations, and several people were fully interested in making useful cryptocurrencies. I find it hard to talk to those people, but I did like seeing real-world use cases for cryptocurrency. Even then, I thought many of the Gitcoin projects were stupid — IMO, creating some dashboard or display or status token should not require much crowdfunding.

- Everyone knows everything about Bryan Johnson and his penis. His olive oil takes up half the medicine table — I ate that for lunch most days since it was free. It leaves a spicy feeling in the throat. Since I never drink other olive oil straight, I don't have anything to compare it to, but I did feel satiated afterwards. In high school I tried a tablespoon of medium chain triglycerides (MCTs, fancy oil for your brain promoted by BulletProof coffee guy), which left me on the toilet for a few painful hours — trying the same with olive oil set off warning bells in my head, but nothing happened. 

- Since I went for an off-week, not many short-term people were around and there weren't many planned events. Consequently, I saw everyone going into the gym at least once, every day. Even I went, spurred by my roommate to train just a muscle group a day. We all know how effectively these gateway drugs work, and soon I was spending an hour a day lifting smelly heavy objects with him. Please don't think I'm complaining — I enjoy feeling sore and working out, but weights are always smelly and usually heavy. 

{{ add_pic("vitalia1/4.png", "One of the talks, History of Cybernetics, hosted by Cass and Fraiz") }}

- Most longevity people focused on genetic stuff, altering aging or improving cognition at the DNA level (I think this approach comes from watching Aubrey de Grey?). I find this approach a bit long-winded — highly skeptical that simply switching a fetus's SNPs to the slightly higher IQ ones would lead to any benefit in intelligence without causing some awful incidental genetic mutations. Price of progress or avoidable folly?

## Honduras

{{ add_pic("vitalia1/5.jpg", "") }}

### Climate

- Man, I've lived somewhere cold for the past 6 years (Pittsburgh then Boston then SF then Seattle then Chicago then Zurich) — it is NICE to go outside wearing only a t-shirt. And pants and stuff, but I mean the one-thin-layer thing. Very comfy aside from the sweat and the AC being too cold sometimes. 

- Oh yea, and I forgot there are also a lot of bugs in warm places. My first night I thought "oh it's so warm, let's sleep on the patio" — woke up when it started pouring, and noticed like 30-some bug bites all over my feet. I had been warned of the sand flies, but I looked with my flashlight whenever I felt something and never saw anything. My roommate referred to these creatures as "no-see-ems".

- It is beautiful on the island. Birds are making calls I've never heard (there's one that does a proper [chirp](https://en.wikipedia.org/wiki/Chirp) like the signals concept), coral reefs that you can just kayak to and look at, and crazy tropical fish. It's interesting to me that fish can be all sorts of gaudy neon colors while birds are advised to be more camoflagued. Also, I didn't notice sexual dimorphism in the fish, which is definitely a phenomena in many bird species (I remember reading that male ducks are shinier so they can distract predators from the female ducks). Possibly this is because the mother fish is not needed for the fish babies to survive, unlike in birds, so fish don't if one sex of adults takes the hit more often.

{{ add_pic("vitalia1/6.jpg", "Weird pirates everywhere in my hotel") }}

### Food

- Food is not cheap. At the resort you have an option of three places, and all the food is... dry? This is the word I'm going with. I felt like all the food I ate parched me. Their rice contains less water than I'm used to, beans are dry, meat is usually dry, fish (fried) is dry. The last day I had a pasta with lobster, that meal really felt good and wet.

{{ add_pic("vitalia1/7.jpeg", "We have the same ramen at home, except not the shrimp flavored one. I bought one of those to try") }}

- The grocery store nearest my hotel stocked a lot of American goodies, possibly for the tourists to feel at home. I saw several Kirkland branded things, and many familiar canned goods. This kinda took away the appeal of shopping at a local store, but I did find a few items to enjoy. Particularly, this brand of "drinkable" yogurt. The yogurt is basic cup-yogurt consistency in a large water bottle form-factor, and it's delicious though hard to drink. Prices mostly match US groceries. 

{{ add_pic("vitalia1/8.jpeg", "") }}

### Random

- On Roatán, transportation is spotty (flag a taxi and haggle, or flag the bus and haggle, or walk) and WiFi is mid (people complained they couldn't videoconference). I experienced my first rotten egg while cooking one night, which stank up the cooking area and temporarily took my pan out of commission. I float-tested the rest of the eggs and set aside any that even stood up in the water. Luckily I only had one bad egg, and most of the standing ones were ok too (just yolk disintegrated instead of in one blob)

- Because there is so much tourism, everyone takes payment in USD but gives change in Honduran Limperas so you WILL get a chance to collect bills as souvenirs. No coins due to 1 Limpera = 4 cents


# Longer term

- I acquired a long reading list through my conversations that I need to work through — I imagine this will be arduous scientific stuff. Some people recommended me various longevity supplements which I'll read more about. 

- I met the implant folks at AugLim, showed them my [demo](../../projects/implantables) and joined their Discord group. Hopefully we'll be able to work together synergistically in the future once I get the power delivery sorted out.

- Quantified Self people made a nice group chat, this will be a neat place to post personal analyses (maybe?) and get feedback or inspiration on self-experiments

- I got a lot of SF connections so hopefully finding a group house will be easier. 

- Several people asked me to help their project with some electronics, this will fund the Year further. 

- I'm now considering incorporating just so what I'm doing is more legible to other people. When I tell people I'm taking a gap to work on personal projects, sometimes I see them assume I'm just doing nothing and having fun. But that's not the case — I am working hard on stuff I think is useful, I'm just not employed and this is not a startup and it's not a research project. Open-endedness is difficult for people to understand, and even harder for me to convey. So maybe startup? 

- Oh, and I made a great friend :)

{{ add_pic("vitalia1/9.jpg", "My new friend James") }}
</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 23 Feb 2024 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Apple Vision Pro Hackathon: A Retrospective</title>
      <link>blog/avph_reflection</link>
      <description>Thoughts on the Vision Pro</description>
      <content:encoded>If you have a Vision Pro we could borrow and live in/near Pittsburgh and are free before May 2024, I would be interested in hosting this hackathon again. Please email/DM me if you'd like that, and we can make it happen :)

&lt;hr&gt;

Hello! Last weekend I hosted an Apple Vision Pro Hackathon (aka AVP Hackathon), which simply entailed asking everyone I knew if I could borrow a Vision Pro and then asking everyone if they wanted to develop on it for a couple days.

{{ add_pic("avph_reflection/0.png", "") }}

I think hackathons are a good event for colleges since full-time coders are usually too world-weary to also spend their weekends coding. Nonetheless, I picked a bad time to ask, since I mostly knew researchers and we're a month shy of the next big conference deadline. 

Anyway, somehow I got a crew of four people and we spent 48 hours building apps and demos for the AVP. The main goal was to write app that would make you want to wear the AVP (the most useful are screen-mirroring and web browsing). 

We also had a rule that someone had to be wearing the AVP at all times —  in the end, this didn't even need enforcing because we were iterating so fast that someone always needed it. In the end, we made 3.5 working apps, which is a pretty good hitrate. 

{{tableofcontents("blog/avph_reflection.md")}}


# Day 1 

Started at 10am, got to the space around 10:30am. I provided beverages and coffee.

{{ add_pic("avph_reflection/1.jpg", "") }}

We spent most of the day learning Swift. I made and loaded a custom 3D model and then tried to write custom gestures that would spawn/move it around. 

{{ add_pic("avph_reflection/2.jpg", "") }}

I cribbed the head/hand tracking from a [Hand Ruler](https://github.com/FlipByBlink/HandsRuler) app, then kept cutting the code down until I could understand all of it. I think everyone else had a very similar process, since the default examples that Apple provides are too full-featured to just illustrate single concepts at a time. 

{{ add_pic("avph_reflection/3.jpg", "") }}

The AVP is meant to be a single-user device, so it doesn't let you do screen mirroring unless the same user is signed into both the Macbook and the AVP. In order to avoid taking off the headset every time we uploaded code, we spent a lot of time looking through the AVP cameras at our screens trying to make small edits. This is really close to the limit of what the AVP's AR resolution is capable of, and this process would've been much more comfortable if we could just project our displays into AVP space.

When we wrapped up the day, everyone sort-of understood the Swift structure and had run some example code already. The language is pretty straightforward, and the code completions in the Xcode IDE are more full-featured than any IDE I've used before.

# Day 2

Late start, 11:30am. My roommate had some group project to work on in the morning, and everyone else woke up late.

{{ add_pic("avph_reflection/4.jpg", "") }}

Someone else finished their teardown of an example, and made an app where the press of a button opens an immersive video. I finished wrapping up all my code into a simple demo app ([on github here!](https://github.com/kongmunist/Vision-Pro-Head-Hand-Tracking-Demo)) and started working on custom gesture recognition. 

&lt;blockquote class="twitter-tweet tw-align-center"&gt;&lt;p lang="tl" dir="ltr"&gt;Italian simulator &lt;a href="https://t.co/TqmlcglZLM"&gt;pic.twitter.com/TqmlcglZLM&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy (@oldestasian) &lt;a href="https://twitter.com/oldestasian/status/1767012032132616565?ref_src=twsrc%5Etfw"&gt;March 11, 2024&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

Gestures have no helper functions. Even in the [Happy Beam demo] provided by Apple, to check that the user has formed a heart-shaped hand gesture they just calculate the distance between specific hand joints and use some smoothing to make it a bit nicer looking. I wrote some functions like `closeTo(jointnames, threshold)` to make simple gestures easy to implement, then used them to detect this Italian pinched fingers gesture. 

By ~3am, everyone had finished their app to a point where we could film it. Due to daylight savings, we thought it was an hour later.

# Demos

In the end, we had written the following apps: 

- Gaussian splatting model viewer running on Metal, with hand gesture control

- Collaborative video upload gallery for AVP/iPhone 15 spatial videos
{{ add_pic("avph_reflection/riftv1.jpg", "") }}

- AR interface to control the color and brightness of real-life smart bulbs
{{ add_pic("avph_reflection/AVP_Light_App.jpg", "") }}


- Lighter demo like the early iPod apps

&lt;blockquote class="twitter-tweet tw-align-center" data-media-max-width="560"&gt;&lt;p lang="en" dir="ltr"&gt;Throwback lighter app, made on the &lt;a href="https://twitter.com/hashtag/AppleVisionPro?src=hash&amp;amp;ref_src=twsrc%5Etfw"&gt;#AppleVisionPro&lt;/a&gt; &lt;a href="https://t.co/y3sJr8NvB9"&gt;pic.twitter.com/y3sJr8NvB9&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy (@oldestasian) &lt;a href="https://twitter.com/oldestasian/status/1767978456908968044?ref_src=twsrc%5Etfw"&gt;March 13, 2024&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

(I don't have all the videos yet so this list will be updated with the missing ones later)

# Takeaways

## Real device stuff

- At first when I looked through the AVP, I compared the passthrough-AR with other AR headsets I've tried — the AVP is the best I've tried, so I enjoyed it. After a few hours of wearing/developing on the AVP, I began to compare it to no-headset and it loses on every metric. Some LED lights are flickery through the AVP camera, and reflective stuff (screens, windows) looks warped and not flat.

- The IPD calibration (motor moving lenses closer/further) is required everytime we switched headsets, but that takes only ~10 seconds. I find it weird that I had to hit the Digital Crown button to start the IPD, but maybe this is an Apple-y design principle that nothing physical moves unless the user moves something physically. - Since we couldn't get screen mirroring working, we had to look through the AVP to see my computer screen to edit code and upload stuff. The text is surprisingly legible, but no amount of surprise overcomes the fact that it's not good enough to be comfortable. Two of the hackers couldn't read off their screen through the headset at all, and they don't even wear glasses normally. - AVP is clearly meant for a single owner — I think only 2 user profiles can be added at once. This made the hackathon a bit harder, but not impossible — for instance the eye tracking is only calibrated to one person at a time, but it's good enough that other people could use it without needing to redo the calibration.

- The device is not socially accessible. All the experiences are kinda "in your head", requiring screen mirroring or recording to get someone else to see it. All the videos I show are shot on-device using Developer Capture in Reality Composer Pro, and a minute-long video takes ~1-2 minutes to transmit to your laptop. 

## Coding in Swift

- Dev time is quick for small apps. A fresh upload required a bit more time, but each iteration of the code only took ~30 seconds from pressing the Xcode Run button to seeing the app in the headset. On my Macbook Pro with an M1 Max, build times were only ~5-10 seconds and barely noticeable.

- It's possible to upload code through Unity and Swift, and it may be easier for you to get started if you have Unity experience. However, I did hear that there were some features which didn't exist in Unity but did in Swift, so for full features you may want to just bite the bullet and learn Swift. Swift is a great to program in once you get it. Two days after the hackathon, I actually felt an itch to write a Mac app so I could continue using the Xcode IDE. 

- Swift examples provided by Apple are incredibly full-featured, meaning they can put out [four example demos](https://developer.apple.com/documentation/visionos/world) and show most features in use in the code. However, this is not super beginner friendly — I'd much rather have single-use example demos. 

- Examples are great for illustrating what is possible. If you see a transparent window with a floating button in the Hello World demo, you know that somewhere in the example project is the code that makes that happen.

- Xcode is a great IDE, it has both an AVP simulator which launches separately and a containerized one that runs inside the editor. The containerized one does not show windows faithfully, meaning you should always check what they look like in the real simulator

- The AVP simulator can pretty accurately do windows, buttons, and other functionality, so you don't even need to cough up the cash for a device if you're interested in developing more straightforward apps. The only thing it doesn't have is hands — gestures can only be developed using a real device. I remember seeing a [blog post about faking hands](https://varrall.substack.com/p/hand-tracking-in-vision-pro-simulator) in the AVP simulator, but did not try it myself yet. 

{{ add_pic("avph_reflection/ide.png", "") }}

## Dev community

- Forum posts are lacking, and very few people have posted Vision Pro development questions. If we assume 100k units have been sold, and only 10k to developers, then we can only expect ~100-1000 people active on the developer forums. Consequently, there are very few tutorials — large alpha in this space! Within a few months, you could easily become the foremost authority on VisionOS development. 

## Idealogical feedback

- I think if the AVP is meant to be a work device, then it's going to need something equivalent to hotkeys but for 3D space. Custom gestures are really cool, and they offer a lot of possibilities for new interactions (i.e. subtle hand gestures can trigger web apps or specific programs), but they can only be accessed in an Immersive App meaning no other apps can be open simultaneously, preventing window management or any fun systems-level control using custom gestures. You could make the comparison that iPhone apps are only allowed to recognize custom swipe/tap gestures when their app is open, but I think this limits the AVP much more when the available space of interactions comes from fine-grained continuous hand tracking.

&lt;hr&gt;
# Conclusion

Anyway, super fun weekend, and I'm incredibly happy that everyone got to a working demo despite starting from zero re:Swift. I'll reiterate that I'd love to do this again in April 2024, so if you have a headset or want to join next time, just reach out! Out-of-towners are welcome to stay on my couch.</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 13 Mar 2024 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Most-scanned bulletin boards at CMU</title>
      <link>blog/cmuads</link>
      <description>Finally publicizing some sneaky data collection from 4 years ago</description>
      <content:encoded>Hello!

Back when I attended CMU, there were these massive bulletin boards around campus full of adverts from students and profs. Students usually promoted their app/club/org/student course, and professors pushed their classes.

{{ add_pic("cmuads/0.jpg", "") }}

When I started putting my own ads on these boards, I was always curious which b-boards were the most frequently looked at. For instance, the board between the Sorrells library and the bathroom gets a lot of traffic, but the people walking there were probably pretty focused on studying. Whereas the UC bulletin boards get less foot traffic, but the people there seem calmer since they aren't walking fast to get to class.

Anyway, when I finished a campus events aggregator called [Carnegie Calendar](../../projects/carnegiecalendar) and wanted to promote it by putting up bulletins, I finally had my chance to determine the highest engagement b-board.

I printed out 63 of these flyers promoting Carnegie Calendar, each with a unique QR code demarcated by this little number in the corner. Each QR code led to the same website with a different trailing URL argument, which my server recorded in a text file. 


{{ add_pic("CCzoomin.png", "See the little 1?") }}

I got a bunch of friends to help me put up the QR codes (Thanks Nancy Sam Ruijie and I think Nate?) and waited for the data to roll in.

I always wanted to sell this data to some overzealous student group that puts up a ton of flyers and cares about it, but after 4 years I'm realizing that's probably not gonna happen. So I've tabulated the results into this map below.

{{ add_pic("cmuads/1.png", "") }}

More granularly, the per-bulletin board data can be found [here](https://docs.google.com/spreadsheets/d/1T-BplbYhJhCCI-hyfR-BIS-O5AUM7uf--s4L8abqz_0/edit#gid=1010815453).

A day or two after all the flyers were up, I realized that it would be free traffic if I also advertised the website in the school Facebook groups. I made a separate URL extension for the group links and posted it. The single Facebook link received around 6x the visits to all in-person links despite only being up for like 8 hours. 

{{ add_pic("cmuads/2.png", "") }}

As with all deploys, some event I scraped the first day of public release ended up busting the website, and I didn't realize until noon the next day. I always wonder about those missed first impressions. Anyway, enough lore for the day. Cya next time!
</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 15 Apr 2024 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>A Layman's Guide To The ICNIRP Guidelines On Limiting Exposure To Electromagnetic Fields (100kHz-300GHz)</title>
      <link>blog/icnirp</link>
      <description>I read this and it was actually interesting</description>
      <content:encoded>Hello!

In this post I'm going to sum up all the cool things I learned about the human body and the way the body interacts with electromagnetic (EM) radiation. All these facts I learned from a casual perusal of the ICNIRP 2020 guidelines doc, which you can find yourself [here](https://www.icnirp.org/cms/upload/publications/ICNIRPrfgdl2020.pdf).

{{ add_pic("icnirp/babies.png", "ICNIRP guidelines were probably not read by the creator of this meme") }}

ICNIRP is an international non-profit made up of random scientists who enjoy quantifying electromagnetic radiation safety. They read the most up-to-date literature every few years and compile a large guidelines doc describing limits of what the human body. These guidelines are then used by researchers and government regulatory bodies to establish safe legal bounds on human-EM exposure.

{{ add_pic("icnirp/0.png", "") }}

# Why did you read this dry and boring safety text?

Recently, I needed to describe the safety of an electromagnetic system that interacted with the human body. To do so, I needed to show that the design conformed to the ICNIRP guidelines — this was a task I had been putting off in case A) the system wasn't actually safe, or B) the guidelines would be complex and hard to understand. 

I was close to giving up in favor of relying on a secondary source's interpretation of the ICNIRP guidelines, but in a moment of clarity, I decided to first give the primary text my best crack. And thank goodness! The text is full of interesting little details, explanations of why the limits are what they are, and the different ways EM radiation can damage the human body. I found the info I needed and got out, but I found the guidelines so interesting that I told myself I'd revisit them after my deadline. 

The rest of this blog will be in list format, interrupted by occasional screenshots. 

&lt;hr&gt;

# Interesting electromagnetic trivia

- ICNIRP only acknowledges three ways EM radiation can harm human tissues: 1) electric fields below 10MHz can stimulate nerves, 2) biological membranes can break down or change permeability in response to sufficiently strong fields, and 3) EM radiation can cause tissue heating.

- At higher frequencies &gt;10MHz, damage from tissue heating (3) happens way before cell permeability changes (2), so we can ignore 2 as long as we respect the guidelines for (3). At lower frequencies &lt;10MHz, nerve activation (1) happens way before the permeability stuff (2), so again we can ignore 2 as long as we follow the guidelines for (1). At 10MHz, changes in permeability kill you immediately (just joking)

{{ add_pic("icnirp/1.png", "") }}

- The frequencies that can affect nerves are limited to ~100kHz and are described by participants as tingling. As frequency increases to 10MHz, this sensation becomes one of "warmth" and we start to care more about thermal safety. Nerve effects are mainly described in the other ICNIRP doc for lower frequencies.

- Waves at a higher frequency penetrate less deeply into human tissues. Because of this, below 6GHz the guidelines are concerned with measuring temperature rise with "specific energy absorption rate" (SAR, W/kg), and above 6GHz they measure temperature rise with "absorbed power density" ($S_ab$, W/$m^2$). 

- Humans' core body temperature is 37°C and can vary up to 1°C throughout a day. Correspondingly, the guidelines try to keep EM radiation exposure below a power level that increases body temperature 1°C (as a conservative limit, they actually try to keep it under 0.1°C). At the 100kHz-6GHz range, a SAR of approximately 6 W/kg leads to a core body temperature increase of 1°C. The issue with increased body temperature is that the heart has to work harder and it causes more accidents

{{ add_pic("icnirp/2.png", "") }}

{{ add_pic("icnirp/3.png", "") }}

- From this we also learn that children can dissipate heat better than adults and therefore have a higher SAR threshold.

{{ add_pic("icnirp/4.png", "") }}

{{ add_pic("icnirp/5.png", "") }}

- ICNIRP also gives us a neat reference for how much power an adult human uses normally. At rest 1W/kg, at stand 2W/kg, and 12W/kg running. I guess this means standing desks actually work?

{{ add_pic("icnirp/6.png", "") }}

- Earlier I mentioned the distinction between surface and deep tissue EM energy absorption. The doc elaborates that at 6GHz, 86% of the power is absorbed in the first 8mm of skin. Surface heating is also less worrisome because we can get rid of it more easily.

{{ add_pic("icnirp/7.png", "") }}

- More tangential trivia, almost all human tissues get damaged beyond 42°C, exhibiting very little interperson variation. To respect this limit, ICNIRP tries to limit localized tissue heating to 41°C. 

{{ add_pic("icnirp/8.png", "") }}

- Extremeties are usually around 33-36°C, while core tissues are closer to 38°. Therefore extremeties are limited in heating to +5°C, while core is limited to +2°C. 

{{ add_pic("icnirp/9.png", "") }}

- Testicles stop making sperm when people sit down because they heat up. I wonder if the increase in desk jobs is responsible for the global decline in sperm count? 

{{ add_pic("icnirp/10.png", "") }}

- They approximate whole-body SAR by using a 10g meat cube. They say the spread of heat in a 10g mass is "close enough" to a larger mass. 

{{ add_pic("icnirp/11.png", "") }}

- ICNIRP wants to limit body temperature rises to 0.1C, because of this they choose a whole-body SAR limit of 0.4W/kg. For civilians who don't know what they're up against, the limit is 5x better at 0.08W/kg. This reduction is a bit arbitrary to me, but I appreciate the safety factor. 

{{ add_pic("icnirp/12.png", "") }}

- Old people sweat 25% less effectively than younger people

{{ add_pic("icnirp/13.png", "") }}

&lt;hr&gt;

I realized that 5G cell phones use frequencies covered by these guidelines (450MHz-6GHz and 24-52GHz). In light of ICNIRP's explanations, people who freak out about 5G affecting their babies don't make much sense — it's just the effects of heating, and our communications are so efficient that the heating is not even significant. While I enjoy doing my own research, these people should get better at it before being so vocal. 

{{ add_pic("icnirp/babies.png", "") }}

It's cool: reading these guidelines I get a better understanding on how to think about quantifying safety. I also realize how much harder it would have been to spec safety without everything compiled together — thank you ICNIRP!

Anyway, I hope you successfully stay away from dangerous EM waves until I see you next. Cya next time!
</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 16 Apr 2024 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>MonoChrome: In Defense Of Using One Chrome Window At A Time</title>
      <link>blog/monochrome</link>
      <description>Browser meta-war</description>
      <content:encoded>Hello. Today I'm going to tell you about a work-style switch I made about a year ago. First, a tangent.

# Tangent

{{ add_pic("monochrome/0.jpeg", "") }}

In 2021, I watched a coworker swipe up their Mission Control page to reveal ~20 different applications all on the same desktop. Extremely surprised, I asked why they did that, and they said it was faster. At the time, thought of only using 1 desktop was alien to me because I segregated each work subject into a different desktop (PCBs, software, messenger apps).


Later I realized multiple desktops actually do suck — there's a 0.5-1 second delay when switching desktops which cannot be reduced, and my ProMotion 120Hz screen never turns on when I have multiple desktops. This is, I think, a hard limitation meant to guarantee the OS has enough time to shift memory around and load up the main apps in the new window. But it slowed me down, and then I started using a single desktop. 

# Main point

I used to do the same thing with my Chrome tabs. 100 tabs spread across 4 windows, constantly alt-tabbing through to find the right one. One for electronics, one for articles and blog posts, one for wikipedia pages to read, etc. This was a bad strat, but at the time I felt "organized". 

But the same thing was true for multiple Desktops, why not apply the same lessons? Now I just keep one window open. If it gets cluttered, I go through and clear it out. Article? Read and close it. Personal blog you want to correspond with? Write them an email and close it. If you don't have time now, just save it in a Google doc or the Notes app, I'm sure you'll get back to it, if it's so important *wink wink*

{{ add_pic("monochrome/1.png", "") }}

As an example, my current window (above) is just stuff I need to take notes on. These are all the tabs I have open. Contrast with my old "Session Buddy" saved tab lists which I've never ever looked twice at:

{{ add_pic("monochrome/2.png", "") }}

{{ add_pic("monochrome/3.png", "") }}

Imagine, I have hundreds of these lists, totally rotting in there. 

Seize the day, close those tabs, and take back the RAM for the important stuff: your buttery-smooth 120Hz screen refresh rate 
</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 12 Jun 2024 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Edge Esmerelda Experience</title>
      <link>blog/eee</link>
      <description>Popup City No. 2</description>
      <content:encoded>Hello! Again I have returned from a distant land with tales from a popup community. This city was/is called [Edge Esmerelda](https://www.edgeesmeralda.com/), and is/was located in Healdsburg, CA.  

{{ add_pic("eee/0.png", "") }}

In part, going was an experiment. I remember reading someone's blog post about "asking favors/questions you are SURE will be refused" as a way to make sure you really know the bounds of your social position (a la [Veritasium](https://www.youtube.com/watch?v=vKA4w2O61Xo)) — I did this about a year ago when I asked 3 people if I could borrow 20,000 Swiss Francs so I could stay in Switzerland. None of them had it, but all of them said yes.

Regarding Edge Esmerelda, I had just recently gone to SF and did not really want to go back so soon, but decided that if it was free then I would. I DM'd one of the organizers and asked if I could come for a week if I sorted my own housing, and they were very generous to me. Every time I do this exercise I am surprised.

Anyway. Here's what

# Edge Esmerelda is like

## College

College is a sweet time where all your friends live together and hang out often, and everyone you meet has a lot of potential. I am not _that_ old, but so far life has not been like that. But Edge was, and it made me miss living near cool people. 

## Harvard

Through one lens, Harvard is a place where smart people and rich people (not mutually exclusive) go to mingle. At the end, everyone gets a Harvard degree, and it's hard to tell which is which. In my first few days at Edge, I met many incredible biologists and engineers working on crazy ideas — measuring brain activity using lasers, airships for cargo and surveying, and balloons which slow down global warming. Later on, I met some crypto people who also founded hard tech companies. 

## A Conference

Most conferences I've been to go something like this: during the day people give talks and listen, groups go off to dinner, and then every night there's a party. If you tag along to dinner with someone interesting, you do get to chat with them, but otherwise you can't because it's either between sessions (rushed) or after drinks (incapacitated).

Being at Edge for a week felt like the perfect conference — there was no rush, but you still felt urgency to talk to interesting people. And there were events, but an interesting conversation took precedence. And because the talks were noncommittal, they were not one-sided since everyone who attended had a stake in the discussion that followed. I quickly found a stable group, but there were new faces to meet at every meal. If Edge was a conference it would be a perfect one.

## America

Massive simplfiication but stick with me: America was founded when a group of people unsatisfied by their local community chose to go somewhere else with all their homies. In 2024, new land is in short supply so "somewhere else" becomes "anywhere &gt;2 hours away from a major airport". Also in 2024, Protestantism has been superseded by scientism, and so "religious homies" becomes "science/engineering homies". 

## A Commune

In exchange for coming, all I had to do was be involved: I spoke with new people every day, gave talks, and attended events. But I really don't think I pulled my weight. Other people organized Pilates sessions, planned hackathons and brainstorming sessions, and threw parties. It felt a lot like my ideal commune — one community, in constant contact, giving and receiving favors so rapidly that a balance is achieved and life is enhanced for everyone. 

# List of stuff that happened

- Saw horses (2)

- I gave a talk on implants

{{ add_pic("eee/1.png", "") }}

- Showed off my holograms to a ton of people

- Experienced my first Pilates session

{{ add_pic("eee/2.jpeg", "") }}

- learned you can 3d print spring-y stuff, like this knife!

{{ add_pic("eee/4.jpeg", "") }}

- Stayed at Hotel Anson for 3 nights where I made friends with Canadians and non-Canadians alike

{{ add_pic("eee/3.jpeg", "") }}

</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 25 Jun 2024 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>I've been thinking about conviction</title>
      <link>blog/conviction</link>
      <description>I could clump my friends into four groups:

# Stable, unsure: pleading
Nice jobs which let them stay home 3 days a week. Whenever we talk it’s their boss’s kid’s birthday. Free mangoes in the office. I watched Queen’s Gambit and started playing Lichess. I thought there would be more than this.

# Unstable, unsure: worried
Making just enough to eke out a life’s work that’s fun to talk about at parties. Always at a sweet 6 month gig doing exactly what they want, wide open road afterwards. I should get a job. I was thinking of going back to school. What am I doing with my life? 

# Stable, sure: steady
Family people with a long lease. The days are the same, each one a joy. You catch up but nothing’s changed. Dog’s at the vet again for eating marbles. We’re rewatching House. We’re making dinner with zucchini from the weekend farmer’s market. 

# Unstable, sure: pursuing
Risking and failing, doing their work that with a smile. Independence, installations and workshops, but also grant rejections, rent increases, food stamps. I’ll be at the VR conference next month. Oh yea, I work for Nintendo at the moment. I’ve been exploring wax as a sculptural medium.

{{ add_pic("convictionmeme.png", "") }}

This is an incomplete lens, but it is possible to analyze your life through it, figure out where you are and where you want to be. And if you aren’t sure yet, I think everyone gets there when they’re older, or comes to terms with their inability to change anymore. Anyway, who am I to say? I am only 25.

</description>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 26 Oct 2024 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Let the foot slip</title>
      <link>blog/footslip</link>
      <description>climbing is life</description>
      <content:encoded>I want to preface by saying I feel stupid writing life lessons. I haven't lived very long, and anything I can write about seems obvious to me (of course it would). And also I cannot live your life for you — I didn't understand most advice until I was in a place to be able to give it myself i.e. I usually did not follow it until too late, learning the hard way. Oh well. 

Anyway, I've been rock climbing for about 6 months now. The last time I did something physical consistently was some lifting in high school, so it's been about 6 years. It's really exciting learning what your body can do. I can do four pullups when I could do none before, I can climb a few V4s, someone recently asked me to show them a route, etc. Socrates was right.

Something I noticed is that a lot of climbing is about trusting your own body. "trust the feet" to hold onto a tiny ledge, believing in your own ability to maintain a one-handed hold for half a second so you can get the next hold. This is fine and all, but what really surprised me is how to build this trust. Trust is not declarative like "I pronounce you man and wife", it can only be conveyed by testing it, finding the boundary. On many climbing setups, I felt like my feet were not gonna hold, but I realized that I'm careful on average, so it's about 50-50 when I think I can't hold it. Most of the time I think I'll fall, I just try it anyway and often I'll finish a tricky climb. 

The other side is people who never fall. Sometimes I'm watching someone, cheering them on internally; they reach a precipice, look around and up and down, fingers release and they jump off thinking they could never have made it. Sometimes they're right. Sometimes they're old, and know they can't afford to slip. But they will not grow. This is the cost. 

{{ add_pic("footslip.jpg", "Anyone else feel like this is the equivalent of a hamster wheel for humans? Just me??") }}

Lots of real things act like this. [Alexey](https://guzey.com/abolish-the-nih/) rails against the NIH for only funding sure-thing research, and I agree; it makes for boring science when everyone knows the results before the work is done, wasting effort, money and time creating ZERO entropy. Research questions should be kinda 50-50 on whether they'll work— at least the results will be interesting. Realizing you can ask for stuff is like this too — at least some of your asks should be rejected. You should aim to miss ~10% of your flights to minimize airport time. You will not improve your ELO by beating up kids (at chess) or make money by taking sure bets. Returns are made of risk.

Another advantage of letting the foot slip is that you learn how to fall. Falling is inevitable, and practicing unintentional falling allows you to recover gracefully. If you miss a flight, they'll often rebook you for free (even though it's your fault??). A failed experiment generates results which can be another piece of a bigger puzzle. A rejected ask is still a connection, and shows you a way to get better. A loss in chess lets you review your weaknesses, or learn their strengths. An attempt to write life advice that spells out too many details when the reader can infer for themselves the point of "learning to fall" applied to the rest of the examples can still help the readers who can't figure how that applies.

Ok sorry this is too long, you probably get it. Climbing is life, so the lessons are the same. Gotta figure out where the feet slip to figure out when they won't. And gotta figure out when YOU believe the feet are gonna slip and when they aren't, and when you're right and wrong, to figure out when to just send it anyway.

</content:encoded>
      <author>andykongresearch@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
    </item>
  </channel>
</rss>
