<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Andy Kong's Blog</title>
    <link>https://andykong.org</link>
    <description>Thoughts and Work</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 09 Aug 2023 13:07:29 +0000</lastBuildDate>
    <item>
      <title>First Post!</title>
      <link>blog/first</link>
      <description>WHAT'S GOING ON I'M MAKING A WEBSITE</description>
      <content:encoded>**WHAT'S GOING ON I'M MAKING A WEBSITE**

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 24 Jun 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>The exploration of unexplorable search spaces</title>
      <link>blog/unexplorable</link>
      <description>Any hard problem can be solved by randomly guessing — if you're good enough at guessing.</description>
      <content:encoded>Any hard problem can be solved by randomly guessing — if you're good enough at guessing.

Take my website's aesthetic. The colors composing my website were not chosen at random. I went on a nice site called [colourlovers.com](https://www.colourlovers.com/) where people discover and post colors they think are beautiful and I found a curated selection of the best colors out of the 16.7 million RGB ones. Not only that, they were already matched in a palette of 4 complementary colors! Thank god for these color-space explorers, without them my site would be a garish combination of 4 colors I liked individually, but put together look like an alternative Mardi Gras parade.


&lt;img src={{ url_for('static', filename = 'dreammagnet.png')}} width="450" class="d-block mx-auto"/&gt;
&lt;p class="caption"&gt;[Dream Magnet](https://www.colourlovers.com/palette/482774/dream_magnet) has one of the prettiest cyans I have ever seen&lt;/p&gt;


That incident got me thinking. I think it is amazingly frustrating that many problems have solutions that can be guessed. In any field, almost all problems can be randomly, instantaneously solved to a greater extent than methodical approaches are currently solving them. This is because we know what type of thing we're guessing (integers from 1-60), just not what the right guesses are.

In the field of personal finance, I could correctly guess 4 or 5 numbers and win the lottery. The solution space for this one isn't even that big, only 60&lt;sup&gt;5&lt;/sup&gt;=777 million (I wonder if the lucky numbers are intentional), but the importance is several orders of magnitude higher than picking a color scheme for my website while only being 1 order of magnitude harder to find.  

Or take machine learning for instance. Nowadays, modern research labs spent millions training neural networks with their fancy computers. With the invention of [backpropagation](http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf) by Geoffrey Hinton in 1986, every school and company dropped anything computationally challenging they were working on (re: not webdev or IT) to figure out the fastest way to do matrix math really, really fast. The faster your model could multiply huge matrices of floating-point numbers together, the better/faster/stronger it could tell a [dog from a cat](https://www.kaggle.com/c/dogs-vs-cats) or [classify a 32x32 pixel image](http://www.image-net.org/challenges/LSVRC/) as a airplane, bird, etc.
 
Backpropagation left the machine learning community a method for tuning the matrices. As any high school/college kid interested in machine learning knows, the really hard part of machine learning that Geoffrey Hinton didn't solve is &lt;del&gt;automagically importing data&lt;/del&gt; twiddling the matrix numbers to perfection sometime before the heat death of the universe. THAT'S why my neighbor at my first-year college dorm had multiple high-end graphics cards, not to play the Overwatch in 4K at 144Hz while their model is training. My jealousy of their dual 144Hz monitor gaming setup aside, finding the minimum in higher dimensions is HARD. 

&lt;img src="https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png" class="mx-auto d-block" width = 300px style=""/&gt;

&lt;p class="caption"&gt;You think this is bad? Imagine how many bumps it has in the 500th dimension!!&lt;/p&gt;

Confronted with all this complexity from today's approach, I could just give up. Like an Amazon warehouse stocking inventory, I could just begin to guess values and randomly shove them into the matrices anywhere they fit. And, in one world out of many I will achieve a miracle: I will find the global minimum by raw chance. Now, there's no academic clout to be gained this way, and I wouldn't know how my matrices worked. But neither does any other ML researcher. 

Then take a look at medical research. To find new medicines today, we just add random functional groups to an existing, working drug. To check if doing something random made the medicine more effective somehow, we give it to a bunch of rats, and if it works for them, a bunch of monkeys, and if it works for them, the humans that need it. The process takes millions of dollars and lots of time, and in the end we still don't know how the new drug works until some PhD student figures out the mechanism of action 20 years later. 

Why does it matter? It's because I think it's sad that the lives of many experts today will be dedicated to finding the best way to guess at random numbers, with no need to understand *how it all works* once they've done it.</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 30 Jun 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Simple arithmetic addition with neural networks</title>
      <link>blog/advancedaddition</link>
      <description>I have stumbled across a new way to add numbers.</description>
      <content:encoded>Before we start, I want to clarify that I am not just some neural network script kiddie, and that my blog is not going to be just a personal log of all the neural networks I make. I also enjoy electronics and hardware design. That being said, check out this neural network I made!

There is an underlying perception that people new to the topic of machine learning have, and that is this: when a model isn't doing too hot, you need either more data or more hidden layers with more neurons between them. This misconception isn't their fault; it seems everyone in the news is talking about these huge neural networks trained on terabytes of data to decide whether to advertise an eco-friendly backpack or water bottle to an unsuspecting consumer based on their heart rate variance. However, more neurons != better fit, as I will proceed to show. 

We often think of neural networks as endlessly flexible tools. I mean, can't they [approximate any known function](https://en.wikipedia.org/wiki/Universal_approximation_theorem)? And while that is mathematically proven to be true, you, as a real-world practitioner of ML, have to consider how much more complicated the cost function gets for every single neuron you add. The already-bumpy landscape of a 3D cost function becomes more wrinkly, and your model has a much higher chance of getting stuck in a local minima. 

This becomes apparent when you train a neural network to do something as simple as addition. I generated 1000 examples of addition of two integers between 0-100, which looks like this

&lt;img src={{ url_for('static', filename = 'advancedadditiontraining.png')}} width="300" class="d-block mx-auto"/&gt;

## Model A

Using these, I trained a simple neural network with no hidden layers (does that invalidate it as a neural network? I'm not sure the semantics of the name), 2 input nodes, and 1 output node. Let's call it Model A. Here's the results

&lt;img src={{ url_for('static', filename = 'aasimpleexamples.png')}} width="300" class="d-block mx-auto"/&gt;

As you can see, this model is a little off, but asymptotically correct. The error is nearly constant regardless of the inputs, meaning as the inputs go up, the total percentage error of the model goes to 0. In addition, this model only requires 2 multiplications and 2 additions to perform one addition, which is only a 4x cost computationally. Since our computers are definitely more than 4x faster than they were when addition was invented, this model is feasible for use in modern industry. 

Even better, this example of machine learning is explainable. If we take a look at the weights, we can boil this neural network down into one equation. Using x&lt;sub&gt;1&lt;/sub&gt; and x&lt;sub&gt;2&lt;/sub&gt; as the first and second inputs respectively, our neural network's weights can be algebraically expressed.

&lt;img src={{ url_for('static', filename = 'aasimpleweights.png')}} width="300" class="d-block mx-auto"/&gt;
Becomes
&lt;img src={{ url_for('static', filename = 'aaboiled.png')}} width="300" class="d-block mx-auto"/&gt;

This AI model is clearly understandable by any person who knows basic algebra, and therefore can be trusted to perform addition in a transparent method that doesn't involve plotting to kill its human creators with some of its spare neurons. 

## Model B

Now, contrast this simple but effective Model A with a second, more complex Model B. This model has a hidden layer with one neuron in it, in addition to the input 2 nodes and output node. As such, it performs worse addition. 

&lt;img src={{ url_for('static', filename = 'advancedadditionexamples.png')}} width="300" class="d-block mx-auto"/&gt;

We see that the error is significant, definitely not enough to just round away like we could with Model A. This model also took extra time to train, since it had an additional 3 equations to calculate. It is significantly less feasible for use in industry because of its inefficiency and inaccuracy. 


## Model C
"But what about huge inner layers Andy? You're just using one! Won't that solve all our problems like Elon Musk and Andrew Ng promised?" **NO!** Here I've created Model C, which has 10 hidden neurons in its hidden layer. 

&lt;img src={{ url_for('static', filename = 'aabignetexamples.png')}} width="300" class="d-block mx-auto"/&gt;

It's AWFUL! Terrible! Abysmal! A child could do better than that! Whe- oh, we're getting a message from ML themselves. They said to change the learning rate. Whoops. Here's the actual results

&lt;img src={{ url_for('static', filename = 'aabignetgoodexamples.png')}} width="300" class="d-block mx-auto"/&gt;

We can see that though Model C performs better than Model B's small hidden layer, it's doesn't even beat Model A, the simplest model of them all. And additionally, the computation cost is through the roof! Definitely not scalable for industry. 

The takeaway of this is that not all machine learning problems can be solved by throwing tons of neurons at them, in the same way that not all real world problems can be solved by throwing tons of ninjas at them (but most of them could). So before you jam 16 fully-connected hidden layers of 256 neurons each in your neural network, think about how complex the function you're trying to model actually is. And pick a model that fits the function you need, instead of one that fits your mental model of how chunky thicc a SOTA neural network should be. You might get better results for less computation.

&lt;hr&gt;
Bonus: Addition using any of these models is not commutative, which was really funny to me. I think this has applications for curing the boredom once mathematicians and kindergarteners get tired of adding 1 and 1 together. They can now do it in new ways!

&lt;img src={{ url_for('static', filename = 'aanoncommutativity.png')}} width="300" class="d-block mx-auto"/&gt;
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 17 Jul 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>The miracle of brain-computer interfaces</title>
      <link>blog/extraarms</link>
      <description>How BCIs can give us more arms</description>
      <content:encoded>Bear with me. I'm going to try to explain something slightly abstract but incredibly interesting. 

## You, the brain reading this

Look at your hands. Admire your reflection in a mirror. It's you, right?

Wrong. That is not you. You are the brain inside that person. You are a chunk of meat piloting a bone-armored meatsuit, and that meatsuit's hands and feet are the only ways you can interact with the physical world. It is kinda weird to think of our limbs as tools. Our hands act as an extension of our brain into the physical world, a way for our abstract thoughts to influence the concrete world. And it's our best tool because it's so generalizable; I'm currently using the fingers on mine to hit my keyboard in specific combinations to write the words you're reading. 

However, their generality leads us to create more specialized tools to make certain tasks easier for us. Hammers allow us to piece together pieces of wood with bits of metal. Pianos make it easy to create specific sounds reliably. Keyboards enable precise electron changes on tiny, tiny silicon chips. We use our general tools to create and harness more specialized ones. 

But there's a problem. Whenever we use a tool, we are actually trading use of our hands or feet for use of a more specialized tool. Even if we know exactly how we want to use the tool, we have to figure out how to best interact with it. This is an added learning curve called muscle memory.

This might not seem like a big deal to most people, but that's because most people have working hands or feet. They have general tools with which to harness specialized ones. But what if you're already using both your hands and just need another one, like a surgeon in a 36 hour surgery who just needs a nurse to hold a piece of skin out of the way? What if you've lost the ability to control your limbs in bike accident, or have terribly trembling hands from Parkinson's as a result of a random mutation? These people can't harness any tools! They're out of luck in today's technology.

&lt;img src={{ url_for('static', filename = 'extraarmeeg.jpg')}} width="450" class="d-block mx-auto"/&gt;
&lt;p class="caption"&gt;&lt;/p&gt;


## Not just a better keyboard

This is where brain-computer interfaces completely dominate current, existing interfaces. While a joystick might allow us to replace our arms with a more powerful and steady robotic arm, a brain-computer interface allows us to use a robotic arm *in addition to the two incredibly versatile arms we already have*. BCIs aren't just better keyboards; they're the only tool that doesn't require us to give up one of our general tools to use a more specific one. 

I'm not saying BCIs will allow us all to be Doc Ock, or that they'll cure disability forever. There'll still be a learning curve, just like for learning to type. But inputs can now occur at the speed of thought, instead of being limited at the speed at which we can move our fingers. We can connect our brains and interact directly with the world around us, instead of having to go through our muscles as middlemen. 

## What it means for us

When computers first became commercially available in the 1960s, they revolutionized the way we did work. Calculations could be done instantly; large spreadsheets of data could be manipulated at scale. But today, despite all the advances we've made in computing, we do work at the same rate as a worker in the 1960s. We're limited fundamentally by the speed of our interface, the keyboard and mouse.

BCIs are currently still in the early stages of development, and won't be feasible for at least another 10 years. But with a working brain-computer interface, we'll open an entirely new field of ways to connect our brains to the world around us. It's time for a new interface. 


&lt;br&gt;&lt;br&gt;

in the bent get get

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 28 Jul 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Advanced addition and speedup of neural network evaluation</title>
      <link>blog/advancedaddition1</link>
      <description>Linear functions and what it means for Neural Network acceleration</description>
      <content:encoded>Hello! I've been doing some more work on the advanced addition project.

[Last time](https://andykong.org/blog/advancedaddition/), I had just created some silly neural networks that added two numbers. The evaluation was trivial, but I was able to make the simple one explainable by boiling it down to the literal equation with the two inputs. I've made progress on this front.

I was able to break down the fully connected 2-10-1 neuron network (Model C) into its respective equation. Evaluated in order, this looks something like this:

&lt;img src={{ url_for('static', filename = 'advancedaddition1symboliceqn.png')}} width="400" class="d-block mx-auto"/&gt;

Since each variable is just a matrix of some size, and I've nicely arranged them in a way that allows multiplication, we can precompute some of the matrix multiplications for the final equation and arrive at a shortened form of the neural network for implementation purposes. You can think of this shortened form as a much easier model to evaluate for low power, discreet package sizes in remote areas without large computing pools. 

The sizes of each variable are not exactly clear from this LaTeX equation, but trust me when I say its just simple matrix multiplication. Nothing fancy. Depending on the initial setup, the final equation for Model C reduces to something like this:

&lt;img src={{ url_for('static', filename = 'advancedaddition1numericeqn.png')}} width="400" class="d-block mx-auto"/&gt;

We can see that the equation is very similar to Model A's original equation from [this post](https://andykong.org/blog/advancedaddition/), despite Model C's **30 multiplications** and **20 additions** against Model A's **two multiplications** and **two additions**. This got me thinking; why can't we do this to every model? Why can't we precompute the matrix multiplications for every neural network, like those built for edge-computing on cameras or speakers?

I thought I was clever for a second, but then I realized that my models would only be able to approximate linear functions. The only features taken for input would be x&lt;subscript&gt;1&lt;/subscript&gt;,x&lt;subscript&gt;2&lt;/subscript&gt;,...x&lt;subscript&gt;n&lt;/subscript&gt;, and they would just come out with some coefficient but no additional features. This is a formidable technique for simpler functions, but more complex hypotheses would require the activation functions (Sigmoid, Rectified Linear Unit, etc), which cannot be precomputed because they depend on the values of their inputs! 

Activation functions could be broken down some more, but I would have to spend a little more time with it. I think that the individual nature of the function means that a simple precomputation wouldn't be allowed. We could threshold the inputs by backtracking the minimum value required to overcome the ReLU or something, and that would save a bit of time.

So, the takeaway is that most neural networks cannot be sped up using precomputation, since every layer usually has a nonlinear activation function. But in any model with multiple matrix multiplications or additions done in succession, the evaluation of the two matrices can be precomputed to speed up the model. 


</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 01 Aug 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Spin Tank Proof-of-Concept Test</title>
      <link>blog/spintank1</link>
      <description>A new configuration for some old kinds of armor</description>
      <content:encoded>The problem with modern day tank armor is that it relies too heavily on brute strength for defense. Armor can be made to actively resist attacks by means of a light, rapidly rotating hull that deflects projectiles instead of the heavy armor plates used today that simply endure every hit thrown at them.


### Modern tanks suck

&lt;img class="d-block mx-auto" src="{{ url_for("static",filename="spintankabrams.png") }}"/&gt;

Tank armor must be strong enough to resist small arms fire (rifles, pistols, submachine guns) and stronger, penetrating projectiles (anti-tank missiles, SAMs). Thick depleted uranium plating forms the outer shell, and sheds small arms fire like water. This plating, in conjunction with ceramic armor and air pockets, also allows tank armor to redirect the focused explosions of anti-tank missiles. Anti-tank missiles first contact the tank, then use shaped charges to shoot a thin, fast-moving jet of plasma at the hull to melt a deep hole into the cockpit. On a normal tank, this stream goes straight the armor and must be redirected by the aforementioned air pockets to avoid injuring the crew.

&lt;img class="d-block mx-auto" src="{{ url_for("static",filename="spintankantitankmissile.png") }}"/&gt;

Not only is this armor dumb (not reactive), it is also super heavy because it takes every impact head-on. I couldn't find definitive stats on the armor weight, but it's designed to be thick and expendable. All these factors make the tank less manouverable than it could be, and more expensive than necessary. This doesn't have to be the case. 

### Spin tank

What if instead of the front-back-sides-bottom-top armor paradigm, we used one, rapidly-spinning shell as the armor? Keep it thick enough that small munitions won't get through, and let the hull's spinning knock away even perfectly-aimed anti-tank missiles. On the spin tank, in the time it takes the anti-tank missile to prime the plasma, the missile's contact point with the tank hull will have shifted along the perimeter. This means the shaped charges will no longer pointed in the right direction, and will detonate harmlessly, spraying plasma along the hull instead of into it. 

&lt;img class="d-block mx-auto" src="{{ url_for("static",filename="spintankmechanismdrawing.png") }}"/&gt;
&lt;p class="caption"&gt;Missiles will just roll right off the spin tank.&lt;/p&gt;


You might be wondering what kind of rotation speeds are necessary to shed missiles like water. Good question. It depends on a series of factors including but not limited to: projectile mass, projectile velocity, tank hull thickness, tank hull strength, and projectile penetration strength. Unfortunately, I cannot currently simulate this in software, so you'll just have to settle for a hardware implementation!

&lt;img class="d-block mx-auto" src="{{ url_for("static",filename="spintank3dmodel.png") }}"/&gt;

&lt;img class="d-block mx-auto" src="{{ url_for("static",filename="spintankmounted.png") }}"/&gt;

I modeled a quick little dome that fits perfectly on a food processor I have, which spins almost too fast for my phone's built-in slo-mo to capture. Manually counting a mark on the rim, I got about 15 rotations per second. Not bad for a consumer appliance! 

I've been using a chopstick in a slingshot to simulate a missile hitting the spin tank. When the dome is stationary, the chopstick has no problem penetrating it. However, the same shot on a spinning dome will leave a dent instead of a hole. 

&lt;img class="d-block mx-auto" src="{{ url_for("static",filename="spintankstationaryshot.png") }}"/&gt;
&lt;p class="caption"&gt;Stationary chopstick firing leaves a hole&lt;/p&gt;

&lt;img class="d-block mx-auto" src="{{ url_for("static",filename="spintankspinningshot.png") }}"/&gt;
&lt;p class="caption"&gt;Spinning chopstick firing leaves a dent in the hull, but less damage&lt;/p&gt;

I need to experiment more with which latitude I'm shooting, since the hull strength changes at different heights. I should also use different lengths and weights of projectiles, since the chopstick is incredibly narrow compared to the dome itself. Some shots on the spinning hull also tore a larger hole than the one on the stationary hull, which I think is caused by a chopstick penetrating the hull followed by the rotation ripping it out.

At the very least, the spin tank reduces the total target size of the same tank, since hitting the edges of the spin tank with a projectile proves to be entirely useless.

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 20 Aug 2019 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Parts arrived for Altered Perceptions</title>
      <link>blog/alteredperceptions0</link>
      <description>How would you like to see the world from a new pair of eyes?</description>
      <content:encoded>Hello. This is Andy. I'll tell you about my new project, and why I need your help coming up with ideas for it. 

Ever since I was young, I have wanted to be able to see the world a bit differently. I'm not sure if I had gotten bored of the way everything looked, or maybe because I knew that I wanted to see old things in a new light. Whether wishing I could change my reds for blues after hearing about "everyone sees colors differently" theory, or wishing I had a real life heads-up display to show me peoples names and subconscious emotions, I always wanted my eyes to be a bit more effective than they were. 

Soon, I hope to make this a reality. Today I received a few boxes in the mail, containing a PlayStation VR, Jetson Nano, Raspicam, and some assorted cables. Can you tell where this is going? I'm going to run the PSVR from the Nano, and wear the Raspicam on the front of the headset to have a POV livestream that I can intercept and play with before sending to the wearer's eyes. The latency might be bad, and the video stream may not trick me into believing its "live live", but it'll emulate a childhood desire of mine. To see things differently with the press of a button. 

We rely so heavily on our eyes for almost everything. Balance, trajectory planning, social interaction, building cool toys, playing video games. The list goes on and on. And soon, I will be able to have more than one pair. Say I wanted to up the contrast of the world, or overlay line detection over my view? Done in a few lines of code. What about the Minecraft nausea effect applied in real time, or even better, Deep Dreaming every frame I see? Easy enough given the level of control over the data that's available. For the first time, I will have granular control over how I see things. It will be glorious.

## If you find this project interesting and/or think that there's some neat way to edit the visual field that I've missed, don't hesitate to email me to let me know what your idea is, and I'll try to implement it (andyking99@gmail.com).
&lt;br&gt;&lt;br&gt;

&lt;ul&gt;Filters/new eyes I'm trying to implement
	&lt;li&gt;rainbow cycling the colors&lt;/li&gt;
	&lt;li&gt;inverting the world&lt;/li&gt;
	&lt;li&gt;deepdreaming, live (like this video):&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/DgPaCWJL7XI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 13 Nov 2019 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Speed Check</title>
      <link>blog/speedcheckwritingstrings</link>
      <description>What's the fastest way to record a stream of strings in Python?</description>
      <content:encoded>
I'm trying to record a stream of string data coming from a Teensy microcontroller over USB Serial. I chose a high baud rate so I can transfer and save the data quickly, but I don't want to drop any elements from the stream while I'm trying to write it down. So I wrote a little test script to see which method is fastest. 

### Results

For each method, I wrote the string "yaga," 10,000 times. 

Setting elements of a prealloc'd Python list works the fastest. I thought writing to a file would take less time, but it's actually around 3x slower. Python append works pretty fast too, but has the overhead of a list of changing size

&lt;img src={{ url_for("static", filename = 'speedcheckStringWrite.png') }} width="400" class="d-block mx-auto"/&gt;

&lt;a href={{ url_for("static", filename="speedcheckStringWrite.py") }}&gt;Link to code&lt;/a&gt; </content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 10 May 2020 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Variable resistance tests of Velostat/Linqstat</title>
      <link>blog/velostatlinqstat</link>
      <description>Testing the effects of bending, twisting, and touch on the conductive plastic Velostat.</description>
      <content:encoded>This is [Velostat](https://en.wikipedia.org/wiki/Velostat):

&lt;img src={{ url_for("static", filename = 'velostat.jpg') }} width="400" class="d-block mx-auto"/&gt;

It's a conductive sheet that's sometimes used for packaging ESD-sensitive electronics, but is commonly used by wearables hobbyists since it changes resistance when &lt;i&gt;anything&lt;/i&gt; happens to it. Wrinkling. Heating. Bending. Twisting. Pressing. Anything that touches it is detectable!

However, there's not a ton of information about it online, like how much change in resistance is visible for bending vs. twisting, etc. So I decide to do some testing and show y'all the results so you wouldn't have to!

All tests were done on a 6.0 x 1.2 cm piece of the material, which I got from Adafruit. I held it in contact using some alligator clips, which weren't super precise contacts. This is probably why the baseline resistances were so variable, around 55 kΩ ± 5 kΩ. 

&lt;img src={{ url_for("static", filename = 'velostatnormal2.jpg') }} width="400" class="d-block mx-auto"/&gt;

&lt;p class="caption"&gt;Basic experimental setup&lt;/p&gt;

### Twisting

&lt;img src={{ url_for("static", filename = 'velostat180.jpg') }} width="400" class="d-block mx-auto"/&gt;

For these tests, I simply flipped one alligator clip and held the strip taut on the table. Best results came from twisting the strip so much that it curled into a tube, around 360 degrees for this width of strip.

&lt;img src={{ url_for("static", filename = 'velostattwist.png') }} width="400" class="d-block mx-auto"/&gt;


### Bending 

&lt;img src={{ url_for("static", filename = 'velostatbend.jpg') }} width="400" class="d-block mx-auto"/&gt;

I bent the strip into a bit of an omega shape, which didn't change it much. When I pinched it, the overall distance of the strip changed so the resistance went down instead of up.

&lt;img src={{ url_for("static", filename = 'velostatbend.png') }} width="400" class="d-block mx-auto"/&gt;


### Pulling

The resistance increased "linearly" from not pulling to maximum pull strength. This test was a bit tricky since my resistance would change the strips if I touched both alligator clips, so I carefully pulled both ends of the plastic without contacting the clips. 

&lt;img src={{ url_for("static", filename = 'velostatpull.png') }} width="400" class="d-block mx-auto"/&gt;

### Touching/Pressure

When it comes to pressure, Velostat is not that great of a sensor without modification. I found that surface area of the finger touch affects the resistance more than pressing on it, and that non-finger touching with a plastic scissor's handle did not affect it much at at all. 

&lt;img src={{ url_for("static", filename = 'velostattouch.png') }} width="400" class="d-block mx-auto"/&gt;


### Touching (Bent sheet)

Since I wanted to use this as a pressure sensor, I tried folding the sheet in the middle then pressing on that. This worked much better, but required securing by tape or some other adhesive. 

&lt;img src={{ url_for("static", filename = 'velostatfoldtouch.png') }} width="400" class="d-block mx-auto"/&gt;

### Heating

[This](https://electronics.stackexchange.com/questions/452291/velostat-sensitivity-to-temperature) site I visited claimed that heat did not affect the sheet's resistance, but I decided to test it out. I held a soldering iron near my test strip without touching it, and found the biggest changes yet! It took a few seconds to heat to the max resistance, and about a minute to cool back to baseline. 

&lt;img src={{ url_for("static", filename = 'velostatheating.png') }} width="400" class="d-block mx-auto"/&gt;

### Connections and Conclusions

I wanted to connect some wire to this stuff, but [some people](https://www.kobakant.at/DIY/?p=381) online were using copper tape with conductive adhesive and conductive thread, which I didn't have. I tried soldering to it, but it just melted :(. Since alligator clips worked, I tried "crimping" some metal wire onto it, and that seemed to work well. Seems like it just needs to be in tight contact with metal to conduct. 

&lt;img src={{ url_for("static", filename = 'velostatcrimp.jpg') }} width="400" class="d-block mx-auto"/&gt;


That's all! I'm probably going to stick with pressure sensing with my folded sheet, it seems to produce pretty drastic resistance changes. 



</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 17 May 2020 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Speed Check (calling Python Built-in functions)</title>
      <link>blog/speedcheckpybuiltins</link>
      <description>Calling str, type, etc. is free... right?</description>
      <content:encoded>I found this cool [blog post](http://blog.kevmod.com/2020/05/python-performance-its-not-just-the-interpreter) that was trying to show Python's slower execution time does not just come from its interpreter, and instead was more interested in the benchmark that they ran converting ints to strings as a toy example.

They mentioned off-hand that built-in function calls in Python must be traced back to the built-in library by the interpreter, which takes a decent amount of time if most of your code just calls built-in functions. Here's the code: 

&lt;img src={{ url_for("static", filename = 'speedcheckpybuiltinscode.png') }} width="400" class="d-block mx-auto"/&gt;

### Results

All we do is move the str() function into a local variable (allowed since Python is a functional language), but we gain 20-30% speed boost in the total execution times. 

&lt;img src={{ url_for("static", filename = 'speedcheckpybuiltinstimings.png') }} width="400" class="d-block mx-auto"/&gt;

Of course, this is a toy example since we're only calling one function, but it might be worthwhile to precompile code like this if you're not uing Pypi or something. </content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 25 May 2020 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Digital Holograms</title>
      <link>blog/digitalholo1</link>
      <description>Displaying 3D images on a 2D computer screen</description>
      <content:encoded>I was reading about holograms and realized that I could emulate it on a computer. Basically, screen objects don't shift perspective like real objects do (showing a different side when you look from the left vs. right, etc.) 

&lt;img class="d-block mx-auto" src={{ url_for("static", filename = 'digitalholocube.png') }} width="400" /&gt;
&lt;p class="caption"&gt;[image credit](https://light2015blogdotorg.wordpress.com/2015/11/05/holography-art-with-light/)&lt;/p&gt;

However, with head tracking, the computer can compute what new angle your eyes are looking from and recalculate the image view. This can be used for looking at CAD models, or on websites for a more 3D experience, or for hiding information on the sides of a usually unviewable object (like a scavenger hunt or something), or creating a head whose eyes literally follow you around. 

I made a little demo of this, and I'm going to do some more work with eye tracking in-browser soon. I know this concept has been done before, just not with a computer's built-in webcam.

&lt;blockquote class="twitter-tweet"&gt;&lt;p lang="en" dir="ltr"&gt;Hologram displayed on flat computer screen &lt;a href="https://t.co/efOuGUr61W"&gt;pic.twitter.com/efOuGUr61W&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy (@redlightguru) &lt;a href="https://twitter.com/redlightguru/status/1271007318927241216?ref_src=twsrc%5Etfw"&gt;June 11, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 18 Jun 2020 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Privacy screen</title>
      <link>blog/privacyscreen</link>
      <description>Only cool kids can see this laptop!</description>
      <content:encoded>
Howdy! I saw a meme when I was in high school that was a picture of a man. Superimposed onto his bottom text was the phrase "This meme is a red square when you're not looking," the joke being that you wouldn't be able to see it if you weren't looking. Anyway, since then I've had this desire to really implement it, to make some kind of billboard/house sign that nonresidents wouldn't be able to see, perhaps to display the weekly dinner schedule or WiFi password.

Using a Raspberry Pi, raspicam, and downloading dlib's face recognition models, we were able to get it to run at &lt;b&gt;0.5 FPS&lt;/b&gt;. When someone it recognized from the "VIP Faces" folder was in view of the camera, it would turn the screen back on. Otherwise, the Raspberry Pi would black out the screen using some display power management functions invoked from the command. 

This FPS is a little useless since it can't black out the screen fast enough, so I ported it over to my Mac. Here, it runs at \~10 FPS, so it can feasibly block out someone trying to view your screen. Another benefit is it blacks out the screen while you aren't looking at it (since it can't see your face), saving power (Assuming the display turn off and turn on don't cost any extra energy, which they probably do). Anyway, it only took a few hours to write, so here's all the code: 

&lt;!-- HTML generated using hilite.me --&gt;&lt;div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"&gt;&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;pre style="margin: 0; line-height: 125%"&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62&lt;/pre&gt;&lt;/td&gt;&lt;td&gt;&lt;pre style="margin: 0; line-height: 125%"&gt;&lt;span style="color: #008800; font-weight: bold"&gt;import&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;face_recognition&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;import&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;cv2&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;import&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;subprocess&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;as&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;sp&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;import&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;os&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;import&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;glob&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;import&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;numpy&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;as&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;np&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;from&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;functools&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;import&lt;/span&gt; &lt;span style="color: #007020"&gt;reduce&lt;/span&gt;

&lt;span style="color: #888888"&gt;# Start webcam stream&lt;/span&gt;
video_capture &lt;span style="color: #333333"&gt;=&lt;/span&gt; cv2&lt;span style="color: #333333"&gt;.&lt;/span&gt;VideoCapture(&lt;span style="color: #0000DD; font-weight: bold"&gt;0&lt;/span&gt;)

&lt;span style="color: #888888"&gt;# Load approved users&lt;/span&gt;
known_face_encodings &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #007020"&gt;dict&lt;/span&gt;()

rootdir &lt;span style="color: #333333"&gt;=&lt;/span&gt; os&lt;span style="color: #333333"&gt;.&lt;/span&gt;path&lt;span style="color: #333333"&gt;.&lt;/span&gt;dirname(__file__) &lt;span style="color: #333333"&gt;+&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;/faces/&amp;quot;&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;for&lt;/span&gt; fl &lt;span style="color: #000000; font-weight: bold"&gt;in&lt;/span&gt; os&lt;span style="color: #333333"&gt;.&lt;/span&gt;listdir(rootdir) :
    &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; fl&lt;span style="color: #333333"&gt;.&lt;/span&gt;endswith(&lt;span style="background-color: #fff0f0"&gt;&amp;quot;.jpg&amp;quot;&lt;/span&gt;) &lt;span style="color: #000000; font-weight: bold"&gt;or&lt;/span&gt; fl&lt;span style="color: #333333"&gt;.&lt;/span&gt;endswith(&lt;span style="background-color: #fff0f0"&gt;&amp;quot;.jpeg&amp;quot;&lt;/span&gt;) &lt;span style="color: #000000; font-weight: bold"&gt;or&lt;/span&gt; fl&lt;span style="color: #333333"&gt;.&lt;/span&gt;endswith(&lt;span style="background-color: #fff0f0"&gt;&amp;quot;.png&amp;quot;&lt;/span&gt;) :
        image &lt;span style="color: #333333"&gt;=&lt;/span&gt; face_recognition&lt;span style="color: #333333"&gt;.&lt;/span&gt;load_image_file(rootdir &lt;span style="color: #333333"&gt;+&lt;/span&gt; fl)
        known_face_encodings[fl] &lt;span style="color: #333333"&gt;=&lt;/span&gt; face_recognition&lt;span style="color: #333333"&gt;.&lt;/span&gt;face_encodings(image)[&lt;span style="color: #0000DD; font-weight: bold"&gt;0&lt;/span&gt;]
known_face_encodings &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #007020"&gt;list&lt;/span&gt;(known_face_encodings&lt;span style="color: #333333"&gt;.&lt;/span&gt;values())

&lt;span style="color: #888888"&gt;# Initialize some variables&lt;/span&gt;
new_face_locations &lt;span style="color: #333333"&gt;=&lt;/span&gt; []
new_face_encodings &lt;span style="color: #333333"&gt;=&lt;/span&gt; []
turned_off &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #007020"&gt;False&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;print&lt;/span&gt;(&lt;span style="background-color: #fff0f0"&gt;&amp;#39;Starting privacy screen&amp;#39;&lt;/span&gt;)

&lt;span style="color: #008800; font-weight: bold"&gt;while&lt;/span&gt; &lt;span style="color: #007020"&gt;True&lt;/span&gt;:
    &lt;span style="color: #888888"&gt;# Grab a single frame of video&lt;/span&gt;
    ret, frame &lt;span style="color: #333333"&gt;=&lt;/span&gt; video_capture&lt;span style="color: #333333"&gt;.&lt;/span&gt;read()

    &lt;span style="color: #888888"&gt;# Resize frame of video to 1/4 size for faster face recognition processing&lt;/span&gt;
    small_frame &lt;span style="color: #333333"&gt;=&lt;/span&gt; cv2&lt;span style="color: #333333"&gt;.&lt;/span&gt;resize(frame, (&lt;span style="color: #0000DD; font-weight: bold"&gt;0&lt;/span&gt;, &lt;span style="color: #0000DD; font-weight: bold"&gt;0&lt;/span&gt;), fx&lt;span style="color: #333333"&gt;=&lt;/span&gt;&lt;span style="color: #6600EE; font-weight: bold"&gt;0.2&lt;/span&gt;, fy&lt;span style="color: #333333"&gt;=&lt;/span&gt;&lt;span style="color: #6600EE; font-weight: bold"&gt;0.2&lt;/span&gt;)

    &lt;span style="color: #888888"&gt;# Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)&lt;/span&gt;
    rgb_small_frame &lt;span style="color: #333333"&gt;=&lt;/span&gt; small_frame[:, :, ::&lt;span style="color: #333333"&gt;-&lt;/span&gt;&lt;span style="color: #0000DD; font-weight: bold"&gt;1&lt;/span&gt;]

    &lt;span style="color: #888888"&gt;# Find all the faces and face encodings in the current frame of video&lt;/span&gt;
    new_face_locations &lt;span style="color: #333333"&gt;=&lt;/span&gt; face_recognition&lt;span style="color: #333333"&gt;.&lt;/span&gt;face_locations(rgb_small_frame)
    new_face_encodings &lt;span style="color: #333333"&gt;=&lt;/span&gt; face_recognition&lt;span style="color: #333333"&gt;.&lt;/span&gt;face_encodings(rgb_small_frame, new_face_locations)

    matched &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #007020"&gt;True&lt;/span&gt;

    &lt;span style="color: #888888"&gt;# Loop over each face found in the frame to see if it&amp;#39;s someone we know.&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;for&lt;/span&gt; new_face_encoding &lt;span style="color: #000000; font-weight: bold"&gt;in&lt;/span&gt; new_face_encodings:
        match &lt;span style="color: #333333"&gt;=&lt;/span&gt; face_recognition&lt;span style="color: #333333"&gt;.&lt;/span&gt;compare_faces(known_face_encodings, new_face_encoding)
        matched &lt;span style="color: #333333"&gt;=&lt;/span&gt; matched &lt;span style="color: #000000; font-weight: bold"&gt;and&lt;/span&gt; &lt;span style="color: #007020"&gt;bool&lt;/span&gt;(&lt;span style="color: #007020"&gt;sum&lt;/span&gt;(match))

    matched &lt;span style="color: #333333"&gt;=&lt;/span&gt; matched &lt;span style="color: #000000; font-weight: bold"&gt;and&lt;/span&gt; (&lt;span style="color: #007020"&gt;len&lt;/span&gt;(new_face_locations) &lt;span style="color: #333333"&gt;&amp;gt;&lt;/span&gt; &lt;span style="color: #0000DD; font-weight: bold"&gt;0&lt;/span&gt;) &lt;span style="color: #888888"&gt;# Make sure someone&amp;#39;s there&lt;/span&gt;


    &lt;span style="color: #888888"&gt;# Depending on the state of matched, toggle the screen on or off&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; &lt;span style="color: #000000; font-weight: bold"&gt;not&lt;/span&gt; matched:
        &lt;span style="color: #888888"&gt;# sp.run([&amp;quot;xset&amp;quot;,&amp;quot;dpms&amp;quot;,&amp;quot;force&amp;quot;,&amp;quot;off&amp;quot;]) # Debian, for Raspi&lt;/span&gt;
        sp&lt;span style="color: #333333"&gt;.&lt;/span&gt;run([&lt;span style="background-color: #fff0f0"&gt;&amp;quot;pmset&amp;quot;&lt;/span&gt;, &lt;span style="background-color: #fff0f0"&gt;&amp;quot;displaysleepnow&amp;quot;&lt;/span&gt;]) &lt;span style="color: #888888"&gt;# MacOS&lt;/span&gt;
        turned_off &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #007020"&gt;True&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;elif&lt;/span&gt; turned_off:
        turned_off &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #007020"&gt;False&lt;/span&gt;
        &lt;span style="color: #888888"&gt;# sp.run([&amp;quot;xset&amp;quot;,&amp;quot;dpms&amp;quot;,&amp;quot;force&amp;quot;,&amp;quot;on&amp;quot;]) # Debian, for Raspi&lt;/span&gt;

        sp&lt;span style="color: #333333"&gt;.&lt;/span&gt;run([&lt;span style="background-color: #fff0f0"&gt;&amp;quot;caffeinate&amp;quot;&lt;/span&gt;, &lt;span style="background-color: #fff0f0"&gt;&amp;quot;-u&amp;quot;&lt;/span&gt;, &lt;span style="background-color: #fff0f0"&gt;&amp;quot;-t&amp;quot;&lt;/span&gt;, &lt;span style="background-color: #fff0f0"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;]) &lt;span style="color: #888888"&gt;# MacOS, has to run twice to be reliable&lt;/span&gt;
        sp&lt;span style="color: #333333"&gt;.&lt;/span&gt;run([&lt;span style="background-color: #fff0f0"&gt;&amp;quot;caffeinate&amp;quot;&lt;/span&gt;, &lt;span style="background-color: #fff0f0"&gt;&amp;quot;-u&amp;quot;&lt;/span&gt;, &lt;span style="background-color: #fff0f0"&gt;&amp;quot;-t&amp;quot;&lt;/span&gt;, &lt;span style="background-color: #fff0f0"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;br&gt;

That's all! Pretty simple program, but taught me some stuff about display power management and the trickiness of installing dlib. Also, writing this post taught me about HTML formatting ([hilite.me](http://hilite.me/) is kinda nice)

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 09 Sep 2020 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Spudging PCB Stencils for Beginners</title>
      <link>blog/spudging</link>
      <description>How to lay out 50 components on a board really fast</description>
      <content:encoded>Hello! This is Andy. I've made some progress on my EMG circuit since last year, and by that I mean I've iterated exactly once. While assembling my second iteration, I decided to bite the bullet and order a PCB stencil so I would have an easier time putting together the board. It was really quite fun, so I've decided to share the process and my thoughts on it. 

You need to have 
- your blank, unpopulated PCB

- the PCB's stencil

- Solder paste

- a credit card or something similar

- other blank, unpopulated PCBs to support the stencil

- tape

- reflow oven or hot-air gun

# Steps

### 1. Tape your PCB down with some of its friends adjacent
&lt;img class="d-block mx-auto" src={{ url_for("static", filename = 'spudge1.JPG') }} width="400" /&gt;&lt;br&gt; 


### 2. Place your solder mask over the PCB and align it until you can't see any of the green solder mask. Then tape it down. 
&lt;img class="d-block mx-auto" src={{ url_for("static", filename = 'spudge2.JPG') }} width="400" /&gt;&lt;br&gt; 


### 3. Squeeze out a bit of solder paste and get your scraper tool 
&lt;img class="d-block mx-auto" src={{ url_for("static", filename = 'spudge3.JPG') }} width="400" /&gt;&lt;br&gt; 


### 4. Squeegee the paste over the board. Push down, hard, to ensure no leakage. Only do this step 1-3 times, any more and you'll leak it out
&lt;img class="d-block mx-auto" src={{ url_for("static", filename = 'spudge4.JPG') }} width="400" /&gt;&lt;br&gt; 

### 5. Check the coverage. If there are some pads that have no grey, touch up those areas locally.
&lt;img class="d-block mx-auto" src={{ url_for("static", filename = 'spudge5.JPG') }} width="400" /&gt;&lt;br&gt; 

### 6. Remove the solder mask tape carefully, then pull it up from one side. Check the footprints, then add your components
&lt;img class="d-block mx-auto" src={{ url_for("static", filename = 'spudge6.JPG') }} width="400" /&gt;&lt;br&gt; 


### 7. If they're ready to go, pop it in the reflow oven and hit start. Enjoy the fumes for a few minutes, then inspect your board!
&lt;img class="d-block mx-auto" src={{ url_for("static", filename = 'spudge7.JPG') }} width="400" /&gt;&lt;br&gt; 


The hardest part is the actual spudging — make sure to apply a lot of pressure, but resist the urge to get solid grey filled in on the pads. Don't go over it more than a few times. You'll have a solid uniform grey coating on all the footprints, but they'll leak out the sides and ruin the pads. That's all for now, be safe!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 14 Sep 2020 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Playstation Eye does not work for MacOS!</title>
      <link>blog/eyetoypscam</link>
      <description>Mac doesn't play nice with weird USB webcams :(</description>
      <content:encoded>I wasted like an hour and a half today trying to get this Playstation 3 camera working on my Mac:

&lt;img src={{ url_for('static', filename = 'playstationeye.jpg')}} width="300" class="d-block mx-auto"/&gt;

Let me tell you how I debugged it.

After plugging in, the USB name identifies it as a USB Webcam, but simply refuses to recognize its data like one. Apparently this is a problem with many USB webcams and Mac, necessitating the invention of the [macam project](http://webcam-osx.sourceforge.net/), which exists specifically for installing drivers for USB webcams. It was only maintained from 2006-2007, explaining why it doesn't have a github repo (git was invented 2005), but good thing those drivers don't change! 

Except I'm running Catalina, and they wrote their code in 32-bit :(. Someone has been kind enough to port it to GitHub as [macam64](https://github.com/smokris/macam64). 

I tried to follow their steps but my CMake version was out of date. I then tried to use homebrew to update my CMake. It took 20 minutes, updating and installing all sorts of other things, and even had the audacity to tell me it had agreed to qt's user agreement on my behalf (threatening that if I disagreed with its decision, I should uninstall QT immediately). When I ran CMake again, I found that the original CMake location was not the one Homebrew had updated. 

I stopped here. I hate dealing with pathing issues. If I have it, just go find it!!! Maybe one day I will muster the vernacular to venture forth, but today I sighed, and gave up on this webcam. 
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 15 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Creating panorama images manually</title>
      <link>blog/panoramas1</link>
      <description>Experiments in holography</description>
      <content:encoded>
Howdy y'all! I think everyone knows what a panorama is so I'll jump right into it. 

## What's going on?

I'm working on a problem where I need to stitch together 4 images in a 2x2 pattern, into a big panorama/composite image. The image sets look something like this: 

![My messy, feature-full desk]({{ url_for('static', filename = 'panoramassplit.png')}})

## How do you do it?

I started from a simpler problem of stitching just two images together. At first I thought I could do some naive image stitching — i.e. find the row for both images that have the least difference, and just splice at that spot. However, I realized that is probably too naive, and decided to do some Googling.

So instead, it turns out, you can find "corners"/keypoints/features within an image that are distinctive in angle/size/shape. Given two images with a bit of overlap, you can just find the features for either one. The feature detectors can also generate some 128 or 32-vector embedding of each keypoint, which is then used to compare each keypoint to every other keypoint using some "distance" metric, usually euclidean distance. 

Afterwards, you'll have some set of key points that are shared between both images, and the XY locations within both images. These XY points can then be correlated to find some affine transform that... OK, too complicated. Basically, imagine your two images each have a triangle in them. You can find the corners in both, figure out which corner in image1 maps to which corner in image2, and skew your view of one of the images so that the triangles will line up. Then all you gotta do is add the images! Here's an example:

![View 1 of our shapes]({{ url_for('static', filename = 'panorama1_view1.png')}})

![View 2 of our shapes]({{ url_for('static', filename = 'panorama1_view2.png')}})

Intuitively, you know that in the 2nd image, you're looking at the 1st image from a different viewpoint. Namely, from the bottom up. You also know that, as an approximation, transforming the bottom square into the top one will sorta transform everything else as well to match the top. 

## Implementation details

1. Find keypoints/descriptors for each image. I used SIFT, then switched to ORB for speed
2. Find matching keypoints between the two images. FLANN (Fast Library for Approximate Nearest Neighbors) works well, and can find the two nearest neighbors. This is useful because it lets us filter out bad matches using a [ratio test](https://docs.opencv.org/3.4/d5/d6f/tutorial_feature_flann_matcher.html).
3. Calculate the homography matrix between the two images using their matching keypoints (homography is the process of mapping one set of vertices to another, like triangle corners to triangle corners). OpenCV has a function `cv2.findHomography()` for this using RANSAC. It isn't deterministic though :(.
4. Apply the homography matrix to one of the images using `cv2.warpPerspective`
5. Combine the images together.

Voila!

![Combined pic]({{ url_for('static', filename = 'panorama1_combined.png')}})


## Problems

##### Seams
There's weird border/seam issues where the images come together, which I anticipated. I didn't control the ISO precisely on my phone, so there's bound to be issues there. Seam finding is for later, but I think it's possible to apply a uniform shift for every added image to make their borders line up.

##### Distortion
You can clearly see which image was first (top left), and which ones were warped and then added on. I think this is a problem because features become harder to detect after a few images get added, and the last one barely makes it on there in a coherent manner at all. I may be able to do some like detect all shared features before, then transform each set one at a time in order to get better results. Otherwise the alignment is all messed up. Problem for later though!

## Bonus
In the course of messing with `cv2.warpPerspective`, I also made some cool glitch art. Enjoy! Until next time ~

![Glitch art]({{ url_for('static', filename = 'panorama1_glitchart.jpg')}})
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 17 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Creating panorama images manually, pt 2!</title>
      <link>blog/panoramas2</link>
      <description>Better stitching by doing something really easy I just forgot to consider</description>
      <content:encoded>Late last night I was greeted in my dreams by a green, ghastly spectre, who said unto me: "Instead of adding 1 image at a time, did you try stitching 2 and 2, then the two composites? This would greatly reduce the distortion suffered by the latter images," in a spooky voice. I said "No I didn't think of that," then woke up and tried it. It immediately looked way better. Here's the results

![Improved stitching]({{ url_for('static', filename = 'panorama2_betterstitch.jpg')}})

I think due to the distortion being lessened, the total features being found are much better for the two pairs than for adding one image at a time. Here's an example of the keypoints being connected across. 

![Keypoints comparison across the new method]({{ url_for('static', filename = 'panorama2_keypointcrossanalysis.jpg')}})

I guess divide and conquer really has its merits. </content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 18 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Histogram Equalization</title>
      <link>blog/histogram</link>
      <description>Using all your dynamic range to make black and white images look sssickkk</description>
      <content:encoded>I recently found out about histogram equalization. Let me tell you about it! So first, 

### What's a histogram?
A histogram is just a graph of the intensity values within an image. To construct it, let's assume you have an 8-bit image whose intensities span 0-255. You create a 256-length array full of zeros. Then, you go through your image pixel by pixel. We use its value as an index, and increment the number in that array position. In the end, you'll have an array of the counts of how many times we've seen each pixel value. Here's an example. 

![Bread Music]({{ url_for('static', filename = 'breadsong.jpg')}})
![Bread Music Histogram]({{ url_for('static', filename = 'breadsong_histogram.png')}})

It allows you to visualize the relative range/span of the pixels in your image, whether the image is mostly darks or mostly lights. You can (hopefully) already see that with your eyes, but this is a more technical way of seeing it. 

### So what's histogram equalization?
Let's look at an overexposed image's histogram. 

![Bread Music]({{ url_for('static', filename = 'breadsong_overexposed.jpg')}})
![Bread Music]({{ url_for('static', filename = 'breadsong_overexposed_histogram.png')}})

You'll see that the histogram is now bunched to the right, brighter side. More importantly, instead of spanning the entire image, the darkest single pixel is pretty far from the left edge. Our pixels now span 100-255. This is bad because we aren't using the entire dynamic range of the image format, but it's also bad because the image just *looks* bad! 

This is where histogram equalization comes in. We can fix this image by re-stretching out the histogram using [this formula](https://en.wikipedia.org/wiki/Histogram_equalization#Implementation). Now the image's pixel values will once again span 0-255 (like the image above). 

The results look amazing for large nature photos like this one [(source)](https://bl.ocks.org/biovisualize/c31c5eb3bf1c5a72bde9):

![Nature photography benefits greatly]({{ url_for('static', filename = 'histeqbeforeafter.png')}})


### But you said it's only for black and white images! That image earlier is clearly colored!

OK, you got me. For ordinary, nature images, you can perform the histogram equalization process for each color channel (RGB) separately, then put them back together into one image. However, for smaller images (100x100 pixels-ish), you may not get good results because the histogram distribution for the 3 colors won't line up well. Here's an example where I histogram equalized my eye, and it looks like I got a black eye. 


&lt;img src="{{ url_for('static', filename = 'histogram_eye.png')}}" style="display:block"/&gt;
&lt;img src="{{ url_for('static', filename = 'histogram_blackeye.png')}}" style="display:block"/&gt;



&lt;p class="caption"&gt;You should see the other guy&lt;/p&gt;

That's all for now. Cya!




</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 20 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Myo3 Assembly</title>
      <link>blog/myo3_assembly</link>
      <description>Putting together the 3rd iteration of my EMG sensor</description>
      <content:encoded>The PCBs came sometime last week, and I got the rest of my resistors and passives today. Time to construct!

My housemates still are quarantining, so I was trying to stay out of the hallways and common areas (like the workshop we have -.-). I moved all parts into my room, but my desk definitely wasn't bright enough. We make do though. Big shoutout to [HTMLBom](https://github.com/openscopeproject/InteractiveHtmlBom) for enabling my board construction, it's a super useful plugin for KiCAD and you should get it immediately.

### Cool parts of the board:

* Using the TLV2624 to create a virtual ground from my single-sided power supply. It's the laziest way to do it, but I never claimed to be able to do power well.

* This board uses the input stage op-amp with optimal noise, assuming dry electrodes (1 MΩ resistance). This is the LME49721, with 4nV/rt(Hz) voltage noise and something like 1 fA/rt(Hz) current noise. Funnily enough, it's an audio amp, with lots of Thad graphs all over its datasheet. If it works it works!

* I made two boards actually. After the initial LME49721, I switch to an op amp that can source a little more current and drive more capacitance (for the analog filtering). 
	* One board uses the OPA4202, which can drive INFINITE capacitive load. I chose it because it can do that.
	* The other uses the POPA4991, which can only drive 1nF cap load at gain=1. I chose it because it only needs supply voltage similar to the LME49721, meaning I can use a LiPO to power the board instead of trying to source 5V somewhere.

* It has pads on the back! This allows you to strap it directly to your arm to measure muscle signals. I got really sick of putting those 3M red dot electrodes all over myself, especially when they left glue residue all over me. 

* Guard ring 😎 (really it's kind of useless cause I didn't put it on the back as well, but it's the thought that counts (also, I don't really care about input bias current, up until 1 pA or so. Then it generates (G=100000)\*1pA\*(1 MΩ skin impedance) = 1 V offset at the end of the circuit, which would definitely pin it to the rails!))

&lt;br&gt;
###Problems I ran into: 

* TSSOP 16 pin is *super* tight, it's hard to spudge properly (took me 3 tries). 

* I only had 0402 and 0803 parts, but guess what footprint size I decided to use?? :)))))) Luckily both work, the 0402 is really stretching to reach both pads and it doesn't tack as well to the solder paste, but it still works. 

* Also I shouldn't have picked a black solder mask, I can hardly see the 0402 resistors

* I didn't have 220nF capacitors at all, so I sorta just... skipped it. Luckily, if you know the board, you can just decide whether or not you need a part. This particular one was part of a passive high pass filter, which came before a 2nd order active high pass at the same cutoff frequency. I just removed the cap and resistor, and bridged the resistor pads afterwards. 

&lt;br&gt;
### Here's some pics!

#### Front after spudging
![Front of my EMG v3 board]({{ url_for('static', filename = 'myo3front.jpg')}})

#### Back of board, with integrated pads
![Back of my EMG v3 board]({{ url_for('static', filename = 'myo3back.png')}})

Gonna spec its noise and stuff soon!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 22 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Speed Check — Generating a list out of a range out of a len</title>
      <link>blog/speedcheck_listlen</link>
      <description>Why does matplotlib set_data not take an iterable :(</description>
      <content:encoded>As part of my [PPG project](https://twitter.com/redlightguru/status/1331513267697676289?s=20), I had to create a live-updating plot. I'm most familiar with the matplotlib library, which has the same color scheme as MATLAB, so I'm using that. One problem is that they try to make a general-purpose plotting tool (matplotlib.pyplot) that also leaves you free to not worry about which window holds which figure and which renderer is drawing what, when. This is really bad because more general tools are more complex to use, and this is definitely the case here.

It's usually imported as `import matplotlib.pyplot as plt`. To create a 2-part plot involves creating Axes, which are done by `plt.subplot(rows, cols)`. Then you write `line = ax.plot(x,y)`, which doesn't seem bad until you try to *use* the `line` object you just created, and then you find out you have to call `line, _ = ax.plot(x,y)` since it gives multiple things back. 

I won't do a full rant since that's not the point, but basically it's annoying, and if you call `ax.plot()` again, it creates a new line that overlaps with the old line. It's a pain. I needed two live-updating plots, so you can save the `line` object as a variable and call `line.set_data(x,y)` to change the data without adding a new line onto the graph. 

Now, my data was time-series, and I just wanted the index of the array as the x axis. So for data like `[0.5, 1.3, 2.4]`, I wanted an x list that was `[1, 2, 3]`. This is easy enough, it's just `list(range(len(data)))`. And no, I tried using just `range(len(data))`. It has to be complete. I also thought of using enumerate, and taking only the first element in a list-comprehension. So I decided to compare them.

This was my first time using the `timeit` library, and as far as I could tell, it was kinda spotty in terms of consistency. Here's the code:

&lt;!-- HTML generated using hilite.me --&gt;&lt;div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"&gt;&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;pre style="margin: 0; line-height: 125%"&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28&lt;/pre&gt;&lt;/td&gt;&lt;td&gt;&lt;pre style="margin: 0; line-height: 125%"&gt;&lt;span style="color: #008800; font-weight: bold"&gt;import&lt;/span&gt; &lt;span style="color: #0e84b5; font-weight: bold"&gt;timeit&lt;/span&gt;

uniqueTimes &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #0000DD; font-weight: bold"&gt;10&lt;/span&gt;
b &lt;span style="color: #333333"&gt;=&lt;/span&gt; [&lt;span style="color: #0000DD; font-weight: bold"&gt;0&lt;/span&gt;]&lt;span style="color: #333333"&gt;*&lt;/span&gt;&lt;span style="color: #0000DD; font-weight: bold"&gt;10000&lt;/span&gt;

&lt;span style="color: #008800; font-weight: bold"&gt;def&lt;/span&gt; &lt;span style="color: #0066BB; font-weight: bold"&gt;lenList&lt;/span&gt;(lst):
    &lt;span style="color: #008800; font-weight: bold"&gt;return&lt;/span&gt; [x1 &lt;span style="color: #008800; font-weight: bold"&gt;for&lt;/span&gt; x1,x2 &lt;span style="color: #000000; font-weight: bold"&gt;in&lt;/span&gt; &lt;span style="color: #007020"&gt;enumerate&lt;/span&gt;(lst)]
&lt;span style="color: #888888"&gt;# 0.7 ns/elem&lt;/span&gt;

&lt;span style="color: #008800; font-weight: bold"&gt;def&lt;/span&gt; &lt;span style="color: #0066BB; font-weight: bold"&gt;lenList2&lt;/span&gt;(lst):
    &lt;span style="color: #008800; font-weight: bold"&gt;return&lt;/span&gt; &lt;span style="color: #007020"&gt;list&lt;/span&gt;(&lt;span style="color: #007020"&gt;range&lt;/span&gt;(&lt;span style="color: #007020"&gt;len&lt;/span&gt;(lst)))
&lt;span style="color: #888888"&gt;# 0.22 ns/elem&lt;/span&gt;

&lt;span style="color: #008800; font-weight: bold"&gt;def&lt;/span&gt; &lt;span style="color: #0066BB; font-weight: bold"&gt;mapList3&lt;/span&gt;(elem, count &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #0000DD; font-weight: bold"&gt;0&lt;/span&gt;):
    count &lt;span style="color: #333333"&gt;+=&lt;/span&gt; &lt;span style="color: #0000DD; font-weight: bold"&gt;1&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;return&lt;/span&gt; count&lt;span style="color: #333333"&gt;-&lt;/span&gt;&lt;span style="color: #0000DD; font-weight: bold"&gt;1&lt;/span&gt;
&lt;span style="color: #888888"&gt;# 3.7 ns/elem&lt;/span&gt;

functs &lt;span style="color: #333333"&gt;=&lt;/span&gt; [&lt;span style="color: #008800; font-weight: bold"&gt;lambda&lt;/span&gt; : lenList(b),
            &lt;span style="color: #008800; font-weight: bold"&gt;lambda&lt;/span&gt; : lenList2(b),
            &lt;span style="color: #008800; font-weight: bold"&gt;lambda&lt;/span&gt; : &lt;span style="color: #007020"&gt;list&lt;/span&gt;(&lt;span style="color: #007020"&gt;map&lt;/span&gt;(mapList3, b))]

timesEach &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="color: #0000DD; font-weight: bold"&gt;1000&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;for&lt;/span&gt; i &lt;span style="color: #000000; font-weight: bold"&gt;in&lt;/span&gt; &lt;span style="color: #007020"&gt;range&lt;/span&gt;(&lt;span style="color: #007020"&gt;len&lt;/span&gt;(functs)):
    times &lt;span style="color: #333333"&gt;=&lt;/span&gt; []
    &lt;span style="color: #008800; font-weight: bold"&gt;for&lt;/span&gt; j &lt;span style="color: #000000; font-weight: bold"&gt;in&lt;/span&gt; &lt;span style="color: #007020"&gt;range&lt;/span&gt;(uniqueTimes):
        times&lt;span style="color: #333333"&gt;.&lt;/span&gt;append(timeit&lt;span style="color: #333333"&gt;.&lt;/span&gt;timeit(functs[i], number&lt;span style="color: #333333"&gt;=&lt;/span&gt;timesEach))
    &lt;span style="color: #008800; font-weight: bold"&gt;print&lt;/span&gt;(i, &lt;span style="background-color: #fff0f0"&gt;&amp;quot;ns/elem:&amp;quot;&lt;/span&gt;, &lt;span style="color: #007020"&gt;sum&lt;/span&gt;(times)&lt;span style="color: #333333"&gt;/&lt;/span&gt;timesEach&lt;span style="color: #333333"&gt;*&lt;/span&gt;&lt;span style="color: #6600EE; font-weight: bold"&gt;1e6&lt;/span&gt;&lt;span style="color: #333333"&gt;/&lt;/span&gt;&lt;span style="color: #007020"&gt;len&lt;/span&gt;(b))
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;


Turns out the naive `list(range(len(x)))` solution was the fastest! Until next time, cya!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 25 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Myo3 initial tests</title>
      <link>blog/myo3_initialtesting</link>
      <description>It's like they always say, the hard part's the software</description>
      <content:encoded>
I finally got around to powering on those EMG circuits I made. 


## Initial test
For an initial test of any circuit, I was taught to flick the power on and then immediately off. If the current indicator spikes, you know there's a path error, but it might not have burnt out the chip just yet. The idea being not enough power has passed through and messed up the delicate silicon in the chip. I did just this flick trick, and saw a massive current jump immediately :(.

I wracked my brains and traces for any mistakes I could've made in the wiring, but failed to find anything. Went back to the schematic, because maybe I had put in a path to ground somehow? Maybe switched out a bypass capacitor for a bypass resistor? I saw nothing of the sort.

A day later, I looked again at the behest of a friend helping me debug, and found this:

![Flipping the positive and negative supplies is a big no-no]({{ url_for('static', filename = 'myo3_schematicflippedpower.png')}})
&lt;p class="caption"&gt;Flipping the positive and negative supplies on anything is a big no-no&lt;/p&gt;

## Fixing the problems
Luckily, I had only messed up on one chip, a quad op-amp. I was able to just rotate it 180° and have all the same inputs and outputs. Unluckily, it was on the TSSOP|16 package. Excuse the flux everywhere, but here's the chip after fixing.

I also had no solder-wicking braid, so had to fashion some out of a flux pen and some stripped, stranded wire. Worked surprisingly well, but went through wire rather quickly compared to the braided stuff. Wire's cheap anyway. 

![Fixed the power on this quad amplifier]({{ url_for('static', filename = 'myo3_fixingpower.jpg')}})


## Actual initial test
My function generator set to 100 Hz sine wave has a minimum output voltage of 20 mV peak-peak. I'm trying to measure a signal approximately 5 uV p-p in size, so I made a voltage divider from a 50kΩ and 10Ω resistor. 20 mV becomes around 4 uV, and I have a high resistance measuring path to simulate the skin impedance. 

### First stage, G = 1000
The sine wave first visibly appears on the output of the INA (first stage) at around 20 uV p-p, standing at around 50 mV p-p for a total gain of ~1000. This is pretty good, but the next stage is saturated at 12 Hz. I believe I accidentally made an oscillator by being greedy and adding gain to a perfectly good 2nd-order Sallen-Key high-pass filter, but I can rectify this by messing with some resistors. 
~

### Shorted inputs noise
With inputs shorted with a wire, there's a little bit of 60Hz line pickup. That aside, the noise p-p appears to be 70 mV after a 1000x gain, putting the initial, full spectrum noise at 70 uV. As it stands, much of the signal is very high frequency noise. After the RFI/EMI filtering it should be much better since the bandwidth gets heavily reduced, down to around 300Hz. 

![Shorted inputs signal on INA output]({{ url_for('static', filename = 'myo3_shortednoise.jpg')}})

I'll post further updates after I fix the filters. No board ever works its first time, but it gets there if you want it to!

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 26 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Teensy DAQ: ADC Oversampling on the Teensy 4.0 microcontroller</title>
      <link>blog/teensydaq</link>
      <description>300 kSPS to 1 kSPS faster than you can say "free bits"</description>
      <content:encoded>I recently wrote a [Teensy Data Acquisition GUI](https://github.com/kongmunist/TeensyDAQ-Fast) for recording data from a microcontroller/sensor onto the computer. It's pretty simple, just a Python QT5 GUI that has a plot of the data as it comes in over the serial and writes it into a list as fast as it can. After you hit 'stop recording', it writes it into a text file. And yes, this order was the fastest. I checked.

I felt wasteful when first using it because I only wanted to get signals at around 1kHz at most, while the Teensy analog-digital converter running at full-tilt with some averaging (noise reduction) still sampled at well over 200 kHz. What to do with these extra samples? Get extra bits, of course!

![Free-running ADC goes at 362kHz]({{ url_for('static', filename = 'teensydaq_ADCoutput.jpeg')}})
&lt;p class="caption"&gt;Free-running Teensy ADC goes at 362kHz — and it's got two of 'em!&lt;/p&gt;


So while you can average 2&lt;sup&gt;n&lt;/sup&gt; samples to reduce the Noise Power by a factor of n, you can't quite do the same with bits of resolution from your ADC. Turns out when you sum two measurements, their noise magnitude grows by a factor of sqrt(n), while your signal magnitude only grows by a factor of n. The SNR only improves by sqrt(n), since n/sqrt(n)... well, you understand. More on that [here](https://en.wikipedia.org/wiki/Oversampling#Resolution). Basically, you need to sample 4 times per bit of extra resolution you want to get. So if you wanted one more bit, you'd oversample at 4x the Nyquist frequency, if you wanted two you'd have to sample at 4&lt;sup&gt;2&lt;/sup&gt;, etc.

However, I'm still not sure exactly how SNR relates to bits of resolution. I understand the math, but not intuitively. It still feels like you could average two binary numbers to get one that has one more bit of info. Maybe it only has 0.5 more bits of extra information? That would definitely correspond to it having "less noise," which it does after averaging... I definitely don't understand completely. 

&lt;br&gt;

### Noise is necessary

Also, I learned that you NEED noise on your ADC input. Luckily microcontroller voltage references are not the most stable thing in the world, so you usually have enough noise on the pin to wiggle the signal up and down already. I read on some Stackoverflow post that if God made you a 10-bit ADC that was perfectly stable, it could not be directly oversampled to get extra bits of resolution. It'd always give the same answer. Some systems couple a sawtooth wave through a capacitor onto the sampling pin as a form of dithering noise, but I think I don't need to do that. 

### So how many bits can we really get?
So the Teensy ADC goes at 362 kHz, and gives 12 bits natively. We can get 16 bits at 362000/256 = 1414 Hz, and maybe one more bit for a 350 Hz sampling freq. Not bad! We've upgraded our ADC for no noticeable loss, besides maybe some noise loss we could've used the averaging for. I think if we use both ADCs to sample, we'd reduce the noise and perhaps even get 17 bits at 700 Hz, but that's a really minor improvement. 

Anyway. I'll try it later, but this is what I've been exploring for now. 

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 27 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Javascript Libraries</title>
      <link>blog/jslibraries</link>
      <description>What do you mean everything is a global???</description>
      <content:encoded>I am writing a rather complex Javascript project at the moment, and needed to include two different Javascript libraries. One is [p5.js](https://p5js.org/), which is just Processing ported to HTML and used for drawing and a lot of visual display stuff. The other is [mlweb](https://mlweb.loria.fr/), which is a machine learning library written in pure Javascript. 

&lt;u&gt;__One of the cursed things about using Javascript is that most libraries are written exclusively as a global import__&lt;/u&gt;, so importing two large libraries is a nightmare if any of their functions overlap. While the ES6 standard (2015 major update to Javascript) recommends libraries encapsulate their functions into their own namespace, older ones or lazy ones still don't do it.

This is annoying in Javascript, but isn't a problem at all in Python. Importing a library by default forces you to use their import name before any function or variable from that library (think `cv2.imread()` from `import cv2`, or `os.walk` from `import os`). 

In my case, `sin` and `tan` and a few other math functions overlapped. Though both functions do the same thing, the problem arises because of the format that they each return. If the p5.js function returns their own P5 class of int or Array, then the mlweb function calling it wouldn't be able to parse its output and crash. 

### Maybe there's already a fix for it?
I couldn't find anything after an hour of looking, so I made a [post about it on SO](https://stackoverflow.com/questions/64730996/how-can-i-include-a-javascript-library-with-a-namespace-without-manually-exporti). It was my first post, and was answered in 4 minutes in the negatory. Turns out there's no way to import Javascript like this: 

`&lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"&gt;&lt;/script&gt;`

And have it callable by module namespace. Wack!

### Is there anything we can do?
For small libraries, it's doable. You can wrap your entire library.js file in a module declaration and then export each function you want to be able to use [(source)](https://www.typescriptlang.org/docs/handbook/namespaces-and-modules.html):

`declare module "SomeModule" {`&lt;br&gt;
&amp;emsp;`	 export function fn(): string;`&lt;br&gt;
`}`


But this doesn't work for large libraries, since there are so many variables and functions and all kinds of stuff, and as far as I know there's no automatic tool for it. However, I think you could write something simple that would just parse a javascript file and export everything for you. 

Now that I think about it, I did something like this for my [Webcam Heartrate](https://andykong.org/projects/heartratemonitor/) project. I ported a C++ package to Node.js using Emscripten, then used Browserify to port that to a script that I could just include. Then to call it, I added some lines around the library to expose it as a variable, and had to reference that variable to call its functions. May be useful for me to go back through and see if I can't throw together a quick solution...

### Final fix
In the end, it turns out p5.js DOES conform to ES6, and can be wrapped in a big function wherever you call its functions and variables to not leak into the global namespace. With this solution, I stopped looking further into it. You can also invoke p5.js whenever you want in your javascript, if the problem is just the order in which the functions overlap (lazier, but easier).

A Javascript encapsulator script would be really useful if Javascript continues being the norm for web stuff and people push more and more code into Javascript. It doesn't seem super hard, but for now there's not much of a fix and I'm busy. Summer project, anyone?
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 28 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Myo3 Noise Specs</title>
      <link>blog/myo3_noisespecs</link>
      <description>1.24µV of noise at 500,000x gain</description>
      <content:encoded>Today I tested the third iteration of my DIY EMG sensor. 

![Fresh myo3 boards from the oven]({{ url_for('static', filename = 'myo3_board.png')}})
&lt;p class="caption"&gt;Fresh myo3 boards from the oven&lt;/p&gt;

### Measuring the total gain
My function generator only goes down to 20 mV p-p, I'm using a 50kΩ and 10Ω resistor to make a voltage divier to divide my input signal by 5000.

I'm using a 200Hz sine wave, since that's the center frequency of all my filtering. Its amplitude is 50 mV p-p. Afte the voltage divider, the signal actually going into the board is a 10 µV p-p sine wave. After the first stage, the output becomes 50 mV on the scope. It's like a magic trick — first we disappear the signal, then we make it reappear again!

I lowered my output amplitude to as low as it could go, 20 mV p-p. This becomes 4 µV p-p after the divider, which becomes 20 mV after the first stage. What happens after the second stage? Glad you asked:

![A choppy 2V sine wave, produced from a 4µV signal]({{ url_for('static', filename = 'myo3_100hz.JPG')}})
&lt;p class="caption"&gt;A choppy 2V sine wave, amplified from a 4µV input signal&lt;/p&gt;

We see that the 20mV signal becomes 2V, a gain of 100. After both stages, the total gain of a myo3 board is 500,000x, or 114 dB of gain. But that's not impressive if the noise is high — so what _is_ the noise?

### Noise measuring setup
I'm connecting the two inputs to the INA, and then grounding them to the center voltage of the power rails. 

![Noise testing setup for myo3]({{ url_for('static', filename = 'myo3_noiseboardsetup.JPG')}})
&lt;p class="caption"&gt;Noise testing setup for myo3&lt;/p&gt;

And here's what the output looks like after both stages:

![Output signal of the myo3 after shorting its inputs. Peak to peak noise is 620 mV]({{ url_for('static', filename = 'myo3_out3noise.JPG')}})
&lt;p class="caption"&gt;Output signal of the myo3 after shorting its inputs. Peak to peak noise is 620 mV&lt;/p&gt;

Looks bad right? Well, let's do the calculation. 

After 500,000x gain, the output noise has a peak-to-peak voltage of 600-900mV. 0.62V/500000 = 0.00000124, or *1.24 µV*. 

The total noise on the inputs is under 2 µV, before any digital filtering or averaging. Wow! And that's across 50kΩ of resistance, meaning its got low current noise AND voltage noise. For reference, that's less than the thermal noise on a 100kΩ resistor at 20C and 1000Hz bandwidth. 

I think this circuit definitely achieves my original goal of making a biosensing board that doesn't have awful noise dwarfing the measurements. We'll see how it fares in a real test when my USB isolator arrives.
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 29 Nov 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Sledding on a table</title>
      <link>blog/sleddingontable</link>
      <description>We had no sled, but we did have an extra folding table</description>
      <content:encoded>First sticking snow in Pittsburgh today! Me and Elio and Jona wanted to sled, but couldn't find a sled equivalent around the house. Cardboard too soggy and small, container lids too flimsy and weak, no detachable thing on those trashcans anymore :(. What to do? We looked around and found this folding table we had!

![Place your bets as to whether this table makes a good sled]({{ url_for('static', filename = 'tableforsledding.JPG')}})
&lt;p class="caption"&gt;Place your bets as to whether this table makes a good sled!&lt;/p&gt;

It works! Video on my instagram [here](https://www.instagram.com/p/CIR1Gn6nJ6u/). Not super well since it's so heavy, but you can lift the front and sort of glide down a hill over the ice instead of just plowing into it. Pretty good!


</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 01 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Exploring USB-A breakout cables</title>
      <link>blog/USBA_exploration</link>
      <description>The normal USB cable</description>
      <content:encoded>Recently I had the need for a 5V battery to power a circuit. 

Lithium polymer/ion batteries are powerful and cheap because of the hobbyist drone market, but come in multiples of 3.7V-4.2V. NiMH (AA and AAA) batteries are plentiful and expensive, but only come in intervals of 1.5V (9V is also composed of 6 similar batteries wrapping together. Who knew?!). These would be fine, except I didn't want to deal with a voltage regulator since this was just a quick test. What to do?

The wall outputs 5V through a phone charger, but I (1) needed it to be wearable and (2) incapable of killing you if all the op amps and resistors shorted in just the right way. 

I had an extra portable phone charger battery I had gotten from a career fair, which output 5V at a capacity of 2200 mAh. Most importantly, I already had it. 

![Career fairs are good for something]({{ url_for('static', filename = 'ThanksIBM.jpg')}})
&lt;p class="caption"&gt;Career fairs are good for something&lt;/p&gt;

I destroyed a USB cable (the other end was this weird flat thing) and plugged it in after charging the battery. Voltmeter confirms, &gt;5V. 

![A multimeter measuring my portable phone battery's voltage]({{ url_for('static', filename = 'USBA_realvoltage.jpg')}})
&lt;p class="caption"&gt;A multimeter measuring my portable phone battery's voltage&lt;/p&gt;

I expected these batteries to be more regulated, though I suppose there aren't many electronics parts that work at 5V that don't work also at 5.2V. 
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 04 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Photoplethysmography (PPG) Tutorial</title>
      <link>blog/PPG_tutorial</link>
      <description>A guide on how to pronounce the word, and how to implement it in Python</description>
      <content:encoded>Here's the code if you want to give it a shot yourself: [gist](https://gist.github.com/kongmunist/ba659019a483117a846dc2101e27f13d). You will need to download the Haar cascades yourself though.

&lt;hr&gt;

Today I'm going to be telling you about this sick computer vision technique that requires only your webcam feed to get your heartrate. And no, I'm not talking about the phone flashlight trick. I'm talking photoplethysmography (PPG)!

Existing cameras on phones and laptops are amazing, their resolution has allowed some really cool sensing techniques besides taking great photos of your Chipotle burrito. Even knowing this, I found this technique hard to believe. 

I was reading about a project from Microsoft Research called [CardioLens](https://www.microsoft.com/en-us/research/project/cardiolens/), which projected people's heartrates onto their faces using just the camera in a Hololens headset. When your heart beats, your blood vessels swell out a little from the sudden pump motion of the heart. According to this paper, we can see this cyclical pumping as the blood vessels swell and shrink just by averaging the intensity of the colors on someone's face. With a normal camera!

![Cardiolens pulse signal](http://alumni.media.mit.edu/~djmcduff/assets/cardiolens/cardiolens_image_2.png)

Nuts, right? I procrastinated trying this project despite how simple it was reported to be, but after reading about it on [Jimmy Newland's website](https://www.jimmynewland.com/wp/about-jimmy/presentations/remote-ppg-gui/), I felt I could give a decent crack at it. 

![Face and eye detection using Haar Cascades]({{ url_for('static', filename = 'ppg.gif')}})
&lt;p class="caption"&gt;Face and eye detection using Haar Cascades&lt;/p&gt;

Setting up a webcam feed from OpenCV is pretty easy, as is using Haar Cascades for face detection. I downsampled the webcam to a 1/16 of the original size to run face detection at 20 FPS, then ran it through SciPy's FFT (technically the power spectral density). Voila! 

![PPG from a crop on my forehead]({{ url_for('static', filename = 'PPG_forehead.png')}})
&lt;p class="caption"&gt;PPG from a crop on my forehead&lt;/p&gt;

Jimmy recommended using the forehead patch, but I got much better signal from a crop of my webcam under my eyes (cheeks are known for their blush). Interestingly enough, I was reading [one of the earlier papers](https://www.osapublishing.org/oe/viewmedia.cfm?uri=oe-16-26-21434&amp;seq=0) on this and they recommended using the green channel and not the red. Very surprising to me, considering most light used in biosensing depends on red light being more permeable in our skin than the other colors. 

The final signal is only 1-2 intensity level changes of the average on my face. It's kinda crazy to me to know that we can get that kind of noise-free resolution after averaging. Amazing what we can do with today's sensing capabilities.

![PPG from a crop under my eyes, with better signal. You can see the fluctuation of my face intensity from the graph itself]({{ url_for('static', filename = 'PPG_forehead.png')}})
&lt;p class="caption"&gt;PPG from a crop under my eyes, with better signal. You can see the fluctuation of my face intensity from the graph itself&lt;/p&gt;

You also have to be holding incredibly still. Any movement changes the lighting on your forehead, which screws up your intensity chart massively. Even the back-and-forth movement caused by your heart beating screws it up, but if you hold super still it works very reliably. 

I want to use this for an art project, but the error arising from movement makes it impossible. Not sure how to fix either, since the error depends on your lighting environment. 

That's all for now. Cya next time!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 07 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Analog Electronics Basics: Scaling Voltage Rails</title>
      <link>blog/analogscalingvoltages</link>
      <description>What to do when your circuit outputs ±5V but your ADC only takes 0-5V</description>
      <content:encoded>The problem I'm going to be talking about today is how to translate an analog signal from one range to another. I have dealt with this problem three times now, each time was more unintuitive than the last so I thought it'd be useful if I wrote down my process for other people to see what's going on and solve their similar problems. 

![Mapping analog voltages between two rails]({{ url_for('static', filename = 'analogscalinggraphic.png')}})
&lt;p class="caption"&gt;Mapping analog voltages between two rails&lt;/p&gt;

&lt;hr&gt;
### An example of this problem
Let's say you're using a complicated device, which takes in some analog/digital signal in order to control some aspect of itself. As an example, let's say it controls its position. You go to measure the device's current position to control what position you actually want it to go to, but discover from the datasheet or your fried microcontroller that the analog signal out from the device has a different rail-to-rail voltage than your microcontroller. It runs at ±15V and your board runs from USB's 0-5V! 

![Mapping of voltages that would fix your problems]({{ url_for('static', filename = 'voltagescalingmath1.png')}})
&lt;p class="caption"&gt;Mapping of voltages that would fix your problems&lt;/p&gt;

You grab a new Arduino from the scrap heap and think about how to fix this problem. So clearly, if you could just divide all signals by six and then add 2.5V, the rails would match and you would be able to lock onto the signal from the microcontroller. Voila! Easy as cake. Implement an adder op-amp, then have another op amp with feedback resistors so that it divides itself by 6. 

However, you can do it using just one op amp. Introducing, the differential op amp!

![Difference op amp, or subtractor op amp schematic]({{ url_for('static', filename = 'diffamp1.png')}})
&lt;p class="caption"&gt;Difference op amp, or subtractor op amp schematic&lt;/p&gt;

### What they don't tell you 

So most of the top Google results that show you how to implement a differential or subtractor op-amp configuration place some limitations on the problem to make it easier to solve. Maybe they say the resistors R1/R3 and R2/R4 have the same ratio, like the first result on Google, the [electronics-tutorials](https://www.electronics-tutorials.ws/opamp/opamp_5.html) site.

![electronics-tutorials subtractor op-amp configuration](https://www.electronicshub.org/wp-content/uploads/2015/01/1.-Differential-amplifier-circuit.jpg)
&lt;p class="caption"&gt;Electronics Tutorials subtractor op-amp configuration&lt;/p&gt;

Or, they might assume that the signal is zero-centered when telling you the gain. Both the site I cited above and the 2nd result on Google [(this site)](https://www.electronicshub.org/differential-amplifier/) assume both of these things. But this isn't necessarily what you're looking for. It doesn't matter if their example op amps solve for 


![Shitty solution of the differential op-amp]({{ url_for('static', filename = 'analogscaling_shitsoln.png')}})

, that doesn't move the center voltage at all. And it gets worse if you want uneven gain on either side of the center, which may not be zero. What to do?

&lt;br&gt;
&lt;hr&gt;

### How to solve this 
Here's the full solution.

![Schematic of the problem statement]({{ url_for('static', filename = 'scalingvoltageschematic.png')}})

Given this circuit above, 


![Full solution to the differential op-amp]({{ url_for('static', filename = 'analogscaling_fullsoln.png')}})

Seems easy, right? It is! And it shouldn't have been so hard for me to figure that out!

### How's it work?
So let's go over it. R1 and R2 are feedback for the Vin, they do the dividing of Vin in the circuit. The tricky part is the Vref, and especially how it interacts with the Vin. It gets subtracted, sure, but if Vref is not zero then it's going to show up in your final output as well. 

You may be saying "Ohh, but Andy, this isn't like a differential op-amp at all! The noninverting input doesn't even have resistors." Well to that I say, how do you expect to make your reference voltage? You'll usually have to make a voltage divider to ground, and voila, the original subtractor structure appears.

### Example please? 
Sure! Let's say we're solving the above mapping: from ±15V rails to a 0-5V range. This op-amp is in an inverting configuration (negative feedback), so the higher initial voltage is going to have to map to the lower voltage on the output, and vice versa. Here's what we're trying to accomplish.

![Mapping of voltages that would fix your problems, pt. 2]({{ url_for('static', filename = 'voltagescalingmath2.png')}})
&lt;p class="caption"&gt;Mapping of voltages that would fix your problems, pt. 2&lt;/p&gt;

We can just plug in our knowns and solve the linear system of equations for Vref and the resistor ratio. Let's say R2/R1 = r just so we can write it more easily. From the first equation, we have Vin = 15V, Vout = 0V, and the second equation we have Vin = -15V and Vout = 5V. So in LaTeX form:

![Equation one]({{ url_for('static', filename = 'analogscale_eq1.png')}})

![Equation two]({{ url_for('static', filename = 'analogscale_eq2.png')}})

Then we just ask Wolfram Alpha! I'm using v for Vref to make it easier to type.

![Wolfram Query]({{ url_for('static', filename = 'wolframquery.png')}})

![Wolfram Answer]({{ url_for('static', filename = 'wolframanswer.png')}})

Great! So our R1 has to equal 6*R2, and our reference voltage should be somewhere around 2.14V. 

### Implementing the resistor ratio and voltage divider
I want to pick realistic resistors because these circuits are usually needed immediately and in real-life, so I usually use a resistor ratio calculator to make this easier. [This site](http://jansson.us/resistors.html) is a godsend. I'm really lazy in real life, so I'm only going to use the single resistor in series option, but the other ones are usually a little or a lot better in terms of error. 

For the resistors with a ratio of 1/6, it seems a good choice is a 56kΩ and 330kΩ resistor. 

![Resistor ratio solving for r]({{ url_for('static', filename = 'analogscaling_r.png')}})

We'll assume we have access to a 5V source since the second set of rails is 0-5V. We can use the handy voltage divider option on the site to solve for this ratio all at once. Looks like a 33kΩ and 47kΩ resistor will do the trick. 

![Resistor ratio solving for Vref]({{ url_for('static', filename = 'analogscaling_vref.png')}})

&lt;br&gt;&lt;hr&gt;
## Does it work?
Here's the part values we picked out in simulation:

![Vout with +15V is near zero, around -0.1V]({{ url_for('static', filename = 'analogscaling_solutionpos.png')}})

![Vout with -15V is almost exactly 5V]({{ url_for('static', filename = 'analogscaling_solutionneg.png')}})

The +15V rail becomes -0.13V, so very close to the 0V we wanted it to be. The -15V rail nails 5V almost exactly. So yea, I'd say they work. 

Sometimes the solution with realistic resistors will need some tuning, because negative voltages will usually damage a circuit, but you can just tune the initial parameters on the initial voltage mapping to be a little tighter and solve the problem again. 

## Closing
Anyway, I encounter this problem all the time, and as I work more with hardware I think this is an integral "glue" circuit that you should master if you're going to work with hardware of various logical and analog levels. That's all for now, cya next time!

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 08 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Playing sounds with an Arduino, speaker, and no amplifier</title>
      <link>blog/arduinodirect2wav</link>
      <description>LM386 audio amplifier? I 'ardly know her!</description>
      <content:encoded>Today in Being Lazy With Andy, I'm gonna show you how to play audio directly from a microcontroller's output pins — no op amp, no transistor.

&lt;p class="caption"&gt;LED for kicks&lt;/p&gt;
![LED for kicks]({{ url_for('static', filename = 'direct2wav_fullsetup.jpg')}})

## Context
I'm currently doing an Easy button hack-apart, which I'll showcase later. As an intermediate step, I wanted to play custom sounds from the speaker. I don't have any audio amplifiers lying around, and besides, there's not a ton of room in the button itself to fit that. I will probably end up using a transistor, but at the moment I just wanted to test the feasability of playing audio without anything except the speaker and Arduino. The results are useable — loud enough to be heard in a bedroom and passable for voices.

It makes sense for a microcontroller to play WAVs — they're already just pressure levels, so you should be able to play each pressure level as a tone with a set time. Luckily many people have thought of this, and have written lovely libraries for it. Primarily, this relies on some Arduino Playground code written by Michael Smith, and the lovely tutorial by [High-Low Tech Group](http://highlowtech.org/?p=1963) at MIT Media Lab. You can also find this library from Arduino's IDE directly by searching PCM Audio in the Tools-&gt;Manage Libraries... window. 

&lt;p class="caption"&gt;PCM Library at the click of a button&lt;/p&gt;
![PCM Library at the click of a button]({{ url_for('static', filename = 'direct2wav_library.png')}})


## Basic Setup Instructions
Hardware-wise you need to connect the GND to GND, and pin 11 of your Arduino to the V+ of the speaker. I actually think the speaker is voltage polarity agnostic, so either direction should work. I also added a 330Ω resistor in series with the speaker to limit the current draw from the Arduino's pins. I think their max safe draw is 20mA, and this speaker is probably 8Ω and would draw 5V/8Ω = 625mA at max output. With the resistor, this is limited to 5V/338 ≈ 15 mA. 

For the software, the starter code from high-low tech should boot directly and work. When it starts, you should hear a lady's voice say "Arduino Demilvinove" or however you spell it. 

### What about custom audio?
I'm glad you asked! This was the fun part. Their example shows a long array of byte values, centered on 128 as their zero point. They recommend that you use an 8-bit, 8000Hz (8kHz) sample rate, mono channel WAV file. What you need to do is record some audio, then convert it into a .wav with those specific settings. You can do this conversion online, or maybe you already have a local program that can do this. 

Once you have your .wav, you'll need to get the byte values onto your clipboard somehow to paste into the PROGMEM sample array from the example, which is nontrivial because the .wav will want to copy as char codes instead of actual byte values.

## How I did it
I used QuickTime to record some brief audio, and then converted the .aifc file to .wav using [this site](https://audio.online-convert.com/convert/aifc-to-wav), which is handy because it has many of the downsampling options I wanted. 

I then tried to use their Processing script to copy a .wav to my clipboard, but could't run it because it was outdated. I tried to modify it in Processing to compile, but for some reason it wouldn't parse the .wav properly :(. Instead, I found [this web version](https://guilhermerodrigues680.github.io/wav2c-online/) of wav2c. Turns out lots of people have solved this problem, it's just very difficult to find a web version that will do it for you in JS. 

Anyway, it gives you a nice long text of values you can copy and paste into your code. I would like this better if it were all on one line, but that might be a personal preference. 

![Image Caption]({{ url_for('static', filename = 'wav2c_online.png')}})

Paste it in the example code, comment out or delete the old line with all the values, and hit start! Your Arduino should play your sound file!


# Additional things you can try
### Optimizing for voice
I'm encoding voice files specifically, so I figured I could increase the contrast in the file to get better volume. I iterated through in Python and found the largest offset from 128, then multiplied all samples' differences with 128 by some large constant and added them back. So something like `[128, 130, 120]` (diff from 128 is `[0, 2, -8]`) becomes `[128, 138, 88]` after a 5x contrast increase. I think I'm just reinventing the wheel here, but this slight modification made my sound files a little more clear. 

### Increasing the playback frequency to 16kHz
You can also downsample your wav to a less extreme 16kHz audio file. This allows higher frequencies to be represented better (something something Nyquist), and also allows voice files to sound better. I will warn you however that it takes twice as much memory, meaning you can hold maybe 2 seconds of audio now instead of 5? It's significant compared to what it originally was, which was not much.

If you use a 16kHz .wav without changing the code, you'll hear a slowed down version of the file. This is because it still thinks you're using an 8kHz file. To hear a 16kHz encoding, you'll need to download the mellis PCM file from the high-low tech blog post. This allows you a lower level access to the PCM code, which lets you change some variables around to playback faster. It sounds a little better too!

### Playback from SD card
I haven't tried this, but I hope to soon!

That's all for now, have fun!

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 13 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Coin vibration motors arrived!</title>
      <link>blog/coinvibrationmotor</link>
      <description>Now I can do "haptic feedback"</description>
      <content:encoded>We were going over a [paper by Disney Research](https://la.disneyresearch.com/publication/surround-haptics-sending-shivers-down-your-spine/) in class last week that said two sufficiently close vibration motors on someone's skin created a sensation in a single spot in between them. Then, you could just vary the power to one or the other and "move" the phantom dot back and forth between the two, essentially creating a vibrating line for the price of two motors!


&lt;p class="caption"&gt;Image from the Disney Paper "Surround Haptics: Sending Shivers Down Your Spine"&lt;/p&gt;
![Image from the Disney Paper "Surround Haptics: Sending Shivers Down Your Spine"]({{ url_for('static', filename = 'surroundhaptics.png')}})

This even extends to two 'virtual' spots, which can create a second phantom vibration spot in their centroid. How cool!

### I had to try it!
So I ordered these tiny coin cell haptics motors off Amazon. 10 for $6, smaller than my pinky fingernail, and powered off 3 volts. Adhesive side and a foamy side. Nominal resistance is 36Ω, I powered them off my Arduino's 3.3V and it worked well. Not insane vibration, but very noticeable! 

The thing started warming up pretty quickly, so I may have to deal with that, but it could just be a lack of current limiting hardware in the way. It also might self-regulate when it gets hot enough, but I don't want it strapped to my arm when I find that out. I'm probably pulling too much current from the Arduino as-is. 

&lt;p class="caption"&gt;Me holding one of the tiny coin vibration motors I ordered, foamy side up&lt;/p&gt;
![Me holding one of the tiny coin vibration motors I ordered, foamy side up]({{ url_for('static', filename = 'coinvibrationmotors.png')}})

I'm gonna try it and let you know how it goes! Be safe!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 14 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Improving noisy EEG data through single channel averaging</title>
      <link>blog/eegsinglechannelavg</link>
      <description>How to get rid of Gaussian noise using a single electrode's measurements</description>
      <content:encoded>So I'm working with some EEG data, right? The kind collected by these caps. I'm gonna show you how to improve your signal-to-noise ratio from single channel EEG, specifically for detecting continuously repeating weak signals.

&lt;p class="caption"&gt;Traditional EEG cap&lt;/p&gt;
![Traditional EEG cap]({{ url_for('static', filename = 'tradeeg.png')}})

## Background you need to know about EEG
Skip this part if you already know about notch filters at 50/60 Hz -.-

EEG (electroencephalography) is a technique for measuring brainwaves emanating from the firing of thousands of neurons under your skull. When used noninvasively, it measures the clumped firings by taking a voltage potential measurement on the surface of the skull. We can use some of these brainwaves that are predictable (or unpredictable) to control devices. These signals, or potentials, are the first brain-computer interfaces. 

## Traditional EEG noise reduction techniques
All EEG data is [super noisy](https://en.wikipedia.org/wiki/P300_(neuroscience)), and I'm not just saying that to excuse the fact that *my data* is super noisy. People solve this by using great analog electrical engineering or by averaging a bunch of different sites on the head. Think about it; if the noise is Gaussian random, and you measure the same spot on the head for the same signal 100 times and average all of them together, then the noise goes down by a factor of 10! Amazing!

Unfortunately, electrodes take linear time to put on, and it's difficult to find 100 spots for electrodes on your head at all, much less at the same spot. So what happens instead? Well, we can repeat a stimulus a few times, and average *those*.

If the stimulus is fast enough, we get to do this a few 10s of times in a few seconds, and reduce our noise by a decent amount. Combining this with multiple amplifiers, and you get almost the same thing as 10x decrease in noise.

## What's the catch? 
The catch is that this technique is usually only applied to triggered potentials like the P300 or increase in alpha wave activity. When the user *activates* a stimulus, like seeing a card or closing their eyes, the trial starts. Then, averaging a few trials, the data becomes usable. This takes forever — a minute or so, and is COMPLETELY UNUSABLE in the real world. 

It's also difficult to do with just one channel, for the reasons mentioned above. Multiple channels just makes it easier

&lt;hr&gt;
## My use case
OK, I admit it. I'm trying to detect the SSVEP, or the steady-state visually evoked potential. Read more about it [here](https://en.wikipedia.org/wiki/Steady_state_visually_evoked_potential). Basically, if you look at a light blinking from 10-20Hz, there's a very clear peak at that frequency in the EEG data measured over your visual cortex. 

However, it's a continuous signal, so I can't "time" when it starts and stops. I'm trying to do single channel SSVEP detection, instead of multi-channel. And, this signal is weak above 15Hz, and I'm looking for it at 35Hz. That's the opposite of most traits that make it easier to find. 

I'll show you my data. I'm looking for the 35Hz signal content of this brainwave. Top graph shows the waveform, measured over 40 sec, and bottom graph shows the power spectral density of the top graph (Basically the Fourier transform).

&lt;p class="caption"&gt;35Hz signal from EEG DROWNED in noise&lt;/p&gt; 
![35Hz signal from EEG DROWNED in noise]({{ url_for('static', filename = '35_full_noavg.png')}})

You can barely see the peak at all, much less differentiate it from any adjacent peak! I mean, it's high, but it's not that high, and it is surrounded by taller brothers. 

What if I told you there was a way I could make that graph into this one?

&lt;p class="caption"&gt;35Hz signal from EEG clear as day&lt;/p&gt; 
![35Hz signal from EEG clear as day]({{ url_for('static', filename = '35_full_04window.png')}})

That's a much nicer looking peak, isn't it? 

## The catch pt 2
I'm using 40 seconds of data here. NO WAY anyone sits still for 40 sec allowing you to collect their brainwaves to control something. But I'm going to show you that this still works with a much smaller window of 3 seconds, which I think makes a usable BCI

&lt;hr&gt;
## How to do single channel noise averaging
This technique takes advantage of the fact that a single channel of this EEG data oscillating at 35Hz repeats itself every second. Well, it repeats itself every 1/35 second to be more exact. What we can do is average over a window that contains an integer multiple of full waveforms (1/5 sec, 1/7 sec, 1/35 sec, 1 sec), and if the signal exists at all it will be amplified, and any other signal will be destroyed. Then we can take the FFT of it, and figure out what frequency we're looking for.

Here's an example using a shorter chunk of data, 3 seconds. We see that with no averaging, the 35 peak rivals the 25Hz peak, and both are dominated by the &lt;20Hz peaks. 

&lt;p class="caption"&gt;A raw 3 second clip from our wave shows no 35Hz prominence&lt;/p&gt; 
![A raw 3 second clip from our wave shows no 35Hz prominence]({{ url_for('static', filename = '35_3sec_noavg.png')}})

However, with averaging, the 35Hz peak is much higher than the 25Hz. While still lower than the high peak, we can just constrain the search space to &gt;20Hz frequencies, and this will give us the answer we want. What else can we do to make ths more apparent?

&lt;p class="caption"&gt;An averaged 3 second clip from our wave shows slightly higher 35Hz peak, but nothing amazing&lt;/p&gt; 
![An averaged 3 second clip from our wave shows slightly higher 35Hz peak, but nothing amazing]({{ url_for('static', filename = '35_3sec_06window.png')}})

Actually, to fix this, you can increase the FFT resolution (not really because of math or whatever it's really just sine interpolation but it looks like it, okay?) by padding the data with zeros! 

&lt;p class="caption"&gt;Animation of the FFT resolution increasing in front of our very eyes by using zero-padding&lt;/p&gt; 
![Animation of the FFT resolution increasing in front of our very eyes by using zero-padding]({{ url_for('static', filename = 'padright.gif')}})

if you change the window size and pad with zeros on both sides, the signal becomes very nicely defined, and the 35Hz is almost trivial to detect.

&lt;p class="caption"&gt;Well-isolated 35Hz signal from our previously noisy-as-hell data using single channel averaging and zero-padding&lt;/p&gt; 
![Well-isolated 35Hz signal from our previously noisy-as-hell data using single channel averaging and zero-padding]({{ url_for('static', filename = '35_padboth2.png')}})

### Caveats pt 3???
These are not cherrypicked examples, they're just the first 3 seconds of my collected data. 

I tried this with other starting positions and it is a bit finicky (all BCI stuff is), but the principle is still sound. For me, if one section didn't work, an adjacent one usually did. 

One problem is other multiple of 5 frequencies also compound when your window is a multiple of that size (I used 0.2 and 0.6 sec windows mostly), though the zero padding helps with that.

### Conclusion
I haven't seen this mentioned in the literature (if you had, please email it to me!). Since the SSVEP is so weak at higher frequencies but the number of waveforms appears more often, this should make high-frequency SSVEP detection much easier. Anyway, I hope you found this useful, and/or interesting. If so, smash that like button. Cya!!

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 18 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Arduino Function Generator: Square Wave with Variable Duty Cycle</title>
      <link>blog/arduinodutycyclesquare</link>
      <description>How to make a free, customizable square wave generator from a microcontroller you already have</description>
      <content:encoded>Redirecting you to the [post...](https://kongmunist.medium.com/	arduino-function-generator-square-wave-with-variable-duty-cycle-c186e53e860a)

&lt;script type="text/javascript"&gt;
	document.location.href = "https://kongmunist.medium.com/	arduino-function-generator-square-wave-with-variable-duty-cycle-c186e53e860a";
&lt;/script&gt;</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 23 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Easy Button Hack Pt. 1</title>
      <link>blog/easybutton1</link>
      <description>Learning about SD cards, transistor audio amps, Arduino sleep and interrupts</description>
      <content:encoded>WOOO. In sort-of order, today I learned: how to read from SD cards, how to play wavs from SD cards, how to boost speaker volume using a transistor, how to limit transistor current using a resistor, how to trigger interrupts for Arduino sleep modes, how to change what shape causes an interrupt, which pins are and AREN'T interrupt pins, how to hijack the button on an Easy button (not easy, actually).

Here's the end result: An Arduino rig that powers off until the button is pressed, then plays a random sound file from the SD card before powering off again. Gonna solder everything tomorrow and move it to battery power. 

![Pictured are the audio amplifier, speaker, Arduino Uno, and SD card]({{ url_for('static', filename = 'easybuttonday1.jpg')}})
&lt;p class="caption"&gt;Pictured are the audio amplifier, speaker, Arduino Uno, and SD card&lt;/p&gt;
&lt;br&gt;&lt;hr&gt;
###Things I learned today that each took me at least an hour to figure out
- SD cards in Arduino can only be a certain length and all caps, called 8.3 format ('YESYESY.WAV' for example)
- Arduinos only have 2 interrupt pins (2, 3), the other ones don't work for hardware interrupts. 
- Transistors require a resistor in-line with the emitter (V+), and make a decent audio amplifier.
- Random library does a clever initialization of seed using analogRead(A0) or another random analog pin. Otherwise it'll be the same set of numbers every time (for me, 1 1 2 4 5).

#### Nifty stuff that I thought you'd like to know
- Internal pullup resistors have variable resistance, around 10-50kΩ or so. 
- The UNO is pin-by-pin compatible with the Nano. 
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 24 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Testing out some high power LEDs</title>
      <link>blog/LEDs3W6piece</link>
      <description>Casting light on far away surfaces using 3W LEDs</description>
      <content:encoded>I bought some cheap, high-power LEDs off Amazon about a week ago. I wanted something really bright that came with a backing, and I didn't want too many. They look like this:

&lt;p class="caption"&gt;Cheap, 3W LEDs with aluminum backing connected to a 9V wall adaptor&lt;/p&gt;
![Cheap, 3W LEDs with aluminum backing connected to a 9V wall adaptor]({{ url_for('static', filename = 'LED3W_poweringsetup.JPG')}})

I have here soldered it to a 9V wall adaptor, which outputs a DC 9V at 0.3A max load. This is precisely what was specified by the Amazon page, though the accuracy of that I cannot be sure. I tried a 5V at first and that failed. VERY BRIGHT! Hurts to look at for any amount of time, at arm's length. Heats up negligibly when turned on for under a minute.

I funneled their light through my projector lens, but just got something like this. I guess I'll need a diffuser or something between the LEDs and lens to get a uniform wall projection.

&lt;p class="caption"&gt;Projected image produced by the LEDs and a lens from a projector clock. All 6 LED segments are separated out, instead of diffused&lt;/p&gt;
![Projected image produced by the LEDs and a lens from a projector clock. All 6 LED segments are separated out, instead of diffused.]({{ url_for('static', filename = 'LED3W_projected.JPG')}})

I tried to diode test them using my trusty multimeter, but it saturated. I then tried using a 2V voltage reading across one LED, and it saturated as well! The voltage drop for these babies was 2.8V each!! I've never seen them so high before, though it's probably to do with needing a lot of current to turn on the LED. </content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 28 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Working with BrainFlow pt. 1</title>
      <link>blog/workingwithbrainflow1</link>
      <description>A new way to stream OpenBCI data to Python</description>
      <content:encoded>TL;DR: The OpenBCI GUI sucks, I show you how to stream [Ganglion](https://shop.openbci.com/products/ganglion-board) data to Python through BrainFlow to make your own GUI. 

&lt;hr&gt;

### Why would you want to make your own?
For whatever reason, the OpenBCI GUI feels clunky to use, and has bad digital filtering. So aesthetics. Also, when I reimplemented the filters in Python Scipy, I got much better FFT peaks, SNR, etc. I wanted to make my own version of the GUI so I could get useful results out of it. 

A month ago, the website said to use a combination of pySerial and their own library, pyOpenBCI. I didn't do it at this time because it looked complicated. I went back today and realized that they deprecated this old guide. Hurrah!

### BrainFlow

They migrated over to this thing called [BrainFlow](https://brainflow.readthedocs.io/en/stable/Examples.html#python-get-data-from-a-board), which had a lot of setup code but only required one library. BrainFlow also offers filtering, and some other stuff that scipy doesn't for 1D data, like wavelet denoising. Now, personally I don't understand wavelet denoising at all, so I am going to do some reading before using that. However, this library is a good thing. It seems like a lot of cheaper hobbyist headsets are already on this platform, and that makes it all the easier to use. Their examples are also excellent, and work (sort-of)

Today, I set up the GUI (which is really just a live plot), and compared the BrainFlow FFT to Scipy's. 

### GUI Setup
I wrangled some matplotlib plots and added them to an animation so they'd update live. Turns out calling `fig.tight_layout()` takes like 80ms, which really slowed down my plotting time (10FPS tops). However, if you take out `fig.tight_layout()` from your animation loop, the plot axes start drawing over themselves! 

&lt;p class="caption"&gt;Plot axes overlapping, making your plot an illegible mess from hell&lt;/p&gt;
![Plot axes overlapping, making your plot an illegible mess from hell]({{ url_for('static', filename = 'brainflowgraphoverwriting.png')}})

Turns out the `tight_layout()` call notifies the figure that the background is stale (literally a boolean attribute of `matplotlib.Figure` called `stale`) and needs to be redrawn. I had left blit=True because I wanted good speed, and I guess that makes the background not redraw every loop. IDK, I read the code and it still didn't make sense. 

I solved this by calling `fig.set_visible(True)`, which conveniently turns `stale=True` and fixes the background. Tada! Now I'm at 50Hz update rate on the graph. Excellent. Now let's see how the filters do

### Brainflow params
BrainFlow requires a `BrainFlowInputParams()` params object to accompany the board id when initializing the "BrainShim" object. This params doesn't need much, but it does need the name of your serial port that your Bluetooth dongle is connected to. This might be hard to find, but I found a one-liner you could run to get the serial port's names as strings. [SO post here](https://stackoverflow.com/questions/12090503/listing-available-com-ports-with-python), but the terminal one-liner is 

```
python3 -m "import serial.tools.list_ports; print([comport.device for comport in serial.tools.list_ports.comports()])"
```

Works if your base Python has serial pip installed.

### FFT fightoff: BrainFlow vs. Scipy
I started off just comparing the FFTs. I used a window of 256, an overlap of 128, and a Blackman Harris window (Scipy's was 256, but I feel like I used that wrong...). The results were.... complicated. 

Sometimes the peaks were comparable, but Scipy's would be much clearer among the taller peaks of noise

&lt;p class="caption"&gt;BrainFlow vs. Scipy PSD/FFT using Welch's method&lt;/p&gt;
![BrainFlow vs. Scipy PSD/FFT using Welch's method]({{ url_for('static', filename = 'brainflowvsscipy3.png')}})

Other times, the BrainFlow plot would have higher peaks for some frequencies, and lower peaks for others. 

&lt;p class="caption"&gt;BrainFlow vs. Scipy PSD/FFT using Welch's method&lt;/p&gt;
![BrainFlow vs. Scipy PSD/FFT using Welch's method]({{ url_for('static', filename = 'brainflowvsscipy4.png')}})

I think Scipy's graphs looked cleaner, even if they were a little noisier. The peaks were higher, which is what I really wanted. I would've liked to quantitatively chosen, but since I just sampled the air, I couldn't get SNR of anything since there was no signal to compare to. Oh well. It's small dice anyway, considering if the signal is awfully noisy we can't get anything out anyway.


Until next time!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 30 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Hello World for Brain-Computer Interfaces</title>
      <link>blog/bcieyeclosedetection</link>
      <description>Detecting alpha wave activity with EEG to predict whether my eyes are open or closed</description>
      <content:encoded>Hello! I'm gonna tell you everything I know about the most consistently evoked signal in all of EEG and how to detect it — the closed-eye [alpha wave](https://en.wikipedia.org/wiki/Alpha_wave) spike.

&lt;blockquote class="twitter-tweet tw-align-center"&gt;&lt;p lang="en" dir="ltr"&gt;Detecting whether eyes are open/closed using &lt;a href="https://twitter.com/hashtag/EEG?src=hash&amp;amp;ref_src=twsrc%5Etfw"&gt;#EEG&lt;/a&gt; alpha wave activity&lt;a href="https://t.co/OnwhTsJSsa"&gt;pic.twitter.com/OnwhTsJSsa&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy (@redlightguru) &lt;a href="https://twitter.com/redlightguru/status/1344538994235875328?ref_src=twsrc%5Etfw"&gt;December 31, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;br&gt;

This alpha wave signal is used to confirm that EEG electrodes are set up to sample brainwaves properly, and not just sampling the atmosphere's noise (which unfortunately looks a lot like passive brainwave activity).

Because the spike in alpha wave activity happens when your eyes are closed, this makes personally seeing it rather hard — you have to do a screen record or otherwise plot the amplitude to be able to see the spike after you've opened your eyes.

## Experimental Setup
Data are being recorded on a single channel of the OpenBCI Ganglion EEG board. The electrode placements are as follows: GND on right earlobe, Channel 1 (-) on left earlobe, and Channel 1 (+) on position Oz of the 10-20 international system. 

Oz is right over the occipital region of the brain, near the bump on the middle-back of your head (this is called the inion). Sample rate is 200Hz, safety from being shocked in the brain by mains insured by the board streaming samples to my laptop over Bluetooth.

&lt;p class="caption"&gt;Picture of the Ganglion board, with two earlobe electrodes and the scalp electrode. It's spiky to go through hair&lt;/p&gt;
![Picture of the Ganglion board, with two earlobe electrodes and the scalp electrode. It's spiky to go through hair]({{ url_for('static', filename = 'alphawaves_electrodes.jpg')}})



Once streamed onto the computer over BrainFlow (I made &lt;a href="../workingwithbrainflow1" target="_blank"&gt;another blog post&lt;/a&gt; about that), the data are filtered using SciPy's IIR filters. Mainly, a 60Hz notch filter and 5-75Hz bandpass filter, with 40dB attenuation for the stopband. These are applied with the `scipy.signal.filtfilt()` function. I don't know if the dB number is like power (div by 20 then take the logarithm) or normal (div by 10 then take the logarithm), but the stopband is completely flat either way so I don't really mind. 

FFT is calculated using [Welch's method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.welch.html). Alpha band integration is as simple as adding up all the magnitude contributions from frequencies in the band 7-13Hz, but Brainflow has a built-in function for doing this (if you use this, make sure you feed in the tuple as (power, freqs) instead of the (freqs, power) that `scipy.signal.welch()` gives you). That's it!

Implementation: I'm creating a live plot using a matplotlib animation. The indicator is drawn on-graph as both text and an emoji. 

## Results
Eyes closed are pretty reliably detected by a spike, with a lag of about a second. Eyes opening, the alpha waves drop about as fast as they rise. This is probably due to my EEG data window being 5 seconds, since I'm not doing any averaging that would otherwise slow it down. 

&lt;p class="caption"&gt;Graph of baseline alpha wave activity&lt;/p&gt;
![Graph of baseline alpha wave activity]({{ url_for('static', filename = 'alphawaves_eyesopen.png')}})

&lt;p class="caption"&gt;Vastly increased alpha wave activity when user's eyes are closed&lt;/p&gt;
![Vastly increased alpha wave activity when user's eyes are closed]({{ url_for('static', filename = 'alphawaves_eyesclosed.png')}})

## Reliability problems
*Within session*, I had no problems with reliability. I saw my average alpha activity around 8 uV^2, and it went above 12 or so with my eyes closed so I set that as the breakpoint. However, alpha activity also decreases with increase in drowsiness, and I finished this pretty late last night, so it could not work for me right now (not drowsy) the way the breakpoints are set.

I believe the baseline alpha activity varies from person to person, so the threshold would probably have to be adjusted for new people. 

I think that's it for now. Follow me on [Twitter](https://twitter.com/redlightguru)!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 31 Dec 2020 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Fresnel Lens</title>
      <link>blog/fresnellens</link>
      <description>Making flat lenses using special surface ridging</description>
      <content:encoded>Hi! Today I'm going to tell you a bit about these really cool lenses that use angled surfaces to achieve magnification and other lens effects in a flat form factor instead of a curved one. I also show you some big ones I bought, and what they're useful for. We're talking about [Fresnel Lenses](https://en.wikipedia.org/wiki/Fresnel_lens)!

&lt;p class="caption"&gt;Fresnel lens versus regular convex lens&lt;/p&gt;
![Caption](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Fresnel_lens.svg/242px-Fresnel_lens.svg.png)

## Lighthouses and Firemaking

They were originally created for use in lighthouses, avoiding the bulky size required by normal lenses. My first experience of one was a credit card-sized one, which my friend kept in his wallet in case he was stranded and needed to start a fire. Even though the day was sunny and we found dry kindling, the low-powered lens had difficulty lighting our kindling. It burnt the loose fibers easily, but created no flame.

&lt;p class="caption"&gt;Fresnel lens in the shape of a credit card, for easy carrying around in the wallet&lt;/p&gt;
![Fresnel lens in the shape of a credit card, for easy carrying around in the wallet]({{ url_for('static', filename = 'fresnel_creditcard.jpg')}})

### Projection and Magnification

I needed a lens for a project, and I stumbled upon an Amazon item that was just a big flat sheet Fresnel lens for like $3. It should have all the same properties as a normal lens, just in a flat package, so I bought it immediately. Anyway, here it is. 

&lt;p class="caption"&gt;Fresnel lens I bought magnifying office items&lt;/p&gt;
![Fresnel lens I bought magnifying office items]({{ url_for('static', filename = 'fresnel_magnify.jpg')}})

As you can see, it's like a normal sheet of printer paper in size, and works quite well as a magnifier. I think this specific one is used to help old people read better. 

There's a few distortion effects since the sheet isn't rigid, but those can be worked out by holding it better. The instructions provided also state that the lens works better from one side than the other, which to me seems to violate [Helmholtz Reciprocity](https://en.wikipedia.org/wiki/Helmholtz_reciprocity). This says that the start and end of a light ray can be reversed without any effect — sending a ray down its start angle will always lead to the end, and sending the ray up its ending angle will always go back to its start. But what do I know? Maybe it's because the ridges are only on one side of the lens. 

The lens also works for projecting light sources. Here I am holding it up to my ceiling light, and it creates a mirror image of it quite well. 

&lt;p class="caption"&gt;Fresnel lens projecting an image of my ceiling lamp onto the wall&lt;/p&gt;
![Fresnel lens projecting an image of my ceiling lamp onto the wall]({{ url_for('static', filename = 'fresnel_reflectproject.jpg')}})

I'm going to use this to try and make a wall projection. Cya!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 05 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Preliminary 5W LED Tests</title>
      <link>blog/5WLEDprelimtests</link>
      <description>Projecting 5W of light onto a wall — DIY Projector</description>
      <content:encoded>I show you tests of some 5W LEDs I bought off Amazon, and how they look projected onto a wall that's far away.

## Setup
I got 10 of these yellow LEDs with legs in the mail, along with some aluminum backing plates. It's inordinately hard to find high power LEDs with the backing plates attached, but they're so cheap I just caved and bought them separately. 

&lt;p class="caption"&gt;Setup of the 5W LEDs. Each LED has a drop of 6-7V, so I am using two in series to test them with my 12V power supply&lt;/p&gt;
![Setup of the 5W LEDs. Each LED has a drop of 6-7V, so I am using two in series to test them with my 12V power supply, which is just a laptop wall brick.]({{ url_for('static', filename = '5Watters.jpg')}})

I'm using a laptop wall brick, which outputs 5V and 12V at 1.5A each. Kinda cool, high output current bricks are a little hard to find. I always feel bad cutting the plug to get to the wires too... 

Anyway. I soldered on the two LEDs, no thermal paste just mechanical contact for now. I'll probably mount it on a larger heatsink, because this thing gets HOT. The circular indents around the edge look like they're for screws to hold in place. 


## Projection
You remember those [Fresnel lenses](../fresnellens) I bought a while back? This is what they were for. Here I got my brother to hold the sheet above the LEDs, and as he moved it up and down, the projected spot on the ceiling changed size and brightness. 

&lt;p class="caption"&gt;5W LED (very bright) being held under a Fresnel lens from my last post. I'm holding by the alligator clip since it's very hot.&lt;/p&gt;
![5W LED (very bright) being held under a Fresnel lens from my last post. I'm holding by the alligator clip since it's very hot.]({{ url_for('static', filename = '5W_setup.jpg')}})

&lt;p class="caption"&gt;Far from the LEDs&lt;/p&gt;
![Fresnel lens held far]({{ url_for('static', filename = '5W_tightcircles.jpg')}})

&lt;p class="caption"&gt;Middle far&lt;/p&gt;
![Fresnel lens held in the middle]({{ url_for('static', filename = '5W_circles.jpg')}})

&lt;p class="caption"&gt;Close to the LEDs&lt;/p&gt;
![Fresnel lens held close]({{ url_for('static', filename = '5W_bigcircle.jpg')}})

We only turned them on for about a minute, but the backplate hurt immediately to the touch. Definitely need to heatsink them, and I may even have some thermal paste from a computer build a while ago.

The projected spot is massive just from the floor to ceiling, not to mention one wall to another. It could be even brighter if I funnel the light using some aluminum foil or some mirrors. 

One problem is that the Fresnel lens has distortion issues, and doesn't focus very well at all. The edges of shadows projected are all blurry, probably partially because of diffraction around my finger. Maybe I should collimate the light somehow before I shine it through the screen (I want this to show text eventually), but we'll see what problems I run into with the lens I currently have before I come up with new solutions.

Hopefully it'll be alright. Cya next time!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 06 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>New EEG Technique: Phase-rectified signal averaging</title>
      <link>blog/PRSAhighlight</link>
      <description>Basic method of aligning waves helps when averaging out noise from long time-series</description>
      <content:encoded>Today I'm going to tell you about the technique of phase-rectified signal averaging, or PRSA, applied to EEG signals. I stumbled upon [this](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.879&amp;rep=rep1&amp;type=pdf) paper talking about it. Originally the technique was applied to like astronomy data or something, you can find that here, ["Phase-rectified signal averaging detects quasi-periodicities in non-stationary
data"](https://www.sciencedirect.com/science/article/pii/S037843710501006X). This guy who used to be at Philips Research wrote it, he now works at some Mattress company's research department doing sleep EEG to determine how comfy their products are. Interesting career path.

## What's all this PRSA stuff anyway?
Anyway, onto the good stuff. Basically, sometimes your SSVEP signal is dispersed along a time series — maybe it's not always coherent, and it comes and goes and the phase is not directly sequential. When you average your data, you find that a lot of the signal was overlapping, and not in the good way. It averages to zero, and your signal is worse than the noise previously was. 


![Graphs from original PRSA paper]({{ url_for('static', filename = 'prsaoriginal.png')}})

This technique relies on the waveform *usually* lining up within the noisy signal. Which is to say, when the waveform of interest goes up, the noise+signal overall should go up too, at least for a majority of the times you check it. So you average a big window around all "up" or "down" points, and might be able to pull out the original signal. The original paper uses some jank original signal, so it looks atrocious even after averaging. I'm told this was on purpose. Why couldn't they just use a sine wave?

I can't wait to try it. Looks like it won't work, but it seems stupid enough that it just might.

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 13 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>The first generation of handheld barcode scanners — with laser tubes</title>
      <link>blog/barcodehistory</link>
      <description>History of the LS1000 and MH290, and a demo of the 2nd!</description>
      <content:encoded>Hi! Today we have a history lesson! I'm going to tell you the history of the handheld barcode scanner, and how to use one if you have one lying around. 

## History
All but ubiquitous today, the first barcode scanners were stationary and built into the checkout counter. This made it difficult to scan large items, which couldn't easily be passed above the desk. Though laser diodes were invented shortly after the laser (1962, 1960), they were not mass produced until the late 1980s, so these early barcode scanners had to make do with laser tubes — big, fragile, bulky things that are usually around a foot long. So how did they make them handheld?

&lt;p class="caption"&gt;Original LS1000 barcode scanner versus modern day barcode scanners. The laser tube in the LS1000 is horizontal and takes up most of the length, while the laser diode powering the 2nd is smaller than your pinky.&lt;/p&gt;
![Original LS1000 barcode scanner versus modern day barcode scanners. The laser tube in the LS1000 is horizontal and takes up most of the length, while the laser diode powering the 2nd is smaller than your pinky.]({{ url_for('static', filename='barcodescannerls1000.jpg')}})

With Symbol Technologies, Dr. Jerry Swartz performed calculations in the late 70s and believed that the laser tubes could be miniaturized to 5-6 inches, which were short enough to fit into a handheld scanner. The first company he asked to manufacture them laughed him out of the room, but the second company, Uniphase, believed it could be done. Uniphase later overtook that first company, Spectra-Physics, as the largest manufacture of HeNe laser tubes. The barcode scanner that they created with the shortened laser tube was the LS1000, pictured above. 

Metrologic followed shortly after and released their handheld barcode scanner. They mounted the laser tube along the handle instead along the top shaft, and achieved a much more compact size. 

&lt;p class="caption"&gt;The Metrologic 290 handheld barcode scanner. The laser tube is hidden in the handle&lt;/p&gt;
![The Metrologic 290 handheld barcode scanner. The laser tube is hidden in the handle]({{ url_for('static', filename='mh290.jpg')}})

After some fundamental problems of room-temperature lasing and stable output were solved, Japan began mass-manufacture of compact laser diodes in the early 80s. Symbol began incorporating these diodes into their products, producing much smaller barcode scanners due to the lack of massive HeNe tube to produce the laser. These lead to the kinds of barcode scanners today, pictured above next to the LS1000.

## Where do you find them now?
Today, these early barcode scanners are hard to find and fairly expensive when you do find them. They're all pretty bulky due to the laser tube, and fragile because of it. Nobody manufactures them anymore, so you'll have to look on resale markets — usually you can find one for $40-$100 on eBay. You can tell it has a tube because of the size — smaller, compact models can't possibly house the 5 inch laser tube that the earlier barcode scanners housed. 

In the past, I found a bulk sale of 17 MH290s and purchased them, and am currently reselling in individual quantities on eBay [here](https://www.ebay.com/itm/Metrologic-MH290-Barcode-Scanner-HeNe-Laser-inside/363080135269). I think everyone who buys old barcode scanners also knows there's a laser tube inside them, because why else would you be buying such an old barcode scanner? It works the same as a new barcode scanner, just bigger and dirtier. 

## What do you do with them?
I guess you could open a vintage grocery if you really wanted, but I wanted the laser tube from within! While reading Sam's Laser FAQ, I found out about the MH290 for the first time and that's what started this whole obsession.

&lt;p class="caption"&gt;The laser tube from the MH290 lasing.&lt;/p&gt;
![The laser tube from the MH290 lasing.]({{ url_for('static', filename='barcodescannerlaseron.png')}})

Theoretically, [this guide](http://www.repairfaq.org/sam/sale/henemll1.htm) for the MH290 from Sam's Laser FAQ tells you everything you need to know. Practically there's a few considerations you need to think about, like the current draw being a few amp. A 15V wall adaptor will work, and only requires a bit of soldering to get a wire to trigger the PWM-on pin on the DIP IC that drives the laser. If you use an weak power supply or wall brick which can't supply enough current, you'll notice a slight flickering and buzzing to the laser which makes it look unstable but also *cool* at the same time.

This is the first HeNe laser I've owned, and it's probably useful for science. Let me know if you know of anything cool I can do with one!

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;hr&gt;
### References
[1] [Sam's Repair FAQ guide to powering on MH290](http://www.repairfaq.org/sam/sale/henemll1.htm)

[2] [Scholarpedia entry for bar code scanning](http://www.scholarpedia.org/article/Bar_code_scanning)

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 14 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>EEG fails in the prescence of household EMI</title>
      <link>blog/EMIvsBCI</link>
      <description>I am beginning to have doubts on the feasibility of everday BCIs...</description>
      <content:encoded>Hello!

Yesterday I ran several experiments on my EEG (electroencephalography) headset. You always start with something you know is working — in this case, I checked the impedance (low, 10kΩ instead of the usally 400kΩ), then performed the closed-eye alpha spike test (check [here](../bcieyeclosedetection) if you don't know what I'm talking about). It worked, so I moved on to trying to evoke an SSVEP with a 15Hz flashing light. I had just built this structure for blinking 3W LEDs that looks vaguely like an apartment front architecture student would have to design in a class, and it worked wonderfully. The SSVEP is evoked most strongly from 10-20Hz, with a peak around 15Hz. I used 15Hz, and the peak was as large as the closed-eye alpha waves, which are the strongest EEG signal discovered. Basically, it was working fine. 

&lt;p class="caption"&gt;Picture of the apartment-shaped blinker, Arduino controlled MOSFETs and a CC source made from 1W resistors and an LM317 to drive the big LEDs.&lt;/p&gt;
![Picture of the apartment-shaped blinker, Arduino controlled MOSFETs and a CC source made from 1W resistors and an LM317 to drive the big LEDs.]({{ url_for('static', filename = 'aptblinker.jpg')}})

At this point my laptop began running low, so I plugged it into the wall. The next 2 hours I spent running experiments, trying new signal averaging techniques. However, none of them seemed to be working — there was this constant randomness to the signal. Since I had just checked whether the headset measured EEG properly, I thought for sure that new technique just didn't work. I made the experiments simpler and simpler, and eventually I went back to doing the closed-eye thing. Guess what? Our nice consistent graphs had tanked, and were now noisy as hell with the peak barely discernible. 

I started considering environmental factors, like EMI emission from the things near me. I knew the apartment blinker setup didn't throw off that much EMI — I had put it across the table, and checked to make sure that the 15Hz didn't just spike all the time. Since it had worked earlier, I knew the EMI pick-up was much lower than the SSVEP signal (which is very small, under 10 microvolts). My laptop finished charging, so I unplugged it and tried my alpha test again. And it worked perfectly :(. It was the charger! 

&lt;p class="caption"&gt;Still from the video "Removing RFI Noise from MacBook Power Supply
"&lt;/p&gt;
![Still from the video "Removing RFI Noise from MacBook Power Supply
"]({{ url_for('static', filename = 'macbookrfi.jpg')}})

I'm no expert, but modern laptop chargers pass a huge amount of DC current from the AC wall, and usually convert it using a switching power converter instead of a linear DC converter. They're usually more efficient (80% vs 60%), which you care about when you're passing 80+ watts to charge a computer battery. However, they generate a lot of high frequency noise because they have to switch constantly, on and off, to produce a stable current. Most of my info here I read a [teardown by Ken Shirriff](http://www.righto.com/2015/11/macbook-charger-teardown-surprising.html), but I also found a [Youtube video](https://t.co/cDLqXJmekF?amp=1) which shows how to reduce EMI produced by a Macbook MagSafe (though an older version). 

Though the EEG board I'm using doesn't have a physical connection to my laptop, it did sit right next to my laptop as it charged. The Macbook grounds itself to its chassis, which means that there should be some radiated EMI from the casing itself to nearby devices (I can see this on the EEG output as increased 60Hz noise when I touch my laptop — even when it's not plugged into the wall!). Since EEG is so sensitive, I think the proximity to my charging laptop is what did my signal in ;(. Wack!

## Discussion
It's fine that I have to redo the experiments, but my main concern is for the usefulness of EEG in daily settings. I mean, brain-computer interfaces are the future of HCI (in my opinion); they're useless if they can't work in the presence of everyday electrical noise.

I started using the SSVEP because it's a stable, high bandwidth signal, but if the alpha spike can't even function in the presence of noise then I don't know what can. 

&lt;p class="caption"&gt;The alpha wave spike when your eyes close is one of the strongest EEG signals&lt;/p&gt;
![The alpha wave spike when your eyes close is one of the strongest EEG signals]({{ url_for('static', filename = 'emialphaspike.png')}})

Maybe if we shielded the board it'd be slightly better, but even then the cables have a decent amount of pickup, as does my conductive, sacks-of-saltwater body. Maybe shielded cables and boards will have to be the norm? But then if a person touches the box, it'll still couple noise into the whole system. It seems intractable, considering our power systems are too ingrained to change now, and our brain signals aren't getting any stronger (In fact, if you go on Facebook, they seem to be getting weaker... /s). The future of BCI may not last very long...

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 17 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Trying out various peak detection techniques for the SSVEP</title>
      <link>blog/ssveppeakdetect</link>
      <description>Garbage in, garbage out.</description>
      <content:encoded>Today I tried a bunch of techniques to try and detect SSVEP peaks in long streams of EEG data. 

My data came from three trials were all around 40 seconds long, and involved me staring at an LED blinking at a duty cycle of 50% and frequencies of 18, 35, and 65Hz. The techniques I used were (broadly): single-channel averaging (SCA), phase-rectified signal averaging (PRSA), autocorrelation (AC), and plain ol' power spectral distribution (PSD). Lots of three letter agencies. 


## Procedure
### Data
Since each data file I had was 40 seconds, I first confirmed that using the entire length of data (after a 60Hz notch and a 5-95 bandpass filter) with each technique showed a prominent peak at the frequency of stimulation, as a baseline. If using the entire length of data didn't work, then I was really screwed trying to detect it using less data. The only data to not show a peak at the stim frequency was the 65Hz data, which makes sense because of the small signal size since frequencies past 20 begin to attenuate. The 18Hz was the largest, and the 35Hz followed.

### Sliding windows
For practical applications, I'd only be able to record about 5 seconds of data from the EEG before the user gets tired of staring at blinking lights. I wrote a neat function that chunked the 40 second data into overlapping, 5 second data windows. The function then runs one of the above peak detection methods on the 5 second window, and records the peak before moving on to the next 5 second window. This way, I get to see how the techniques fare across all 40 seconds, but still use a realistic time window. These graphs are longitudinal, and they'll be the focus of this post. 

We want to see a clear horizontal line at the frequency of interest (FOI) (either 18 or 35 Hz). I'm summing how many times each frequency shows up on the graph, and printing them out in order. I'm defining "SNR" as the number of times the frequency of interest shows up divided by the number of times the next, far-off frequency shows up (so 34 and 33 don't count for a 35Hz peak, but 30Hz does). Let's get started!

## Experiments
I bet you're wondering what experiments I ran today! I'm glad to show you the graphs. 

### How do SCA, PRSA, and PSD stack up to each other?
The real question! Let's take a look! All peak detection was performed on a 5 sec sliding window over the entire data stream. Peaks are just frequency with the the max power between 10 and 60Hz. Shown here is the 35Hz data. Specifics of each technique are given below.

- PSD: uses scipy.signal.welch, default options

- SCA: 200 sample averaging window, non-overlapping. Welch used after averaging to get PSD.

- PRSA: 200-wide window, anchor points determined using T=2 (next two points had to be &gt; than last two points). Welch used afterwards for PSD. 


&lt;p class="caption"&gt;Graph of power spectral distribution of a 35Hz signal, fed in 5s increments and peak detected from 10-60Hz&lt;/p&gt;
![Graph of power spectral distribution of a 35Hz signal, fed in 5s increments and peak detected from 10-60Hz]({{ url_for('static', filename = 'PSD5secwindow.png')}})

&lt;p class="caption"&gt;Graph of power spectral distribution of a single-channel averaged 35Hz signal, split in 5s increments and peak detected from 10-60Hz&lt;/p&gt;
![Graph of power spectral distribution of a single-channel averaged 35Hz signal, split in 5s increments and peak detected from 10-60Hz]({{ url_for('static', filename = 'SCA5secwindow.png')}})

&lt;p class="caption"&gt;Graph of power spectral distribution of a PRSA 35Hz signal, split in 5s increments and peak detected from 10-60Hz&lt;/p&gt;
![Graph of power spectral distribution of a PRSA 35Hz signal, split in 5s increments and peak detected from 10-60Hz]({{ url_for('static', filename = 'PRSA5secwindow.png')}})

The SNRs are as follows, though from the graph it's pretty clear which one has the best line around 35 Hz (It's the PRSA, the line is clearly there and it's not as noisy as SCA).

- PSD              &amp;emsp;&amp;emsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;          # SNR 2.8 at 10 sec, &amp;ensp;&amp;emsp;  1.0 at 5 sec

- SCA  &amp;emsp;&amp;emsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; # SNR 3.7 at 10 sec,  &amp;ensp;&amp;emsp; 2.0 at 5 sec

- PRSA      &amp;emsp;&amp;emsp;                 # SNR 24.3 at 10 sec, &amp;emsp; 2.8 at 5 sec


Bonus - Autocorrelation + PSD (Not shown)      # SNR 1.35 at 10 sec, 1.1 at 5 sec

Here's the picture of the PRSA working on a 10 second window of data. It's almost a solid bar, concentrated around 35Hz. This was the best result I got.

&lt;p class="caption"&gt;Graph of power spectral distribution of a PRSA 35Hz signal, split in 10s increments and peak detected from 10-60Hz. Notice how much nicer it is than the noisy messes above.&lt;/p&gt;
![Graph of power spectral distribution of a PRSA 35Hz signal, split in 10s increments and peak detected from 10-60Hz. Notice how much nicer it is than the noisy messes above.]({{ url_for('static', filename = 'PRSA10secwindow.png')}})

### Does order of filtering matter for single-channel averaging?
No, except for the notch filter. Here, the blue line shows filtering before averaging, and the orange shows filtering afterwards. We see that the 60Hz makes a comeback if you average after filtering, but it doesn't really affect the rest of the graph at all. 

&lt;p class="caption"&gt;Single channel averaging comparison between filtering before (orange) and after (blue)&lt;/p&gt;
![Single channel averaging comparison between filtering before (orange) and after (blue)]({{ url_for('static', filename = 'SCAfilterordercomparison.png')}})

### Can you use overlapping windows for single-channel averaging?
No. It really heavily concentrates the frequency band you allow it to have (If you do 40 length windows at a sample rate of 200Hz, you get heavy power at ALL multiples of 5Hz, and nowhere else). Completely unusable. The power of each peak isn't even a metric for anything either. 

### Does autocorrelation improve the ability of the other methods mentioned above, applied before or after?
Not really. Usually makes it worse, but not by much. 

### Can you use autocorrelation alone for peak detection?
Nah. The autocorrelation is insanely good if you run it on long data windows (think 20+ seconds), but really terrible at windows of less than 10 seconds. 

### What's the best solution you've found?
The PRSA definitely takes the cake for best peak detection method I looked at today, but I think that they all suck for short data lengths (&lt;=5 sec). It's quite difficult to get any of them to be consistently active, especially since the SSVEP is so temporally varying anyway. 

I think I'm going with the PRSA for now, since it appears to be sort of consistent. Maybe I can do a history-based voting system or something...

&lt;p class="caption"&gt;Comparison of SCA and PRSA for a 5 second sliding window. SCA is much noisier, and doesn't always help fill in the gaps left by PRSA&lt;/p&gt;
![Comparison of SCA and PRSA for a 5 second sliding window. SCA is much noisier, and doesn't always help fill in the gaps left by PRSA]({{ url_for('static', filename = 'SCAvsPRSA5secwindow.png')}})

## Takeaways
These techniques work great for me for more than 10 seconds of data, but that just isn't feasible for real usage. Also it's annoying for me to look at a light for that long. 

## Next steps
I want to make the PRSA run live. I need to add a peak detection history plot to my GUI.

I also want to try the different wavelength LEDs again. With peak detection, it might be better than just amplitude thresholding or something.

As far as new research goes, I want to try using the Stability Coefficient, empirical mode decomposition, and similary of background. Those are next. EMD removes noise, SC offers decent SSVEP identification accuracy at short data chunks around 1 second (voting system!), and similarity of background is close to SC. I also want to try multiple electrodes, and concat their data as if it's one stream. Then I could use PRSA and get great results from "10 seconds of data". 

Ok! Cool, cya around.</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 18 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Quantifying Interface Speeds</title>
      <link>blog/interfacespeeds</link>
      <description>Comparing the information transfer rate of the keyboard, mouse, and other input devices.</description>
      <content:encoded>A lot of brain-computer interface(BCI) researchers will express their results in terms of how much information the user can input in one minute, known as information transfer rate (ITR), measured in bits per minute.

I haven't yet seen a compiled list of the ITR of common household interfaces, so I thought I'd make one. This list will only cover INPUT bits/min, so I'm not going to be calculating how much text you can read off a screen or a number readout since that's OUTPUT information.

I'll list each device, explain its peculiarities and any choices I made regarding number of inputs, then give the stats. Here's a table of the results, read on for how I came to each number!

### Definition of bits for ITR
Before we start, it may seem strange to quantify the mouse or touchscreen in terms of bits, since they're almost continuous input devices. Here, "bits" refers to the log2 of the number of options available. For example, if a user in a BCI trial is trying to select one button when there are two available to pick, then they transmit one "bit" of information per correct choice they make. If choosing a button takes the user 5 seconds, then they'll be able to make 12 choices in a minute. making 12 bits per minute the ITR.


## Compiled Tables (Both available at page bottom)
&lt;p class="caption"&gt;Table of information transfer rates of all common devices&lt;/p&gt;
![Table of information transfer rates of all common devices]({{ url_for('static', filename = 'bpmITRCommon.png')}})

Let's get into it!

# List of Common Input Devices
&lt;hr&gt;

## Thermostat
&lt;p class="caption"&gt;Picture of a thermostat&lt;/p&gt;
![Picture of a thermostat]({{ url_for('static', filename = 'bpmthermostat.jpg')}})

I pressed 10 buttons in 6.4 seconds. 

##### # of Inputs: 5
##### Input Speed: 1.5/sec
##### Bits/min: 209

&lt;hr&gt;

## TV Remote
&lt;p class="caption"&gt;Generic TV remote&lt;/p&gt;
![Generic TV remote]({{ url_for('static', filename = 'bpmtvremote.png')}})

Since TV remotes have notoriously gummy buttons, they're a lot slower than the microwave. I pressed 10 buttons in 7.13 seconds. I used a slightly different television remote than the one pictured, which is why the # of inputs may not line up. 


##### # of Inputs: 42
##### Input Speed: 1.4/sec
##### TV Remote Bits/min: 453

&lt;hr&gt;

## Microwave
&lt;p class="caption"&gt;Common Household Microwave&lt;/p&gt;
![Household Microwave]({{ url_for('static', filename = 'bpmmicrowave.png')}})

Starting with common household electronics, the humble microwave. Though I've never used any button besides the numpad, start, and cancel, there are many more buttons which configure the microwave. I count 25 buttons, plus the trigger that opens the door. 

I can hit 10 buttons in 4.5 seconds (2.2/sec), but they were all sequential and next to each other so I'll say 2 buttons/sec. Log2(26) = 4.7, 4.7 bits/input * 2 inputs/sec * 60 sec/min = 564. Evidently, I did great in high school chemistry.


##### # of Inputs: 26
##### Input Speed: 2/sec
##### Microwave Bits/min: 564
&lt;hr&gt;

## Game Controller
&lt;p class="caption"&gt;They control nuclear submarines with these!&lt;/p&gt;
![They control nuclear submarines with these!]({{ url_for('static', filename = 'bpmgamecontroller.png')}})

We're starting to get complicated. As anyone who has played Smash knows, single button presses do very little — combinations of keys are necessary to even play. I will count single button presses, joystick positions, and a joystick position + button press. 

There are 4 bumpers, 4 positions on the left pad (D-pad), 4 buttons on the right, and four in the center. Joysticks are analog, but we're going to discretize them as 8 edge positions and the neutral position. They also have an inward click, which I'm going to count as a button. Buttons alone, there are 18 total. There are 9 joystick positions per side, and moving between any two of them gives you 9 choose 2, or 36 unique joystick movements.

Combining them gets complicated. Using the left joystick, you can't press anything on the D-pad or left-center, since your thumb covers that. You can, however, still use the bumper/trigger on the left side. So for the left joystick, you have access to 12 buttons. Doing this calculation for the right joystick, you get 13 buttons to work with. Since we already counted the neutral position + buttons in the single button presses, we have 8 joystick positions. For the left and right joystick respectively, that gives us 96 and 104 combination presses. 

I don't have a controller like this, but my friend who is a pro Smash player does. 
10 joystick moves in 2.25 seconds, averaged over 16 trials (left-right flicking, neutral to edge and back). 1.84 sec for 10 face buttons, 2.35 sec for 10 trigger buttons. Since the center buttons are harder to press, and not everyone is a competitive Smash player, I'm going to use the slower time for the button inputs. I'll use the joystick speed for the combo speed, since it'll be the limiting factor. 

&lt;table class="tg"&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th class="tg-0lax"&gt;&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Buttons&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Joystick&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Buttons+Joystick&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Totals&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;# of Inputs&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;18&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;36&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;200&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;254&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;Input Speed&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;4.255&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;4.444&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;4.444&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Average: 4.38&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;Bits/min&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;1065&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1379&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;2038&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;4,482&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;br&gt;
##### Game Controller Total Bits/min: 4,482

&lt;hr&gt;

## Touchscreen

&lt;p class="caption"&gt;The Apple iPhone 11, most sold phone in 2019.&lt;/p&gt;
![The Apple iPhone 11, most sold phone in 2019.]({{ url_for('static', filename = 'bpmiphone11.jpg')}})

I'm using an Apple iPhone 11 as my reference phone since it's the most sold smartphone of 2019, according to Wikipedia. There are a lot of possible input gestures on the phone, and the touchscreen enables them to be fast. I'll count each input separately to make the count more accurate. However, I'm not counting pinches, so this will be a bit of an underestimate. 

&lt;p class="caption"&gt;Icons from the navigation bar on Instagram. Green box is 100 by 75 pixels. Note how it's rectangular in the x-axis. It's much easier to press very wide, short buttons than very tall, skinny buttons on a smartphone, which I didn't realize before.&lt;/p&gt;
![Icons from the navigation bar on Instagram. Green box is 100 by 75 pixels. Note how it's rectangular in the x-axis. It's much easier to press very wide, short buttons than very tall, skinny buttons on a smartphone, which I didn't realize before.]({{ url_for('static', filename = 'bpmphoneminsize10075.png')}})

For smallest input size with the finger, I used the sizes of the icon set from Instagram since they're quite reliable (I don't remember ever misclicking one). These also match the icon sizes in the footer of the iPhone clock app. The green square shown is 100x75 pixels. This divides into the iPhone 11 screen resolution of 1792x828, giving us around 198 non-overlapping rectangles. 

There are also 4 buttons around the phone, two of which can be pressed at once to trigger a different event (screenshot, shutdown, etc.). 4 choose 2 is 6, so that adds 10 inputs for a total of 208.

I pressed on the screen 10 times in 4 seconds. It's not noticeably faster using two fingers. I held down 20 icons on my home screen in 27.55 seconds and dragged 20 times around the screen in 17.8 seconds. Each drag is from one square to any other square, making it 330x329. That's a lot of inputs, but the log2 takes it down. And there are 4 swipes, which can be done in the center or from the edge of the screen. I did 20 swipes in 15.5 seconds. 

&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;margin-left: auto; margin-right: auto;}
.tg td{border-color:white;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:white;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class="tg"&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th class="tg-0lax"&gt;&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Taps/clicks&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Hold down&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Drags&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Swipes&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Totals&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;# of Inputs&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;208&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;198&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;39,006&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;8&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;39,420&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;Input Speed&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;2.5&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;0.725&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1.12&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1.29&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;Average: 1.41&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;Bits/min&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1155&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;332&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1025&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;232&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;2,744&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;br&gt;
##### Touchscreen Total Bits/min: 2,744
&lt;hr&gt;

## Touchscreen (Stylus)
Though not many smartphones these days have a stylus anymore, it does allow much more pinpoint clicks while allowing the same speed and gestures that using a finger does. I'm going to use the same speeds for taps, holds, drags, but will not count swipes or pinches since they are only possible with fingers.

I'll use the screen resolution we used earlier for the iPhone 11 (1792x828), and a smaller 60x60 box for the minimum pointing size. This gives us 412 potential sites for input. I'll include the volume/power/home buttons only in the taps again. 

&lt;table class="tg"&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th class="tg-0lax"&gt;&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Taps/clicks&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Hold down&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Drags&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Totals&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;# of Inputs&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;422&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;412&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;169,332&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;170,166&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;Input Speed&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;2.5&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;0.725&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1.12&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;Average: 1.45&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;Bits/min&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1308&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;377&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1167&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;2,852&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;br&gt;
##### Touchscreen Stylus Total Bits/min: 2,852

&lt;hr&gt;

## Touchscreen (Keyboard)

&lt;p class="caption"&gt;Picture of the iPhone keyboard, with diction key highlighted. I didn't want to find another picture so I reused this one&lt;/p&gt;
![Picture of the iPhone keyboard, with diction key highlighted. I didn't want to find another picture so I reused this one]({{ url_for('static', filename = 'bpmdiction.jpg')}})


I'm using my iPhone 6s keyboard for this one. I'll count each page of keys separately, since they're harder to get to. I'm doing phone keyboard separately because A) we have a lot of muscle memory for it, which lets us input keys a lot faster than a usual interface, and B) to double check my numbers, the final bits/min of this should be bounded by the total bits/min of the touschreen.

There are 29 typing keys on the primary keyboard, plus 26 for capital letters for a total of 55 keys. I type at 45 WPM on the phone, which is 225 characters/min. Including the space after ever word, that's actually 270 characters/min, or 4.5/sec.

On the second page, there are 25 keys (I'm not recounting the delete, return, or spacebar), which can be pressed to your heart's desire, except when the spacebar happens. Since the spacebar is essential to typing, I'm going to count my typing speed with a spacebar after every character, giving us 18 keys in 20.29 seconds. I'll also count the space after each key, so 36 keys.

I'll do the same for the third page, which also has 20 keys (5 are shared with page 2). I typed 19 in 20.93 seconds. Here I'll also count the space after each key, so 38 keys.

&lt;table class="tg"&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th class="tg-0lax"&gt;&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;First page&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Second page&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Third page&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Totals&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;# of Inputs&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;55&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;25&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;20&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;100&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;Input Speed&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;4.5&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1.77&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1.816&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Average: 2.7&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;Bits/min&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;1,560&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;460&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;471&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;2,491&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;br&gt;
##### Touchscreen Keyboard Total Bits/min: 2,491

&lt;hr&gt;

## Mouse/trackpad + Screen
&lt;p class="caption"&gt;Macbook Air&lt;/p&gt;
![Macbook Air]({{ url_for('static', filename = 'bpmmacbookair.png')}})

I'll be using my Macbook Air screen and touchpad as the reference for this one. With a screen resolution of 1440x900, it's worse than the smartphones we just saw, but I think the finger has a broader input area than the mouse pointer. For inputs, I'm counting clicks, double clicks, right clicks, drags, and swipes (scrolling included). 

For smallest reliable click size, I'm using the geometric mean of the icons from Google Chrome and the icons on the MacOS desktop. The green square shown is 30x30 and the desktop icons on MacOS are a slightly larger 70x70, for a combined 45x45. I spend more than half my time on Chrome, and the file icons in Finder are the same size as their icons, but dragging that small an icon is difficult. 

&lt;p class="caption"&gt;Smaller of the "smallest reliable click size" bounding boxes. This is from the Chrome toolbar, and has size 30x30 pixels.&lt;/p&gt;
![Smaller of the "smallest reliable click size" bounding boxes. This is from the Chrome toolbar, and has size 30x30 pixels.]({{ url_for('static', filename = 'bpmmouseminsize3030.png')}})

This divides into the screen to give us 640 clickable squares, which I use for single click, double click, and right click. Drag is from any square to another, so 640x639 = 408,960. There's 2 finger swipes and three finger swipes (from any side or center of the touchpad), and a pinch for a total of 19 potential swipes.


&lt;p class="caption"&gt;Picture of the first mouse&lt;/p&gt;
![Picture of the first mouse]({{ url_for('static', filename = 'bpmfirstmouse.jpg')}})

I clicked 20 times in 18.20 seconds, double clicked 20 times in 19 seconds, right clicked 18 times in 18.24 seconds, dragged 20 times in 26.15 seconds, and swiped 20 times in 20 seconds. 

&lt;table class="tg" style="margin-left: auto; margin-right: auto;"&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th class="tg-0pky"&gt;&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Taps/clicks&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Double clicks&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Right clicks&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Drags&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Swipes&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Totals&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;# of Inputs&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;640&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;640&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;640&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;408,960&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;19&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;410,899&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;Input Speed&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;1.099&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1.05&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;0.986&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;0.764&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1.0&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Average: 0.98&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;Bits/min&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;615&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;587&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;552&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;855&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;255&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;2,864&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
##### Mouse+Screen Total Bits/min: 2,864

&lt;hr&gt;

## Keyboard
&lt;p class="caption"&gt;Physical keyboard, reduced because it doesn't have the numpad on the right.&lt;/p&gt;
![Physical keyboard, reduced because it doesn't have the numpad on the right.]({{ url_for('static', filename = 'bpmkeyboard.jpg')}})

I'll be using my Macbook Air keyboard, which is quite similar to the one shown above. I'm counting single keypresses for typing and not typing separately, and also combination keypresses (Control, Alt/Option, Command, Function, or Shift + any other keys).

Believe it or not, there are 78 keys on the keyboard! I expected way less. I can type at 80 WPM on a good day, which is 400 keys/min assuming an average word length of 5 characters a word. I'm going to count each key that actually types a symbol in a normal text editor, which is 49 keys. Since holding shift doubles each key, this makes 98 inputs at 400 keys/min. Including the spacebar after every word, that's 480 keys/min, or 8/sec.

That leaves 14 function keys, which I can press 20 times in 5 seconds. 

There are 7 combination keys (separating the left and right Command and Alt/Option) which can be pressed down in any combination with any typing or nontyping key. There are 69 of those keys. It's unrealistic to expect to be able to press any combination of the keys though (Imagine holding down all 7!), so we'll cap it at 3 combo keys at most. We also have to subtract out Shift + the 49 typing keys, since we already counted those in the typing test.

For 1 combo key, we have 7 options. For 2, we have 21. For 3, we have 35 options. In total, that gives us 63x69=4347 options for input. I can press 20 combinations in 14 seconds. 

&lt;table class="tg"&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th class="tg-0lax"&gt;&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Single (typing)&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Single (other)&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Combo&lt;/th&gt;
    &lt;th class="tg-0lax"&gt;Totals&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;# of Inputs&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;98&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;14&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;4,347&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;4,459&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;Input Speed&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;8&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;4&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1.429&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Average: 4.48&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;Bits/min&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;3175&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;914&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;1036&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;5,125&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
##### Physical Keyboard Total Bits/min: 5,125

&lt;hr&gt;


&lt;p class="caption"&gt;Table of information transfer rates of all research devices&lt;/p&gt;
![Table of information transfer rates of all research devices]({{ url_for('static', filename = 'bpmITRResearch.png')}})


# List of Experimental Input Devices
These are research devices that are still working out the kinks, and don't work incredibly reliably yet. I'm including voice control because it's a biometric device instead of a button input like the earlier ones, and doesn't work for everyone's voice yet.


&lt;hr&gt;

## Voice Control (Diction)
&lt;p class="caption"&gt;Voice control or diction button on an iPhone keyboard&lt;/p&gt;
![Voice control or diction button on an iPhone keyboard]({{ url_for('static', filename = 'bpmdiction.jpg')}})

I used Google's voice to text function to dictate the sentence "Google's free service instantly translates words, phrases, and web pages between English and over 100 other languages." At 114 characters (118 with punctuation) in 7.37 seconds, this yields a whopping 165 WPM, or 957 characters/min. Apple's diction feature took the same amount of time, which was how long it took me to read the sentence.

Since voice diction cannot detect punctuation or capital letters (except at the beginning of sentences), this is out of the 10 numbers, 26 letters, and spacebar. That leaves us with 37 unique input keys. 


##### # of Inputs: 37
##### Input Speed: 15.47 characters/sec
##### Bits/min: 4,835

&lt;hr&gt;

## Fingerprint/Face recognition
This is probably a special class of "input", but I thought it'd be cool to include. 

### Fingerprints
[NIST](https://www.nist.gov/news-events/news/2004/07/nist-study-shows-computerized-fingerprint-matching-highly-accurate) says fingerprint scanners only give false positives 0.01% of the time. Assuming this is consistent, the fingerprint scanner reliably can differentiate you and 700,000 others from everyone else on earth, which is 10,000 other groups of 700,000 people. I'll say that's 10,000 inputs. My phone takes around 1 second to scan, but sometimes fails, so I'll say it operates at 1.5/sec

##### # of Inputs: 10,000
##### Input Speed: 1.5/sec
##### Fingerprint Scanner Bits/min: 1196

### Face
Also from NIST, the best face recognition has a false positive rate of 0.08%, which is much higher than the fingerprint scanner. You'll be reliably recognized along with 5,600,000 others by the best face recognition, which is 1/1250 groups. It is much faster than a fingerprint scanner though, at least on the iPhone.

##### # of Inputs: 1250
##### Input Speed: 1/sec
##### Face Recognition Bits/min: 617

&lt;hr&gt;


## Eye tracking

&lt;p class="caption"&gt;Screenshot of someone using a Tobii Eye Tracker. The big blob is the predicted gaze location, which takes up about 10% of the screen&lt;/p&gt;
![Screenshot of someone using a Tobii Eye Tracker. The big blob is the predicted gaze location, which takes up about 10% of the screen]({{ url_for('static', filename = 'bpmtobii.png')}})

Eye tracking uses your gaze location as input, usually to steer your cursor, but also for indicating attention. Tobii is one of the largest eye-tracking companies, selling hardware that just plugs into your computer and reports your gaze. However, it has a large inaccuracy which isn't mentioned on the website, and refuses to even mention accuracy specs for any of their products. 

If you watch anybody using it on YouTube like [here](https://www.youtube.com/watch?v=uM0QtujhjcA), you'll see the large spot size which indicates its approximate confidence zone. The frame was originally 850x475 pixels, and the size of the green square is 75x60 for an error margin of about 15% (geometric mean of x and y error). 

If this is the minimum spot size, we can do a similar calculation to the touchscreen section and find that there are around 90 differentiable squares on the screen. However, this doesn't include clicking, and the number of flicks an eye can do in a second. If I use my touchpad click, then I can look and click at 10 locations in 9.34 seconds. If I use a blink (which some eye trackers do use), then I can do 10 blinks in 6.68 seconds. 

The newest Tobii tracker polls at around 33Hz, but has a lot of smoothing going on. I'm going to say it updates the spot at 10Hz, because I don't notice it lagging at all whenever the eye flicks from one spot to another.


##### # of Inputs: 90
##### Input Speed: 1.07 clicks/sec, 1.5 blinks/sec
##### Eye Tracker Bits/min: 417 clicking, 584 blinking

&lt;hr&gt;

## Brain-computer interfaces (EEG)

There's a few different ways to detect what someone's thinking of. I'll go over both the P300 and SSVEP.

### P300
The [P300](https://en.wikipedia.org/wiki/P300_(neuroscience)) is a signal that arises when your expectation of what you're seeing is different from what you expect it to be. These BCIs work by flashing a grid of objects (letters, labeled buttons) and removing one object each time. If you ever don't see the one you're trying to select, then your brain emits a P300. It needs a lot of averaging across many trials to be done reliably though, and it's a very unintuitive way to select things. 

From this [review paper from 2018](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5924393/), the average bits/min of P300 BCIs is around 30. The [best one](https://pubmed.ncbi.nlm.nih.gov/25080406/) incorporated your brain's reaction to a familiar face to help you select a target more robustly, and hit 80 bits/min with an accuracy of 81.25%. 


&lt;p class="caption"&gt;A flowchart of SSVEP brain signal processing&lt;/p&gt;
![A flowchart of SSVEP brain signal processing]({{ url_for('static', filename = 'bpmssvep.jpg')}})


### SSVEP
The [SSVEP](https://en.wikipedia.org/wiki/Steady_state_visually_evoked_potential) is a signal that is also evoked by visual stimulus, but in this case it's caused by repeated flashes of light. It can be reliably detected for flickering between 10 and 20Hz. To use the SSVEP as input, you create an LED display or use a monitor with many buttons on it which are each flashing at a different frequency. Then, you look at the buttons for a short window of time. The frequency spectrum of the recorded EEG signal will have a peak at the frequency of the button you were focusing on, and the computer will know which one you want to select.

The tradeoff here is a longer window raises accuracy but lowers the bits/min transferred, with a minimum of about 3s of data needed to detect it reliably. The best SSVEP ITR I found is from [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5783827/), where they reached a mean of 325 bits/min. I believe this is the offline ITR though, so when they actually measured people using it live they got an ITR of 199 bits/min.

##### # of Inputs: Variable
##### Input Speed: 2/sec (max)
##### BCI Bits/min: 199

&lt;hr&gt;

# Discussion
The computer (trackpad + screen + keyboard) has undoubtedly the highest input rate of any device, clocking in at 7,989 bits/min. It seems that bits/min comes more from an interface which registers button presses quickly than one that offers many simultaneously available options. 

The game controller did better than I expected, beating out the smartphone handily, but that's just on input speed. The game controller offers much less analog control than the touchscreen, and has fewer available applications. You also must remember that the game controller does not have a high-density screen built-in, and must rely on there being one available.

BCIs really need some work, seeing as their best technique clocks in under a thermostat. 

# Both Tables

&lt;p class="caption"&gt;Table of information transfer rates of all common devices&lt;/p&gt;
![Table of information transfer rates of all common devices]({{ url_for('static', filename = 'bpmITRCommon.png')}})

&lt;p class="caption"&gt;Table of information transfer rates of all research devices&lt;/p&gt;
![Table of information transfer rates of all research devices]({{ url_for('static', filename = 'bpmITRResearch.png')}})
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 21 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Oversampling vs. Averaging for Noise Reduction</title>
      <link>blog/oversamplingvsaveraging</link>
      <description>Spoiler: always oversample.</description>
      <content:encoded>Hi! Today I was curious about the effects of oversampling a single, high sample frequency channel of data vs 4 simultaneously sampling channels of the same data, and which would give better data. This ties into my work on EEG processing, where I have the chance to use up to 4 channels but don't want to just average all of them because that seems too simplistic. 

## What did I try then?
So, I generated a sine wave at a low-ish frequency (50Hz) of amplitude 1, then added Gaussian noise of power 10 to it. It has duration 1 second, but I created it using a 100kHz "sample frequency". Here's what it looks like. 

&lt;p class="caption"&gt;Graph of the original and corrupted signal. Orange is the original&lt;/p&gt;
![Graph of the original and corrupted signal. Orange is the original]({{ url_for('static', filename = 'ovacombined.png')}})

I then took 1 sample every n samples in order to create two downsampled data streams. One was sampled every 100 samples, giving a sample rate of 1kHz. The other took one point every 500 points, and had a sample rate of 200Hz. I then compared their FFTs.

&lt;p class="caption"&gt;Spectrum of the 200Hz sampled signal vs the 1kHz sampled signal. Blue is the 200Hz.&lt;/p&gt;
![Spectrum of the 200Hz sampled signal vs the 1kHz sampled signal. Blue is the 200Hz.]({{ url_for('static', filename = 'ova1kvs200zoomout.png')}})

&lt;p class="caption"&gt;Spectrum of the 200Hz sampled signal vs the 1kHz sampled signal, zoomed in to show the difference. Blue is the 200Hz, and much noisier.&lt;/p&gt;
![Spectrum of the 200Hz sampled signal vs the 1kHz sampled signal, zoomed in to show the difference. Blue is the 200Hz, and much noisier.]({{ url_for('static', filename = 'ova1kvs200.png')}})

I've scaled the peaks so they line up, but we see clearly that the 1kHz signal (orange) has much lower noise compared to its peak compared to the 200Hz signal. Let's see what happens if we downsample the 1kHz signal down to 200Hz and compare spectrums then.

&lt;p class="caption"&gt;Blue is the 1kHz signal downsampled to 200Hz, orange is the original 200Hz sampled signal. The downsampled data is much less noisy&lt;/p&gt;
![Blue is the 1kHz signal downsampled to 200Hz, orange is the original 200Hz sampled signal. The downsampled data is much less noisy]({{ url_for('static', filename = 'ovadownsample.png')}})

We see that the downsampled data beats the 200Hz anyway, even though they're at the same length now and sample frequency now. 

What if the 200Hz sampling had 4 unique channels? These are created frmo the original 100kHz signal, and are side by side. What if they were all averaged together to reduce the Gaussian noise (simulating averaging channels)?

&lt;p class="caption"&gt;Blue is the 4-channel average, each 200Hz. Orange is the downsampled 1kHz spectrum.&lt;/p&gt;
![Blue is the 4-channel average, each 200Hz. Orange is the downsampled 1kHz spectrum.]({{ url_for('static', filename = 'ovaavgvsdown.png')}})

Blue is the averaged, orange is the downsampled spectrum. We see that the noise has gone down by a lot in the original 200Hz spectrum, but it's still higher than the oversampled one by about a factor of 2 (just from comparing peaks visually). Here's the same plot including the original, unaveraged 200Hz channel.

&lt;p class="caption"&gt;Same as the last graph, but with the original 200Hz sampled spectrum in green. &lt;/p&gt;
![Same as the last graph, but with the original 200Hz sampled spectrum in green. ]({{ url_for('static', filename = 'ovaavgvsdownplusorig.png')}})

Though I didn't scale the peaks, we see that the noise power has gone down by a factor of 2 (makes sense because of the Gaussian noise), but it still doesn't beat the downsampled 1kHz data. Even if we lower the original signal frequency, the oversampled one continues to outperform the multi-channel setup.

The orange is actually the averaged one here, it has higher noise peaks than the original signal.

&lt;p class="caption"&gt;At a lower signal frequency, the downsampled 1kHz signal still has the lowest noise (green). The averaged 4-channel signal has even higher noise than the original 200Hz signal actually (orange vs. blue).&lt;/p&gt;
![At a lower signal frequency, the downsampled 1kHz signal still has the lowest noise (green). The averaged 4-channel signal has even higher noise than the original 200Hz signal actually (orange vs. blue).]({{ url_for('static', filename = 'ovalowfreq.png')}})

## Discussion
I didn't think the effect would be so drastic, but it seems that sampling at a high rate then downsampling is a very effective strategy compared to averaging all your channels together. But then I don't know what to do with my 4 channels then...

This result is counterintuitive because having 5, 200Hz channels and averaging them is equivalent to sampling at 1kHz and downsampling. 

If we imagine a 1kHz sampled signal, what we did to generate the 200Hz signal was take 1 of every five sample points. This means if we had 5 “channels” (each offset by 1 from the beginning), then its equivalent to down sampling the 1khz signal. But it’s weird that using 4 I didn’t have nearly the same effect, and I don't expect going to 5 channels to change much. 

It might be different because our 200Hz channels were taken from the original signal, which was created at a much higher sample rate (100kHz), instead of taking them frmo the 1kHz signal. But it seems like a minor difference, honestly. 
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 24 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Touch Reactive LEDs</title>
      <link>blog/touchreactiveLEDs</link>
      <description>Electronic piano, anyone?</description>
      <content:encoded>
Hi! Today I’m going to go over a small Arduino project I made using LEDs and the [FastTouch library]().

It's a row of LEDs mounted in protoboard that lights up when you touch the front lead. Multiple can be touched at one time. The only interesting part of the code is allowing each pin to both detect touch and power the LED. Here's why.

&lt;blockquote class="twitter-tweet tw-align-center"&gt;&lt;p lang="en" dir="ltr"&gt;Made a little row of LEDs touch-sensitive. Tracks with your finger really fast! &lt;a href="https://t.co/C6R0Aa9zEK"&gt;pic.twitter.com/C6R0Aa9zEK&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Kong (@redlightguru) &lt;a href="https://twitter.com/redlightguru/status/1353184760026296320?ref_src=twsrc%5Etfw"&gt;January 24, 2021&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

## How FastTouch detects human touch
So the way the FastTouch library works is by detecting the capacitance of the human body, which is a few hundred picofarad. When assigned to a pin, it configures the pin it's reading as `INPUT_PULLUP`, which means it's held at 5V by a ~10k internal resistor. Then it changes the pin to `OUTPUT` and writes the pin as LOW, and counts how long that pin takes to hit 0V. If there's a human touching the pin, we're holding some of the charge from the 5V on the pin, so it takes longer to discharge to 0V. If we're not touching it, then it should discharge almost immediately. 

This works best if the human is touching the ground of the Arduino with their other hand, but also works without that because we are capacitively coupled to our environment, and the ground of the Arduino through the air (something I don't understand well enough to explain here). 

&lt;p class="caption"&gt;Top, unlit LEDs&lt;/p&gt;
![Top, unlit LEDs]({{ url_for('static', filename = 'touchLEDs1toppic.png')}})

## Problem
Because the library counts how long each pin takes to discharge, if we just wired an LED+resistor in series from the pin output to ground, it would discharge immediately through the LED and read no human touch. If I had used one pin for the touch and one pin for the LED, I'd only be able to do 6 LEDs because the Arduino only has around 12 usable pins. So what did we do?

## Solution
Instead of wiring the LED to ground, each one is wired to a pin that is held `HIGH` when we check the touch, then brought `LOW` (ground) when we light the LED. Because the LED prevents backwards current flow, it's an open circuit when checking touch, then allows forward current to light it up when indicating touch detection. Boom! 2x reduction in pins.

&lt;p class="caption"&gt;Picture of the underside of the board. Alternating resistors&lt;/p&gt;
![Picture of the underside of the board. Alternating resistors]({{ url_for('static', filename = 'touchLEDs1resistorunderside.png')}})

## Construction
This took me like two hours to solder and put together, mainly because A) the LEDs had to be in front of the finger pads for visibility, but the pins behind the LEDs had to connect to the finger pads in front, and B) I wanted it to look nice. 

That's all for now, take care.</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 29 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Detecting instantaneous frequency is hard</title>
      <link>blog/IFproblems</link>
      <description>Major blockades to fast, effective BCI</description>
      <content:encoded>Hi! Today we're going to be talking about FFT bins and time. 

When you first take an FFT, whether it be `scipy.fft.rfft` or MATLAB's built-in, you may have had to convert from a "bin number" to the frequency range that bin represented. This formula is pretty simple, but might've been tricky to google initially; the Nth bin contains frequencies from N x (sample rate/sample length) to (N+1) x (sample rate/sample length). 

If you were like me, you spent some time staring at the constant (sample rate/sample length) term that each bin was getting multiplied by. The surprising part was that this ratio was independent of sample rate, meaning that the frequency resolution you got from the FFT is entirely dependent on how long the signal is sampled and nothing else. Of course, the total NUMBER of bins you can get is increased if the sample rate goes up (up to the Nyquist frequency of sample rate/2), but that doesn't help you if your signal is in the low Hz. 

Now, if you do RF stuff, this hardly matters. FM radio operates on increments of 0.1Mhz, so a 10 µs sample length is sufficient to get the bin resolution you need. This is basically finding the instantaneously frequency of a signal — we don't notice 10 µs at all. 

The problem arises when you start analyzing the frequency content of EEG, which operates strictly under 100Hz, and usually under 20Hz. Now to get 1Hz bin resolution in your peak detection, you need at least 1 second of data. This is fine for research-grade BCI, but ruins usability when BCI takes 4x the time to press a button and is only 80% reliable. 

Besides that, the number of inputs is reduced, since only 1 Hz differences can be detected. This leaves us with the whole Hz increments from 10-20Hz, which is only about 10 options. If the possible frequency bins were doubled, then there would be twice inputs; however, to get 0.5Hz bins you'd need 2 seconds of data. These are hardly instantaneous methods now, and EEG doesn't become any more reliable with tighter frequency bins.

And don't even get me started on the time-frequency resolution tradeoff. EEG signals are incredibly temporal, meaning a 10Hz signal may only exist for a second within a longer period of recorded data. This is why techniques like phase-rectified signal averaging work so well for it. But that's another post.

Bye!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 31 Jan 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Physical Word Length Calculator</title>
      <link>blog/wordlengthneonsign</link>
      <description>Determine how much string you'll need to spell out a phrase</description>
      <content:encoded>Hi! I'm making a neon sign with cursive on it, and wanted to know how much neon I'd need to spell out all the letters. To do this in real life, you'd need to cover each letter in string and measure the string, or draw it on a computer and count how long the curve was. 

I wasn't able to find the latter calculator anywhere, so I wrote this little p5.js sketch that you draw a letter onto and it tells you how many pixels long it is. Using this total pixel length and your neon wire length, you can calculate the inches/pixel and figure out how large your letters can be. 

## Instructions
Draw on this canvas and when you release the letter it'll tell you how many pixels long you drew. It doesn't matter how fast you draw, it'll be about the same length either way.

The old shapes are drawn in light grey so you can draw them to scale with each other. Press k to delete the last saved shape, or tap if you're on mobile.

&lt;div id="sketch-holder"&gt;
&lt;/div&gt;

&lt;script src=https://cdn.jsdelivr.net/npm/p5@1.2.0/lib/p5.js&gt;&lt;/script&gt;

&lt;script type="text/javascript"&gt;
	var drawPt = [];
	var drawPtHistory = [];
	var dragged = false;
	var totalDist = 0;

	function setup() {
	  canvas = createCanvas(640, 360);
	  canvas.parent('sketch-holder');

	  textAlign(CENTER, CENTER);
	}

	function draw() {
	  background(255);
	  fill(0);
	  text("Press z or tap to erase the last shape", width/2, 40);

	  if (drawPt.length == 0){
	      text("Total length: " + nf(totalDist,0,2) + " pixels", width/2, 20);
	  } else {
	      stroke(0);
	      for (let i = 0; i &lt; drawPt.length-1; i++){
	          line(drawPt[i][0], drawPt[i][1], drawPt[i+1][0], drawPt[i+1][1]);
	      }
	  }
	  
	  for (let i = 0; i &lt; drawPtHistory.length; i++){
	      stroke(200);

	      let mhm = drawPtHistory[i];
	      //prletln("mhm len", mhm.createCanvas());
	      for (let j = 0; j &lt; mhm.length-1; j++){
	          line(mhm[j][0], mhm[j][1], mhm[j+1][0], mhm[j+1][1]);
	      }
	  }
	}

	function keyPressed(){
	    if (key == 'z'){
	        drawPtHistory = drawPtHistory.slice(0,drawPtHistory.length-1)
	    }
	}

	function mouseDragged(){
	    drawPt.push([mouseX, mouseY]);
	}

	function mouseReleased(){
	    totalDist = 0;
	    for (let i = 0; i &lt; drawPt.length-1; i++){
	        totalDist += dist(drawPt[i][0], drawPt[i][1],
	                          drawPt[i+1][0], drawPt[i+1][1]);
	    }

	    if (totalDist &lt; 10){ // Remove on tap for mobile users
			drawPtHistory = drawPtHistory.slice(0,drawPtHistory.length-1)
	    } else{
	    	console.log("Total dist: " + totalDist);
		    drawPtHistory.push(drawPt);
		    drawPt = [];
	    }
	}

&lt;/script&gt;

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 01 Feb 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Calculating the expected value of the PA scratch offs</title>
      <link>blog/palotteryexpectedvalue</link>
      <description>It's ok, all proceeds pay for the benefits of older residents of Pennsylvania</description>
      <content:encoded>Hi! Yesterday I had a dream where I won a math prize (unlikely) and received a prize of 200 million dollars (even more unlikely). I then woke up, and decided that I needed to lose 20 dollars immediately on the Pennsylvania lottery (I think the odds of this one are the most unlikely out of all three occurences). 

After getting a bunch of scratch off dust all over my bedroom carpet, I ended up $20 poorer, and decided to calculate the expected value of each game I played. The [website](https://www.palottery.state.pa.us/Scratch-Offs/Active-Games.aspx) lists the chances of winning at all (usually around 1:3 to 1:5), then also has a pdf in each page that lists "Chances of Winning" for that game in particular. 

A brief aside, some lotteries will show your odds instead of chances. This is SIGNIFICANTLY different. Odds are wins:losses, instead of wins/(wins+losses), which in this case would change 1:3 to 1/4. 

&lt;p class="caption"&gt;An example "Chances of Winning" pdf from a real active game in PA as of 2/4/21&lt;/p&gt;
![An example "Chances of Winning" pdf from a real active game in PA as of 2/4/21]({{ url_for('static', filename = 'palotterychances.png')}})

PA Lottery offers scratch-offs that cost 1, 2, 3, 5, 10, 20, and 30 dollars. I took the most recent game of each value, and added up the expected values (prize amount * probability of winning prize). I know it doesn't generalize well since I'm only using one of each cost level, but YOU try to scrape a PDF with variable rows. Have fun with that. This data was collected on 2/3/21 from the [PA Lottery site](https://www.palottery.state.pa.us/Scratch-Offs/Active-Games.aspx).


&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;margin-left: auto; margin-right: auto;}
.tg td{border-color:white;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:white;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class="tg"&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th class="tg-0pky"&gt;Scratch-Off Game&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Cost&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Expected Value&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Net Loss&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;% Change&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;3 Mil Extra&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;$30&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;$23.10&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;-$6.9&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;-23%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;1 Mil Jack&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;$20&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;$15.01&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;-$4.99&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;-25%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;WIN WIN WIN&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;$10&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;$7.31&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;-$2.69&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;-26.9%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;Leprechaun&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;$5&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;$3.52&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;-$1.48&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;-29.6%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;Wild Cash&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;$3&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;$1.98&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;-$1.02&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;-34%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;O'Lucky Coin&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;$2&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;$1.31&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;-$0.69&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;-34.5%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0lax"&gt;Clover All Over&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;$1&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;$0.71&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;-$0.29&lt;/td&gt;
    &lt;td class="tg-0lax"&gt;-29%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;br&gt;

Results seem as expected, the more you pay, the less you lose proportionally, though not in raw cash. And, of course, house always wins. The calculated chances also reflect this, with the $30 dollar games offering the highest chances of winning anything. 

Weirdly enough, it seems that the loss from $1 games is the same as loss from $5 games, which makes you feel like you should buy 5x 1$ games instead of 1x 5$ game. Whatever gets you going, I guess. You lose money either way.




</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 04 Feb 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Blink Detection Using Infrared Reflective Sensors</title>
      <link>blog/blinkdetectioninitial</link>
      <description>Preliminary experiments in focus tracking.</description>
      <content:encoded>Hello! 

Recently I found out that blink frequency is associated with focus — when shown a more engaging and interesting video, participants tended to blink less. Intuitively this makes sense; if you want to see something, you'll naturally spend less time closing your eyes. I wanted to apply this principle to something more general: living life. But I didn't want to use computer vision! First, face detection is hard to do well, and runs pretty slowly if you also include closed vs. open eyes. I also wouldn't be able to track my blink rate while doing things away from the computer (and believe it or not, much of my life *is* spent away from the computer). So CV was out. 

I had read from a few papers that skin absorbed infrared light super well, so it shows up dark under an infrared camera. I looked around, and sure enough people had been detecting blink with an IR LED and detector situated very closely to the eyes. Since the shiny part of your eyeball is not skin, it reflects a lot of infrared. When you blink, this gets blocked, and the signal you're reading will have a sharp drop. And that's exactly what I did!

## Setup
I ordered some of these IR reflective sensors off Amazon — these are usually used for line following robots or really terrible distance sensors (their output depends on the nearby material's reflectance). 

&lt;p class="caption"&gt;IR reflectance sensors. 4 pin, one LED and one phototransistor with a light filter.&lt;/p&gt;
![IR reflectance sensors. 4 pin, one LED and one phototransistor with a light filter.]({{ url_for('static', filename = 'blinktrack1_reflectancesensor.png')}})

I then wired it up. LED is current-limited by 100 ohms, phototransistor has 1KΩ in series. I think I can reduce this value to make it more sensitive, since it'll pull down to ground more easily.  

Here it is at work: (first time I've hosted a video locally on this site!)
&lt;div style="text-align:center;"&gt;
&lt;video width="320" height="240" controls&gt;
  &lt;source src="{{ url_for('static', filename = 'blinktrack1.mp4')}}" type="video/mp4"&gt;
Your browser does not support the video tag.
&lt;/video&gt;
&lt;/div&gt;

Anyway, signal is pretty clear (spans around 1.5V on the 5V ADC), and isn't too noisy. I have to figure out a way to stop my pupil from messing with the signal, but I'll mount it on some glasses first and see how stable it stays. 

Eventually, I want to push these blink timestamps constantly to a database, and have it ping me when it notices me slipping out of focus. I dunno, something can be done with this additional biometric data. 

That's all for now. Cya!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 12 Feb 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>5050 LED Strip Teardown</title>
      <link>blog/led5050teardown</link>
      <description>Repurposing an LED light strip controller and power supply</description>
      <content:encoded>
Hello! I purchased some LED strips off Amazon. I've always wanted some more lighting, but more importantly, I also wanted to set up some interactive visuals in my room! 


&lt;p class="caption"&gt;Let there be light&lt;/p&gt;
![Let there be light]({{ url_for('static', filename = 'led5050_lit.png')}})


Ideally, I'd create a few scripts that allow users to control the light strip in fun ways. For instance, I wanted to add an encoder dial that would control the position of a single LED, and then when turned, it would just move around strip in a big circle. Or a visual doorknob that would flash when pressed, so I could respond to people outside my door while listening to music. Or a clock that fills up or drains the LED strip as the day progresses, a visual indicator of daylight running out. There's a lot of ideas here.

Anyway, the first step is to purchase a sufficiently long LED strip off Amazon. I measured my room, and need about 50 feet of lights to go all the way around. 

Buyer beware here, if the item description does not mention "chase effect" and is really cheap, it probably doesn't have individually addressable LEDs. They're referred to as 5050 LEDs, and instead of 3 wires have 4 wires — 12 V, R, G and B. Not sure why it's inverted like that. So since the strip is only addressable as a whole, I can't do the knob thing, but I can do some more fun things that I had planned.

&lt;p class="caption"&gt;Unlit reel on my desk. Comes with backing and a nice rubber-y cover. &lt;/p&gt;
![Unlit reel on my desk. Comes with backing and a nice rubber-y cover.]({{ url_for('static', filename = 'led5050_reel.jpg')}})

Most of these come with some app, so we know there will be some Bluetooth or otherwise IoT going on inside. The supply is 12V, and claims to source up to 5 amps. At 60mA per LED at full draw, and 300 LEDs/reel (2 reels), that's a max draw of 36 A. Of course, each LED does not draw 60mA, but we are upper bounding. Though that 60mA was for NeoPixels, maybe it's lower for these...

## Power
Anyway, here's the power supply. Looks like a generic 12V brick, some conversion then a Cockcraft-Walton (I assume that's what the diodes are for?), then another transformer. Take this with a grain of salt, I study computers not circuits. Out comes 14V-ish DC (no load), and 7V goes into the emitter of the transistors in bottom-left.

&lt;p class="caption"&gt;Power supply for the LED lights&lt;/p&gt;
![Power supply for the LED lights]({{ url_for('static', filename = 'led5050_powersupply.jpg')}})

## Control
Lying next to the power supply board unsecured is the control board. You can see the 3 transistors in the bottom left leading to some FAT traces that go to the RGB channels, as well as the 12V input. There's also the board-trace Bluetooth antenna in the top right-hand side.

I couldn't find the transistor number online (X0GA 25), but if you apply 4V to the bottom-right pin of the SOT package, it lights. Good enough for me. I may also just turn all the transistors on and then put my own transistors on the output line.

&lt;p class="caption"&gt;Controlling board for the LED light strip. Was previously just dangling in the power brick...&lt;/p&gt;
![Controlling board for the LED light strip. Was previously just dangling in the power brick...]({{ url_for('static', filename = 'led5050_controlboard.jpg')}})

Looks good. Next step, find a 5V microcontroller and hook it up! We'll see how hard the controls are, depending on the voltage-to-brightness conversion on the transistor.

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 20 Feb 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>MPR121 Touch Sensing Across Barriers</title>
      <link>blog/mpr121passthrough</link>
      <description>Testing capacitive coupling across various barriers</description>
      <content:encoded>Hello! Today I received the MPR121, a nifty little 12-channel touch sensor:

&lt;p class="caption"&gt;Glamour shot of the MPR121 touch sensor&lt;/p&gt;
![Glamour shot of the MPR121 touch sensor]({{ url_for('static', filename = 'mpr121glamour.png')}})

The chip is pretty simple, uses I2C to measure each pin's capacitance using the charge then discharge trick I used [here](../touchreactiveLEDs/). It gives a touch/no touch output, or the raw sensor values per each pin (ranges from 0-400ish). Also has a built-in filtered output that provides smoother data if you want it. 

The setup is easy and not very interesting since the chip is so single-purpose. I'm much more interested in showing you how well the sensor detects touch through a barrier. 

## Testing materials
I used what I had near me, and those were: saran wrap, masking tape, and two kinds of Scotch tape. Here's the table.

&lt;p class="caption"&gt;&lt;/p&gt;
![Table of sensor outputs for the MPR121]({{ url_for('static', filename = 'mpr121chart.png')}})

As mentioned before, the sensor values range from 0-400. On the bare surface I was able to get down to 48, which is a big SNR! None of the others even came close to that, only dipping 10% of the max value for the Saran wrap. This was probably the thinnest barrier I tried, and even that was too much. Despite the low SNR, the sensor still detected my touches consistently when separated by most of the materials I tried. Works great! :)

The capacititance didn't drift that much across my trials (~15 minutes), so I'm not so worried about needing to recalibrate very frequently. These types of biosensors are usually pretty finicky in that regard, so I'm glad. I also didn't try fabric or any porous material because I needed this to be waterproof, but it'd be cool if YOU (dear reader) did, and then told me about it!

That's all for now! Cya!

&lt;p class="caption"&gt;MPR121 wrapped in saran wrap during testing&lt;/p&gt;
![MPR121 wrapped in saran wrap during testing]({{ url_for('static', filename = 'mpr121saran.png')}})



</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 13 Mar 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Intro to Intro to Woodworking</title>
      <link>blog/woodworkingday1</link>
      <description>There's always money in the banana stand</description>
      <content:encoded>Yesterday I had my 2nd in-person class this semester. We met a nice man in Techspark and he showed us the woodshop. The walls were covered in covered plywood shelves and things. We saw a few demo projects that we'd be doing during the course, like a folding stool, push stick, and crate. 


Our first assignment was to make a little banana stand/hammock thing. We went over 4 machines — the band saw, belt and disk sander, oscillating spindle sander, and drill press. I had never seen a spindle sander before, but it was a really cool machine. It's really good for sanding down interior curves, which neither the belt nor disk sander can reach. 

I ended up using the bandsaw a lot. The cut turn radius is quite good, and I learned about using relief cuts to make the cuts even tighter. The bandsaw is also good for widening slots, and I widened my stand's base a bit too much. Ended up having to glue it in place, using some wood filler to make the seams nicer looking.

&lt;p class="caption"&gt;Photo of my banana stand, hookless and bananaless&lt;/p&gt;
![Photo of my banana stand, hookless and bananaless]({{ url_for('static', filename = 'bananastand.png')}})

Finally, I used a hand sander which had a velcro pad of sandpaper that was a little off center to smooth the rough wooden surface. It looks quite weird when it moves, wagon-wheeling before your eyes. 

It's amazing how much scrap wood they have in there. The shop goes through a lot just for demonstrations, and then thick plates are just free for anyone to use in the refuse pile. Good news for me! I'm not sure what to make next — it would be cool to recreate a lamp of mine, or a new wooden spoon. So many possibilities!

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 24 Mar 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Implementing Body Channel Communication Pt. 1</title>
      <link>blog/bcc1</link>
      <description>Harnessing the body's conductivity as a signal path</description>
      <content:encoded>Hello! Today I'm going to be talking about some preliminary experiments I did involving body channel communication, or BCC. BCC is a technique that uses the body's natural conductivity as a signal path, allowing your devices to talk to anything you touch. I implemented a basic version that requires a shared ground, some experiments from an implementation without a shared ground, and show practical techniques that worked for me. 

&lt;p class="caption"&gt;Transmitting signal over the skin&lt;/p&gt;
![Serial plotter transmitting signal over the skin]({{ url_for('static', filename = 'bcc1_signaloverskin.png')}})


&lt;br&gt;

# Background
&lt;hr&gt;

### Why would you want this?
Despite your body's high resistance, large voltage changes propogate fairly well across it. If you wear a watch or some other device, it can digitally write out an I2C/UART/bitwise ASCII signal to a pin that's touching your skin by bringing the pin high and low. This digital logic signal propagates through you to whatever you're touching. 

If the device sends out your student ID, anything device you touch can now identify who is touching it. If it sends out your credit card information, you can pay with just a touch when checking out.

Best of all, it works the other way. If a device is writing bits out to a surface, your watch can also detect when you touch something. If each device transmitted its own MAC address, your watch would know what you touched, and would be able to talk back to it through you. 

&lt;br&gt;

### But doesn't sending a signal require two wires for ground and signal?
Very astute! It does! However, because the body is so conductive, it naturally couples capacitively to the environment around it. This means that the device and the human can share a ground connection without an additional wire; the device is grounded through the building's wiring, and the human through their salty body.

The downside to this is that the ground connection is rather weak. The human-ground capacitor is estimated to be about 100pF or less, so signals don't cross the human-device THAT well. However, there has been research exporing how to robustly use BCC even with this weak connection. More details can be found in [[1]](http://www.alansonsample.com/publications/docs/2018%20-%20UIST%20-%20Enabling%20Interactive%20Infrastructure%20with%20BCC.pdf) and [[2]](http://www.alansonsample.com/research/BCC.html). 

&lt;br&gt;

# What I did
&lt;hr&gt;

## Hardware
My setup is very simple. I used two Arduinos, two 180kΩ resistors, and two jumper wires. There is a "sender" Arduino imitating a smartwatch, and a receiver Arduino imitating a payment terminal or another interactive device. 

The sender has just a single jumper out, and it's digital writing a 10Hz square wave to the jumper wire.

&lt;p class="caption"&gt;Sender Arduino&lt;/p&gt;
![Arduino with jumper wire outputting the digital signal]({{ url_for('static', filename = 'bcc1_sender.jpg')}})

The receiver has a jumper wire on analog pin 0 which is reading at 100Hz. A 360kΩ resistor connects that pin to the ground pin. The extra resistor at the bottom is for measuring a ground truth analog reading, which I'll talk about later.

&lt;p class="caption"&gt;Receiver Arduino&lt;/p&gt;
![Arduino with jumper wire on analog pin, which is connected to the ground by resistor]({{ url_for('static', filename = 'bcc1_receiver.jpg')}})

The resistor is necessary because of the EMI pickup when I added the jumper wire to the analog pin. Here's the noise on the analog pin, then with the jumper, then when I hold the jumper with my finger. 

&lt;p class="caption"&gt;Baseline analog pin readings&lt;/p&gt;
![Serial plotter of baseline analog pin readings]({{ url_for('static', filename = 'bcc1_baselineanalognoise.png')}})

&lt;p class="caption"&gt;Analog pin readings with jumper wire attached&lt;/p&gt;
![Serial plotter of analog pin readings with jumper wire attached]({{ url_for('static', filename = 'bcc1_baselinewithwire.png')}})

&lt;p class="caption"&gt;Analog pin readings with human holding jumper wire&lt;/p&gt;
![Serial plotter of analog pin readings with human holding jumper wire]({{ url_for('static', filename = 'bcc1_baselinetouchinganalognoise.png')}})

The human body acts as an antenna for 60Hz noise, and my body is no different. The spikes you see in the last image are from my environment, and would destroy our signal without the resistor to ground. The smaller the resistor, the easier the environment's noise gets leaked away from the pin. However, this easier leakage also reduces our overall signal more, so I picked a large resistor that approximately matched my skin's resistance. 

&lt;br&gt;

## Signal (Shared Ground)

&lt;p class="caption"&gt;Shared ground BCC setup (technically they're already connected through the laptop's ground)&lt;/p&gt;
![Shared ground setup (technically they're already connected through the laptop's ground)]({{ url_for('static', filename = 'bcc1_connectedground.jpg')}})

To transfer the signal, I touched the output jumper of the sending Arduino, then touched the input jumper of the receiving Arduino. I also pressed the output jumper directly against my other input, to capture a reference for what the output pin's signal looks like directly. 

&lt;p class="caption"&gt;Square wave being transferred over human touch. Green is the pin I'm touching, red is the reference pin&lt;/p&gt;
![Serial plotter of square wave being transferred over human touch]({{ url_for('static', filename = 'bcc1_signaloverskin.png')}})

The signal transfer is pretty clear. The green line is what the receiver reads from my finger, and the red is the actual output of the pin. There's a bit of interference from the ADC, but even without the reference pin the signal transfers really cleanly.

&lt;p class="caption"&gt;Square wave being transferred over human touch sans reference&lt;/p&gt;
![Square wave being transferred over human touch without reference]({{ url_for('static', filename = 'bcc1_signaloverskin1.png')}})

Here's what the noise looks like in the setup if I hold the receiving pin without touching the signal pin. 

&lt;p class="caption"&gt;Noise when touching receiver pin without touching signal pin&lt;/p&gt;
![Noise when touching receiver pin without touching signal pin]({{ url_for('static', filename = 'bcc1_nosignaloverskin.png')}})

The no-touch signal is a bit noisier than I would like, but definitely different enough from the touching signal that we would be able to detect it. 

&lt;br&gt;

## Signal (No Shared Ground)
I connected the sending Arduino to another computer when I did this experiment. The receiver is plugged into my laptop, which was not connected to the wall outlet at the time.

&lt;p class="caption"&gt;No shared ground BCC setup&lt;/p&gt;
![No shared ground setup]({{ url_for('static', filename = 'bcc1_connectednoground.jpg')}})

Without the shared ground of my laptop, the problem gets a lot harder. I first tried holding the signal and receiving wires to compare the shared vs unshared grounds. The square wave is lost in big noise of a completely different frequency, and comes straight back when I connect the grounds again.

&lt;p class="caption"&gt;Unconnected vs. connected ground BCC signal&lt;/p&gt;
![Unconnected vs. connected ground BCC signal]({{ url_for('static', filename = 'bcc1_ngvsg.png')}})

I was worried that the signal wasn't actually getting through at all, but it does. I tried holding the signal wire, then dropping it but still touching the receiving wire. 

&lt;p class="caption"&gt;Received signal while I hold the signal wire then release it&lt;/p&gt;
![Received signal while I hold the signal wire vs when I don't]({{ url_for('static', filename = 'bcc1_ng_touchvsnotouch.png')}})

And when I hold the receiving wire, then drop it, the signal goes to our nice analog baseline. 

&lt;p class="caption"&gt;Received signal while I hold the receiving wire then release it&lt;/p&gt;
![Received signal while I hold the receiving wire then release it]({{ url_for('static', filename = 'bcc1_ng_touchvsnotouch2.png')}})

The detection is sizing up to be fairly tricky, but I think that we should be able to detect the signal even when the grounds are not shared. There's already a clear difference between touching signal and not, but we need to do some work on conditioning the signal to better cross the human-pin divide.


# Next steps
This is a 10Hz signal, which is fairly slow. We can make it much higher, but I needed to go low to show the plots properly on the serial plotter. However, we're limited by the ADC time, which is middling on an Arduino. We may need to switch to another microcontroller if we want more speed. With a higher frequency, we also get better coupling to ground, which hopefully gives us a better transferred signal.

We're gonna try bits next! Cya then!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 17 Apr 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Implementing Body Channel Communication Pt. 2</title>
      <link>blog/bcc2</link>
      <description>Look Ma, no shared ground!</description>
      <content:encoded>Continuing from [last time](../bcc1), I'm trying to make our body channel communication system work without requiring a shared ground between the two devices. And I got it working in a very bootstrapped way, which may be usable for our class demo. Read on for more!

&lt;hr&gt;

# Recap
You may remember my two-Arduino setup from last time, using one as a transmitter and the other as a receiver. Ideally you'd want either uC to work for both functions, but that's easily tackled later. The previous setup utilized the shared ground of my computer's chassis to transmit signal reliably. I also showed some experiments I tried without shared ground, and they didn't work as well. The signals sent without shared ground had a lot of 60Hz noise, and the added signal square wave signal swamped the ADC's input range. 

&lt;p class="caption"&gt;Shared ground BCC setup from last post&lt;/p&gt;
![Shared ground BCC setup from last post]({{ url_for('static', filename = 'bcc1_connectedground.jpg')}})

&lt;hr&gt;

# Propagating higher frequency signals without a ground
Since it's impossible to transmit a voltage with one wire (a potential difference inherently relies on having two "wires"), I decided to fake a radio transmitter. Because the capacitance of the body has decreased impedance for higher frequencies, signals can couple through the body better at higher frequencies. Last time, I used frequencies under 1kHz because I wanted to be able to see it on the serial plotter. No longer!

I wanted to change the transmitter frequency to something closer to 1MhZ. I started with `delayMicroseconds(1)` between direct digial port manipulation to switch the pin on and off, but got a longer delay than I expected when I measured it on an oscilloscope. Then I switched to using 8 No-ops between the digital switching, and got a faster square wave coming out. A half second timer turned the square wave on and off. The receiver code had no change, I left it analog sampling as fast as it could go.

Side note, I also tried writing the Arduino's 16MHz clock out to the transmitter pin since it's faster. It didn't work. When sending spikes with the clock, the analog readings on the receiver integrated together to a constant voltage, but with a large delay and rise time. Not fast enough!

&lt;hr&gt;

# Picking up the spikes

When I first started testing, I realized that there was a great asymmetry depending on what powered the board. I used a USB isolator to "separate" the grounds, and initially I had the transmitter plugged into the isolated USB port. The signals I got had awful 60Hz noise, but when the train of square waves was transmitted you could still clearly see it. If I had stopped here, I would have to deal with a mediocre bitrate and a terrible filtering problem. 

&lt;p class="caption"&gt;The little solid spikes are 60Hz noise, and the denser, thin spikes are periods of transmission.&lt;/p&gt;
![Graph of serial plotter. The little solid spikes are 60Hz noise, and the denser, thin spikes are periods of transmission. ]({{ url_for('static', filename = 'bcc2_every50msIS.png')}})

Then, I had the thought to switch them. Now the receiver was isolated, and my laptop powered the transmitter. Voila! Immediately the 60Hz pickup disappeared, and the spikes were now even larger. Win-win!

&lt;p class="caption"&gt;Receiver readings. Spikes are 1s, and flat bits are 0s. Note the lack of 60Hz noise: when it's flat, it's flat. &lt;/p&gt;
![Receiver readings. Spikes are 1s, and flat bits are 0s. Note the lack of 60Hz noise: when it's flat, it's flat. ]({{ url_for('static', filename = 'bcc2_everyhalfsecondIR.png')}})

I think that the chassis/external cabling of my laptop allowed it to pick up a lot more noise than the isolated Arduino on its own. When the computer powered the receiver, the reciever gets a lot of that noise passed into it. However in this second config, the receiver seems to not pick up much noise on its own.

&lt;hr&gt;

# Interpreting spike clusters as bits

### Through in-line capacitor vs. without
If you recall, I'm linking my receiver's analog input with GND with a high value resistor. I wanted to compare touching the analog pin directly with touching it through a capacitor. I expected to be able to charge the capacitor and sort of integrate the spikes into a more digital looking bit, but it seems to not help that much.


&lt;p class="caption"&gt;Receiver readings when touching through in-line capacitor&lt;/p&gt;
![Receiver readings when touching through in-line capacitor]({{ url_for('static', filename = 'bcc2_withinlinecapIR.png')}})

&lt;p class="caption"&gt;Receiver readings when touching analog pin directly&lt;/p&gt;
![Receiver readings when touching analog pin directly]({{ url_for('static', filename = 'bcc2_withoutinlinecapIR.png')}})

Using a capacitor first seems to increase the overall signal by a tiny amount. Or it could be selection bias! Either way, I don't think it matters too much. 

### Software integration

I then tried to just keep a moving average of the analog readings. This is a 10-sample ring buffer that only averages the past 10 readings. The noise on the 0s has decreased a bit in the averaged output. The spike clusters are also now clearer, but at what cost? The highest point is now around 150 instead of 250. Not so bad. I'll probably choose a max filter instead to better approximate a logic signal.

&lt;p class="caption"&gt;Short-term moving average&lt;/p&gt;
![Short-term moving average]({{ url_for('static', filename = 'bcc2_withsmoothingIR.png')}})

I also tried a counting filter which output 700 if there were more than 5 samples &gt; 100 in the past 10 seconds, and 0 otherwise. 

&lt;p class="caption"&gt;Boolean filter of (num samples &gt; 100) &gt; 5&lt;/p&gt;
![Boolean filter of (num samples &gt; 100) &gt; 5]({{ url_for('static', filename = 'bcc2_countingfilterIS.png')}})

# Highest speed
The fastest speed I tried was a bitrate of 50Hz. A pulse train was sent every 10ms, then not sent out for 10ms. Looks good, there are at least 5 received bits within each window, and I think a human would be able to pretty clearly differentiate between 1s and 0s. 

&lt;p class="caption"&gt;50Hz train of pulses transferring between the transmitter and reciever&lt;/p&gt;
![50Hz train of pulses transferring between the transmitter and reciever]({{ url_for('static', filename = 'bcc2_every10msIR.png')}})


# Conclusion and next steps
I just want to make it clear, this method sucks. It's really hacky, and the receiver needs to have a much better and faster analog-to-digital converter for it to properly receive the spikes. I'm trying it out with a better microcontroller soon, and we'll see if the spikes can be made even tighter. I want a data rate of at least 1000 bits/sec, and it would be *amazing* to hit 40 kilobits/sec since that's a typical RFID data transfer rate. But at least it works without a shared ground now!


</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 27 Apr 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Outro to Intro to Woodworking</title>
      <link>blog/woodworkingday2</link>
      <description>Crate, shelf, stool</description>
      <content:encoded>Hello! I have finished up this class (and with it, the last semester of junior year ;(). In the past 3-4 weeks, I have learned how to use many other woodworking implements, some of which I could've figured out and some of which I couldn't've. The wood glue and nailgun were pretty intuitive, but the flush saw and forstner bits less so. 

I have messed up a few times, and I'm putting these mistakes in public so you know what I won't mess up on again. 
 
 - Did a rip cut on the miter saw
 
 - Flush sawed past the peg and into my wood
 
 - Drilled 4 holes and then jigsawed to make a handle when I could have drilled two and it would've been way easier
 
 - Nailgun'd a crooked part twice before checking it
 
 - Nailgun'd nonperpendicularly so the nail stuck out of the wood piece


&lt;hr&gt;

# What I made in the process

## Bread Shelf
This shelf was made so we could stop keeping bread on top of the fridge, where it blocked some cabinets. We had some preexisting screws in the wall which I measured, and then I made the decorative wall legs too short :(. I could only put one set of screws so the shelf is very weak. Holds bread good though. 


&lt;p class="caption"&gt;Bread shelf without bread&lt;/p&gt;
![Bread shelf without bread]({{ url_for('static', filename = 'wood2_breadshelfwithoutbread.jpg')}})


&lt;p class="caption"&gt;Bread shelf with bread&lt;/p&gt;
![Bread shelf with bread]({{ url_for('static', filename = 'wood2_breadshelfwithbread.jpg')}})


## Crate
The crate was for an assignment. Very bulky, hard to store, but makes a good stool. 

&lt;p class="caption"&gt;Crate assembled&lt;/p&gt;
![Crate assembled]({{ url_for('static', filename = 'wood2_crate.jpg')}})

&lt;p class="caption"&gt;Crate holding Yerba Mate&lt;/p&gt;
![Crate holding Yerba Mate]({{ url_for('static', filename = 'wood2_crate2.jpg')}})

## Unfoldable Stool
The camping stool was our final assignment. It’s meant to be foldable but I messed up the tolerances. Many stool jokes were made in the process. Anyway, seats one. 

&lt;p class="caption"&gt;Pieces of the stool&lt;/p&gt;
![Pieces of the stool]({{ url_for('static', filename = 'wood2_stool1.jpg')}})

&lt;p class="caption"&gt;Stool parts fit check&lt;/p&gt;
![Stool parts fit check]({{ url_for('static', filename = 'wood2_stool2.jpg')}})

&lt;p class="caption"&gt;Completed seat!&lt;/p&gt;
![Completed seat!]({{ url_for('static', filename = 'wood2_stool3.jpg')}})

## For Fun
Me and Elio also redesigned our club logo, and made it out of wood. Looks better on paper, but worth a shot.

&lt;p class="caption"&gt;Completed seat!&lt;/p&gt;
![Completed seat!]({{ url_for('static', filename = 'wood2_roboclublogo.jpg')}})



Cya around next time I make stuff out of wood!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Thu, 29 Apr 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Implementing Body Channel Communication Pt. 3</title>
      <link>blog/bcc3</link>
      <description>Reinventing the radio and using touch as the antenna</description>
      <content:encoded>&lt;div style="font-size:.75em;"&gt; Previous posts here: &lt;a href="../bcc1"&gt;[Part 1]&lt;/a&gt; and &lt;a href="../bcc2"&gt; [Part 2] &lt;/a&gt; &lt;/div&gt;

&lt;hr&gt;

Hello! I have successfully implemented BCC, transmitting signals across the human body through touch alone. With the help of Sam, anything is possible. In this post I'll show you how it works, and some neat applications.

&lt;br&gt;
Here's a demo showing a reactive calendar. When a user touches the pad, their smart watch transmits their encoded ID to the computer. When decoded, the computer knows who is touching it, and can show that user's calendar. 

&lt;p class="caption"&gt;Calendar that detects user identity through their touch&lt;/p&gt;
![Calendar that detects user identity through their touch]({{ url_for('static', filename = 'bcc3_caldemosmall.png')}})

&lt;hr&gt;

# Recap of what doesn't work
[Last time](../bcc2) I showed that high frequency switching on a transmitter could be received as "spikes" of voltage on the receiver side. 

&lt;p class="caption"&gt;Dense, thin groups of spikes are digital 1s, thick spikes of noise are the 60Hz leaking in to our 0 signal&lt;/p&gt;
![Dense, thin groups of spikes are digital 1s, thick spikes of noise are the 60Hz leaking in to our 0 signal]({{ url_for('static', filename = 'bcc2_every50msIS.png')}})

It turns out I was wrong; the spikes are only visible because the grounds were galvanically linked. I had done the separation of receiver and transmitter using a USB isolator, which produces its own ground from the laptop's USB power. This ground happens to be produced at the same potential as the transmitter's ground, so they still share the reference. 

When I powered the transmitter with a battery pack, the spikes dropped down to the level of the noise. They were still visible to the eye as higher movement bits of noise, but definitely impossible to detect computationally. Time to try something else.

&lt;hr&gt; 

# How it works
I'm using a high frequency carrier wave to carry the bits, and an amp to detect the signal in the specific frequency band of the carrier wave. This is basically how radios work. The transmitter uses the digitally-modulated clock signal of the Arduino(8 MHz) to send 0s and 1s. The receiver is a passive RC high-pass filter, followed by an amplifier stage before the ADC.

## Transmitter
&lt;p class="caption"&gt;Picture of the transmitter&lt;/p&gt;
![Picture of the transmitter]({{ url_for('static', filename = 'bcc3_transmitter.jpg')}})

I was lazy with the earlier prototype and tried to transmit bits using the output from flipping a pin in the ```loop()``` of the Arduino as fast as possible. This technique caps out at a frequency of around 100kHz, but I wanted to go higher. 

I found a [way](https://arduino.stackexchange.com/questions/16698/arduino-constant-clock-output) to output the Arduino's clock signal (16MHz square wave) on a pin. Connecting that to the input of a transistor, I could then turn the clock signal on and off using another pin on the base/gate. This became the transmitter signal: high frequency clock when on, grounded when off. 

&lt;p class="caption"&gt;Raw signal from the transmitter&lt;/p&gt;
![Raw signal from the transmitter]({{ url_for('static', filename = 'bcc3_raw.jpg')}})

This turned out to be even easier to implement than what I tried in Pt. 2, since I only had to turn on and off a digital pin in the middle loop.

&lt;p class="caption"&gt;Circuit for the transmitter, which is just a modulated clock signal&lt;/p&gt;
![Circuit for the transmitter, which is just a modulated clock signal]({{ url_for('static', filename = 'bcc3_transmittercircuit.jpg')}})

## Receiver
&lt;p class="caption"&gt;Picture of the receiver&lt;/p&gt;
![Picture of the receiver]({{ url_for('static', filename = 'bcc3_receiver.jpg')}})
The receiver side swamps with 60Hz when I touch the input pin. This is both because the human body is a big antenna for all EMI pickup, and because the Arduino's ADC draws current from the source to measure voltages. 

To get rid of the 60Hz noise, we built an RC high-pass filter at 159Hz. It works surprisingly well for how close the cutoff frequency is to the noise frequency. After that, we had to add a lot of gain (300x) on the signal to see the square wave being modulated. We also built an artificial ground at 2.5V to act as the central reference for the op amp. 

&lt;p class="caption"&gt;Circuit for the reciever, composed of the stages in the above paragraph&lt;/p&gt;
![Circuit for the reciever, composed of the stages in the above paragraph]({{ url_for('static', filename = 'bcc3_receivercircuit.jpg')}})

## Transmitted signal
Here's what we're sending. A header of 10 so the reciever knows a signal's coming, then an ASCII character. 

&lt;p class="caption"&gt;Raw output from the transmitter with labeled sections of the data&lt;/p&gt;
![Raw output from the transmitter with labeled sections of the data]({{ url_for('static', filename = 'bcc3_transmittedraw.png')}})

And on the receiving side, the signal appears perfectly among all the 60Hz noise. 

&lt;p class="caption"&gt;Input on the receiving side, after filtering and gain. The bits are clearly visible&lt;/p&gt;
![Input on the receiving side, after filtering and gain. The bits are clearly visible]({{ url_for('static', filename = 'bcc3_receivedraw.png')}})


## Code
The repo can be found [here](https://github.com/kongmunist/BCC). The transmitter code is very simple, just a digital write and some array reading. The receiver is a little more complex, and a bit hacky to read bitstrings. Works surprisingly well though. 

## Final Demo
Here's a video of the calendar demo. I didn't have time to rig up two "watches" with different IDs, but my friend who is uninstrumented is unable to access a calendar like I am. Sorry buddy, should've been a technlogist!

&lt;div style="text-align:center;"&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/4pwZ0hJcawM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;


# Final Notes

### Limited signal propagation
The signal does not spread across my entire body, which was a little weird. Wearing the watch on my left wrist, I can trigger the calendar using my left hand, left elbow, nose, and right elbow, but not my right hand. I think with more gain on my receiving side I'd be able to detect it there too. I think [this paper](http://www.alansonsample.com/publications/docs/2018%20-%20UIST%20-%20Enabling%20Interactive%20Infrastructure%20with%20BCC.pdf) talks more about distance of effect.

### Low bitrate
The Arduino ADC is fast enough to detect bits reliably at 1kHz, but not much higher than that. I'd need a dedicated ADC and some ring-buffer in software to get a higher bitrate. But honestly, I can't imagine the someone's ID being longer than a kilobyte or so, even if encrypted. that's a lot of bits!

### Cool factor
It's dope that the hardware is so simple. One microcontroller, an op amp, and some passives, and your devices can detect not only touch but also WHO touched it. Very powerful, and quite simple to add to a smartwatch \*hint hint wink wink\*.


If you have questions about implementing this yourself, reach out via email or Twitter. BCC has some dope applications, and I'd love to create more demos using it. 






</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 15 May 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Stimulating the Merkel Discs: Nerve stimulation using TENS</title>
      <link>blog/mechanoreceptor1</link>
      <description>The Firm Handshake-inator 3000</description>
      <content:encoded>Hello! As you may know, there are 4 mechanoreceptors in the skin.

I wanted to create a specific sort of sensation on my skin, and I thought the best way would be to **target one of these specific mechanoreceptors**. 

## Background

These mechanoreceptors are like a biological equivalent of electronic sensors; cleverly shaped bits of nerve tissue that are sensitive to specific vibrations, sustained or brief pressure, etc, and report back to the brain what they feel. Through these sensors, you learn what textures feel like, what shapes you're touching, and all that cool tactile stuff we rely on every day. You can read more about them on [Wikipedia](https://en.wikipedia.org/wiki/Mechanoreceptor#Types). 

While each receptor is most sensitive to a range of frequencies as stimulation (running your hand over a cloth for example), this tells me nothing about the way the receptors send this message back to the brain. 

&lt;p class="caption"&gt;Pacinian corpuscle image I got from &lt;a href="https://en.wikipedia.org/wiki/Pacinian_corpuscle"&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
![Pacinian corpuscle image I got from &lt;a href="https://en.wikipedia.org/wiki/Pacinian_corpuscle"&gt;Wikipedia&lt;/a&gt;]({{ url_for('static', filename = 'mr1_pacinian.png')}})

For example, the Pacinian corpuscles are these bulb-like things which leak calcium ions when they are deformed. This leaked Ca causes nerves to fire at the base of the bulb, and the more deformation, the more Ca, and the faster the nerves can fire. Therefore this receptor bulb sends its info in a frequency-encoded fashion. But how fast? What electrical stimulation speed would I need to "pretend" to be a Pacinian corpuscle that's feeling a vibration?

&lt;p class="caption"&gt;Off-the-shelf TENS unit I used for this testing&lt;/p&gt;
![Off-the-shelf TENS unit I used]({{ url_for('static', filename = 'mr1_tens.jpg')}})

Despite this limitation, I still found something cool. By placing a set of pads on the front of the bottom segment of my fingers, I was able to simulate a tight squeezing sensation on my finger. It felt as if my finger were being pressed on all sides by a vise.

&lt;p class="caption"&gt;One electrode on the back of my hand, one on the front bottom pad of a finger&lt;/p&gt;
![One electrode on the back of my hand, one on the front bottom pad of a finger]({{ url_for('static', filename = 'mr1_elecplacement.png')}})

I'm using pulses of a few hundred microseconds, at a frequency of 70+ Hz. Lower than that, and the sensation no longer felt like squeezing. There's also a persistent buzzing feeling from the TENS, and it is on-par with the strength of the squeezing. A bit distracting, but the squeeze is cool. 

I'm assuming the mechanoreceptors I'm stimulating are the Merkel discs, since squeezing is really a sustained pressure. You could argue I'm feeling a momentary pressure multiple times per second, but I think that's just the same thing as a sustained pressure. 

## Applications
Ideally we get all the mechanoreceptors stimulating separately, since this would let us produce a whole world of different textures when we touch objects in VR. Side note, does anyone want to make a VR game together? We can call it Firm Handshake Simulator 2022.

We could also use it for finger-specific tasks like learning piano and guitar, or sign language. This could be used as a finger indicator. Besides that, I dunno.

Cya next time!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 23 May 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>MacGyvering Electronics From Merch</title>
      <link>blog/macgyver1</link>
      <description>TFW no batteries :(</description>
      <content:encoded>Hello! I moved into Chicago yesterday and needed to do some electronics. Specifically, I needed a battery. But I had left my LiPo bag at home, and my 9Vs with it! What to do?

## Merch to the rescue!

My friend had received a small, rechargeable ring light from a CMU thing. Since it had been made this year and not in 2008, I had figured it probably had a charging circuit and LiPo in it, which would be useful in the future. Today was that day!

&lt;p class="caption"&gt;Ring light on, courtesy of CMU&lt;/p&gt;
![Ring light on]({{ url_for('static', filename = 'mcg1_ringlight.jpg')}})

Sam, if you're reading this, sorry for what you're about to witness. 

&lt;p class="caption"&gt;Cute little LiPo powering the ring light&lt;/p&gt;
![Cute little LiPo in the ring light]({{ url_for('static', filename = 'mcg1_battery.jpg')}})

So I pried off the back hinge and spring, then broke my thumbnail on the front panel. Tried again with my nail clipper while fixing the rest of my thumbnail, and this revealed what I knew had to be in there: the tiniest little LiPo I had ever seen. 

Here's a better view of the circuit. I couldn't identify two of the chips here. 

&lt;p class="caption"&gt;Cheapest way to charge the cheapest LiPo&lt;/p&gt;
![Cheapest way to charge the cheapest LiPo]({{ url_for('static', filename = 'mcg1_circuit.jpg')}})

The top half is connected to the button which turns on the light. There's 3 levels of light + off, so maybe a 4-state... resistor? Another current limiting device? Not exactly sure. But the SOT-23 is a low-side MOSFET, and the battery's ground connection goes through the other chip before reaching the FET. 

Bottom half is the battery charger. Looking at the resistors, it appears they're using a resistor divider with 5100Ω and 1200Ω to step the USB's 5V down to 5 x 5100/(1200+5100) = 4.04V, with at least 0.8mA going across the 1200Ω. Astounding cost saving measure. I'm not sure how the resistor is handling this, since almost 1mA across 1200Ω is 1W wasted heat, but props to them.


And of course, I didn't even have a soldering iron. Two alligator clips saved the day, scavenged from another project.

&lt;p class="caption"&gt;Alligator clips dangerously close to each other&lt;/p&gt;
![Alligator clips dangerously close to each other]({{ url_for('static', filename = 'mcg1_alligator.jpg')}})

In the end, this worked for my purposes, except that it also let in current the other way. I had to add an LED to the path since I didn't have a real diode -.- but that's the last of my electronics adventure today.

## Did you learn anything?
In the middle of doing this whole janky process, I went to CVS to buy scissors to cut some wire and didn't think to just buy a 9V. Serves me right for trying to be clever, next time I'm going for pragmatic.

Cya next time!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 31 May 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Recreating Traxion</title>
      <link>blog/traxionreproduction</link>
      <description>Only 8 years late!</description>
      <content:encoded>Hello! I recently learned of this paper called [Traxion](https://lab.rekimoto.org/projects/traxion/), wherein a genius visionary named Rekimoto discovered that asymmetrical vibration with a linear vibration motor caused a "pulling sensation" on a user's finger. A fake force that feels 32 grams of real force! Amazing!

To tell the truth, I straight-up couldn't believe it until I tried it. With my limited setup, I produced 3V using an LM317 (powered by a 9V I acquired), then drove a transistor with an Arduino to generate the ramp/square wave I needed. 

&lt;p class="caption"&gt;Traxion setup&lt;/p&gt;
![Traxion setup]({{ url_for('static', filename = 'traxion_setup.jpg')}})

Inside the linear actuator, there's a little weight anchored by spring to the center position of the body. This weight is what moves forward when we give the device a HIGH voltage, and it jerks back when we leave it unpowered. This "asymmetry" is what creates the sensation of a force. 

According to this more recent [paper](https://sci-hub.st/10.1109/HAPTICS.2016.7463151), the effect is caused by the skin dragging more in ON direction than when it turns OFF. This is due to the forward ON happening faster than the spring can pull it OFF. Something about detectable vs. unnoticeable acceleration. 

The authors drive their device with a high plateau, followed by a slow ramp down. This eases the spring-back while keeping the ON switch waaay fast. They claim the effect is strongest at 40Hz, but I found that just a simple duty cycled square wave (100Hz, 20% duty cycle) does the effect too, just less. 

&lt;p class="caption"&gt;Linear actuator I used. The little white pin is the weight, and it creates the force sensation in the length-wise axis&lt;/p&gt;
![Linear actuator I used. The little white pin is the weight, and it creates the force sensation in the length-wise axis]({{ url_for('static', filename = 'traxion_linearact.jpg')}})

## Note
This may sound silly, but I was surprised to feel vibration. Yea, yea, the thing is turning on and off really fast, but the paper only ever mentioned the force that it produced! I can't be blamed for wanting the vibration to go away to leave us with a free, fake force produced by a tiny vibrating motor. Maybe that's too easy though.

I want to see if this can be done on a larger scale, maybe from a handheld object? (VR controller would be cool). Though vibrating things are hard to hold onto. 

That's all for now, cya later!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 02 Jun 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Finding My Pacinian Corpuscles in vivo</title>
      <link>blog/mechanoreceptor2</link>
      <description>If only Galen had had a vibrating motor</description>
      <content:encoded>&lt;style type="text/css"&gt;
    p{
        margin-bottom: 1.5em;
    }
&lt;/style&gt;

Hello! I found out you could identify the exact location of one of your mechanoreceptors under the skin without needing a surgeon to cut you open, and I'm gonna show you how it's done. 

&lt;br style="line-height: 1.5em;"&gt;

## Background on mechanoreceptors
I have four unique mechanoreceptors embedded in my skin, and you probably do too. These are like little sensors in our skin which detect sensations like pain, hot and cold, vibration, and pressure.

I read in a psychology textbook that out of the four mechanoreceptors, the two that live in deeper tissue are much less plentiful than the ones that live in the shallow tissue. These deeper ones are the Ruffini endings, which gauge skin stretch near joints, and the Pacinian corpuscles, which detect higher frequency vibrations. They are so sparse, in fact, that you can individually locate them on your skin surface. 

&lt;p class="caption"&gt;MRI scan of the Pacinian corpuscles in a corpse. &lt;a target="_blank" href="https://www.semanticscholar.org/paper/Pacinian-corpuscles%3A-an-explanation-for-palmar-on-Rhodes-Murthy/dfc1c17d4cff70596b4bc3c913e5fef8ef69eeaa"&gt;Source&lt;/a&gt; &lt;/p&gt;
![MRI scan of the Pacinian corpuscles in a corpse ]({{ url_for('static', filename = 'mech2_pacinian.png')}})

The Pacinian corpuscles are the focus of this post since their diameter is about 1mm, which makes them visible to the naked eye. I won't add the photo, but if you google it you'll find photos of a dissected hand with the Pacs exposed. They're huge compared to the other receptors!

I'm abbreviating the Pacinians as Pacs, cause I'm gonna wind up using the word a lot. 

Anyway, My textbook claimed that the Pacs had a big receptive zone that they could sense vibration over, and that they had a peak sensitivity directly over the Pac itself. Looking at this MRI scan, I figured I'd try to find my hand's vibrational "hotspots" using a tiny vibrating wire. 


&lt;br&gt;
# Hardware Setup
I always keep some linear resonant actuators on me, and they came in handy for generating a vibration of a specific frequency. These are just small, self-contained solenoids. I taped two to a jumper wire, and this acted as my vibrating point. I chose a higher vibration frequency of 500Hz to avoid the receptive ranges of my other mechanoreceptors. 

&lt;p class="caption"&gt;Pacinian Stimulator 3000&lt;/p&gt;
![Pacinian Stimulator 3000]({{ url_for('static', filename = 'mech2_pacstim.jpg')}})

I then poked this point all over my hand for about 2 hours. The Pacs are "rapidly-adapting" mechanoreceptors, meaning that they only respond to new stimulus and not to sustained stimulation. This meant each time I poked my skin, I felt a vibration sensation which immediately went away within half a second. So I had to do it a lot. 

I had some luck though. By repeating pokes over the same area a few times, I felt certain points that were especially sensitive. Not only would it feel like vibration, it would feel like a little spike or path of sensation travelling up my wrist/hand. I marked these points with a pen tip.

&lt;p class="caption"&gt;Dots of high vibration sensitivity on the back of my hand&lt;/p&gt;
![Dots of high vibration sensitivity on the back of my hand]({{ url_for('static', filename = 'mech2_pacback.jpg')}})

Can I confirm that these were where my Pacs lived in my skin? No, I'm not going to actually cut myself and check. But the distribution lines up, and the sensation duration and localization also make sense to me. I don't think that the other mechanoreceptors would have been able to detect the 500Hz vibration frequency super well. 

The palm was a bit harder to feel differences in sensation, since the skin thickness also varies from the pads to the joints. Here's a few spots I found on my palm:

&lt;p class="caption"&gt;Dots of high vibration sensitivity on my palm&lt;/p&gt;
![Dots of high vibration sensitivity on my palm]({{ url_for('static', filename = 'mech2_pacpalm.jpg')}})

&lt;br&gt;
# Aftereffects
I held the spike with my right hand, and stimulated my left. I held it loosely, but still felt vibration in my right hand for a long time. At the end of the trial, my right hand had some weird sensations. For the next 30-45 minutes, whenever I tapped my index finger and thumb together, I felt a bouncing/springy sensation. Like hitting a drum, but the drum is just two of my fingers. It worked when I tapped other surfaces too, like my table. This felt strongest in my index finger, but it also worked for my middle finger and thumb. Really weird feeling, but probably not useful for anything. 


&lt;br&gt;
# Closing Notes 
Interestingly enough, there were a few spots on my back-hand that felt no vibration at all. Maybe a dead zone with no nearby Pacs? Blame my mom, I guess.

It was hard to ignore the feeling of the vibrating point poking me. I thought that the sharp-ish point coupled with vibration might actually damage the skin, and my skin did start hurting wherever I poked vertically. I found it hard to separate this sensation from the vibration I was looking for.




Cool effect! I have now separately stimulated the Pacs and the Merkel discs. Two down, two to go.
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 16 Jun 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Paper Delivery: Sinusoidal Skin Displacements</title>
      <link>blog/sinepaper</link>
      <description>Retro papers you probably missed, but shouldn't have!</description>
      <content:encoded>I've been reading a ton of research about how our touch receptors work. You may think it's simple, and it's not. I'm gonna show y'all a seminal paper on how our nerves react to mechanical sensation. When they dropped this supa hot fire in 1982, it inspired a lot of research which underlies our current understanding of how our brain processes touch.

# They're made of meat
Quick recap: there's 4 mechanoreceptors in our skin, and they each detect something different; low and high frequency vibrations, pressure, and skin stretching. Each receptor is a special structure specially designed to release ions in response to a very narrow kind of stimulus. 

Each receptor is attached to a nerve cell, and triggers action potentials in the nerve only when their stimulus occurs. The stronger the stimulus, the faster the receptor dumps ions onto the nerve, and the more often their action potentials get fired. 

These nerves follow a direct path to the brain, and each nerve gets its own "column"-ish of brain cells that receive and process its impulses. So when someone touches your finger, they are firing neurons in on particular, consistent location on your brain's surface. This is how our brains know where we were touched. 

But how do our brains know *what* we touched? How do our neurons interpret that chain of action potentials from the hand's receptors into the sensation of touching fabric, or holding a wooden block? 

&lt;p class="caption"&gt;Is this a response from touching a cat, or a dog? Your brain knows, even if you might not!&lt;/p&gt;
![Is this a response from touching a cat, or a dog? Your brain knows, even if you might not!]({{ url_for('static', filename = 'sinu_singlenervefirings.png')}})

# Old news
Because the fire rate was the most obvious change in response to stronger stimulus, scientists used to think that the neurons talked to each other using average firing frequency. This is called a rate code. 

However, this hypothesis is being challenged today. The brain can react to certain stimulus faster than it takes for a receptor to fire twice, for instance in reflexes. Since the neuron needs at least two firings to determine a "rate", researchers guess that there must be some other information present in a single firing spike besides just how many there are. 

Though they don't really talk about neural coding at all, the paper ["Responses of Mechanoreceptive Afferent Units in the Glabrous Skin of the Human Hand to Sinusoidal Skin Displacements"](/static/johansson1982sinu.pdf) offers a compelling reason to discard the rate code hypothesis. 

# Paper's Setup
Research papers always take a long time to get to the point. In this paper, they poked a plastic rod into a participant's skin, directly over one of the four mechanoreceptors. 

They modulated the rod in a sinusoid, and kept halving its total movement length, starting at 1mm and stopping at 0.125mm peak-to-peak total travel. 1mm is considered 0dB, 0.5mm is -6dB, and so on.

&lt;p class="caption"&gt;Sinusoidal stimulus used to move the rod, and the action potentials produced&lt;/p&gt;
![Sinusoidal stimulus used to move the rod, and the action potentials produced]({{ url_for('static', filename = 'sinu_stimulus.png')}})

The rod was moved for 5 periods of a sinusoid, at a bunch of different frequencies (powers of 2 from 0.5 to 256Hz, then also one at 400Hz). 

How'd they find these mechanoreceptors anyway? They used something called microneurography, or "stab tiny needle into the nerve repeatedly until you only see one mechanoreceptor spiking when we poke the victim's hand with a stiff hair". Kinda cool that our body just works like that, and we can exploit it so well. 

# Graphs please?
Alright, alright! Here's the averaged responses for the PC, or Pacinian corpuscle. These are egg-shaped receptors that sense high frequency vibrations. The field typically uses 1 impulse/stimulus as a breakpoint to indicate the active range of a mechanoreceptor. For the PCs at full amplitude, they are all-sensitive.

&lt;p class="caption"&gt;Responses per sinusoidal period for all recorded Pacinians&lt;/p&gt;
![Responses per sinusoidal period for all recorded Pacinians]({{ url_for('static', filename = 'sinu_PCchart.png')}})

This graph is what provides the really good reason against the rate coding hypothesis. Consider the 0dB line for 1Hz and the -6dB for 16Hz (marked in orange). The firing rates are nearly the same, yet we can clearly tell the difference between a 1Hz and 16Hz movement! 

Clearly, there is a little more going on than simple rate coding. 

# If Not Rate, What Else?
Current researchers are looking into using relative lag between first firing times from a local group or "population" of mechanoreceptors. Relative latency was used in 2008 in a salamander retina with great success:

&lt;blockquote class="twitter-tweet tw-align-center"&gt;&lt;p lang="en" dir="ltr"&gt;In a 2008 paper, researchers measured a salamander&amp;#39;s eye to see how retina cells encoded dark vs bright light when they talked to the brain. Instead of spike count or firing rate, ganglions changed their activation time relative to other ganglions near them. &lt;a href="https://t.co/9mh86s6cwi"&gt;pic.twitter.com/9mh86s6cwi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy (@oldestasian) &lt;a href="https://twitter.com/oldestasian/status/1407171439359311874?ref_src=twsrc%5Etfw"&gt;June 22, 2021&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

Other peeps are using the interspike period, or burst gap, or silent gap between series of nerve firings from a single nerve to determine the frequency of vibration or texture that's being touched.

I think it's some combination of latency across multiple populations, and maybe the number of spikes being sent. But what do I know? Just that there's a lot more to learn about one of our most foundational senses! Cya later!

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 22 Jun 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Comparing The Sugar Percentage Of A Peach vs. Sugary Drinks</title>
      <link>blog/refractometertesting</link>
      <description>Testing out a sweet scientific instrument</description>
      <content:encoded>It's a shame: I've met many smart people who were never educated in the fine art of slapping a watermelon to determine their ripeness by their (usually immigrant) parents. I thought I could do these poor folks a great service by using technology to help them figure out a watermelon's sweetness for them.

&lt;p class="caption"&gt;Refractometer on the left. The liquid goes on the blue part at the top. The bar chart on the right is what you see through the peephole. &lt;/p&gt;
![Refractometer on the left. The liquid goes on the blue, tilted part. The bar chart on the right is what you see through the peephole. ]({{ url_for('static', filename = 'refractest_displaypic.png')}})


The first step is determining how sweet the watermelon is, so of course I had to go buy a refractometer. As it turns out, adding sugar to water changes how much light rays bend when they go through the water, AKA the index of refraction (IOR). A refractometer lets anyone look through a peephole to compare an unknown sugary liquid to water (0% sugar). By looking at where the new line of intersection is on the graph, we can figure out exactly how many degrees Brix there are. Brix is just a complex way to say % sugar by weight. 

&lt;p class="caption"&gt;Arrangement of all the liquids I had to try&lt;/p&gt;
![Arrangement of all the liquids I had to try]({{ url_for('static', filename = 'refractest_showcase.jpg')}})

After I got the refractometer, I really wanted to make sure it gave out accurate numbers for both fruit and regular liquids. It just so happened that there were several sugary drinks in my pantry leftover from a club party, which listed their grams of sugar and grams of total mass under Nutrition Facts. Using this, I'd be able to determine the proper sugar content before confirming it with my refractometer. 

&lt;hr&gt;
# Onto the results!!

In all, I measured 3 drinks (Caprisun, Coke, Izze Green Apple), 1 fruit (peach), and 1 syrup (Hershey Caramel). I'll present them in increasing order by sugar content

## Caprisun (7.9˚ Brix)
The refractometer hit this one on the head, which was helped by the fact that the Caprisun is so liquidy and has very few other ingredients

True sugar: Grams carbs / grams total = 14g / 177g = 7.9%&lt;br&gt;
Refractometer: 7.9% 

&lt;p class="caption"&gt;Caprisun, Pacific Cooler edition&lt;/p&gt;
![Caprisun, Pacific Cooler edition]({{ url_for('static', filename = 'refractest_caprisun.png')}})


## Izze Green Apple (9.7˚ Brix)
A little further off on this drink. It might have been because of the carbonation forming bubbles in the sample.

True sugar: 24g / 248g = 9.7% &lt;br&gt;
Refractometer: 9.4%
&lt;p class="caption"&gt;Izze Green Apple. Tastes like green apple.&lt;/p&gt;
![Izze Green Apple. Tastes like green apple.]({{ url_for('static', filename = 'refractest_izze.png')}})


## Regular Coke (10.5˚ Brix)
This number was a bit harder to find. Turns out a 12oz can of Coke is 355mL, but the density is a bit higher than that of water (1.042 g/mL), which affected the denominator. 

The change in density may also be the reason the Izze's reading is off. Using the density of Coke at the volume of Izze gives a true sugar rating of 9.39%, which is exactly what we got from the refractometer.

True sugar: 39g / 369.9g = 10.5%&lt;br&gt;
Refractometer: 10.3%
&lt;p class="caption"&gt;Coke had the highest amount of sugar by weight, but not by much (~2.6% higher than the lowest, Caprisun)&lt;/p&gt;
![Coke had the highest amount of sugar by weight, but not by much (~2.6% higher than the lowest, Caprisun)]({{ url_for('static', filename = 'refractest_coke.png')}})

## Peach (8.7˚ Brix)
I juiced a slice of this peach, but forgot to take a photo before beginning to eat the rest. No ground truth for this one. 

Refractometer: 8.7%
&lt;p class="caption"&gt;So, still think fruit is healthy?&lt;/p&gt;
![So, still think fruit is healthy?]({{ url_for('static', filename = 'refractest_peach.png')}})

## Hershey Caramel (61˚ Brix)
Because this liquid is so syrupy (it is syrup), I had to dilute it down. My refractometer range also stops at 38˚ Brix. I added 1 part caramel, 9 parts water and mixed it until uniform. I then added this liquid to the refractometer and read it. 

True sugar: 25g / 40g = 62.5%!&lt;br&gt;
Diluted 10x: 6.25%&lt;br&gt;
Refractometer: 6.1%
&lt;p class="caption"&gt;Majority sugar caramel syrup&lt;/p&gt;
![Majority sugar caramel syrup]({{ url_for('static', filename = 'refractest_hershey.png')}})


# Closing notes
## Refractometer Limitations
I had a lot of fun running these experiments, partly because I got to sip sugarly liquids while doing it. I'm also shocked at how accurately my Amazon refractometer performed. However, there were a few challenges with using the refractometer that I want to mention.

1. Sugar and salt are confounding; both raise the index of refraction at similar concentrations. This must be adjusted for in higher-salt liquids.
2. Carbonated drinks bubble up underneath the test plate, which can't be good for accuracy. 
3. Limited range of 0-38% sugar means dilution is necessary for some solutions, which can add error.
4. Lack of density knowledge about Izze or Coke may have also skewed my numbers. As I mentioned above, adjusting the weight of Izze using a higher density gave an alternate sugar percentage that matched exactly what I read on my refractometer.
5. No precision or digital readout means measurements are slow and annoying to perform manually. 

## Weird findings
The variance in the sugar % of drinks is surprisingly small. To be fair, I only did a small number of drinks, but they only varied by 2.6% despite tasting wildly different. 

Also, Caprisun is healthier than peaches if we're going by sugar percentage. Tell that to your mom next time she berates you for picking an "unhealthy" snack!

Now, onto the humble watermelon!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 23 Jul 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>How to get access to your Fitbit data</title>
      <link>blog/fitbit1</link>
      <description>Navigating a slightly complicated process for API noobs</description>
      <content:encoded>I bought a Fitbit to access my heartrate live, or at least to track the way it changes over time. After wearing it for about two weeks, I figured I had enough data to begin requesting it through their API. However, I got confused because there's a bunch of different API names and webpages. I figured it out, and here's how you do the same.

My end goal is a command line GET request that dumps my heartrate logs, for use in Python scripts or a website. To get there, we will use the Web API.

# 1. Sign in and register your app [here](https://dev.fitbit.com/apps/new)
Any URL will do for every URL they ask you for. For every URL I just used my personal website (the one you're sitting on now!). This is meant for external users, so when they agree to let you access their data you can get their authorization token. However, I'm just setting it up once, so I'm ok with manually snipping out my keys and such from the URL redirect. 

I also selected "Personal" while registering my app, so I can get higher fidelity heartrate data.

&lt;p class="caption"&gt;Register a Fitbit app page&lt;/p&gt;
![Register a Fitbit app page]({{ url_for('static', filename = 'fitbit1_register.png')}})


# 2. Head to "Manage My Apps" and click on "Oath 2.0 tutorial page" [[link here]](https://dev.fitbit.com/apps/oauthinteractivetutorial)

I left all the settings default and clicked the link under step 1. Accept the agreement, and you will be redirected to the page you filled in earlier, with a bunch of params after a "#". Copy everything past the "#" and paste it in the "2. Parse response"

After pasting, the site should auto-generate you an "API endpoint URL: `https://api.fitbit.com/1/user/-/profile.json`" as a `curl` command. It redirects to your own profile though. To ask for more useful information, you can check out the Web API reference [here](https://dev.fitbit.com/build/reference/web-api/heart-rate/). For instance, to get heartrate, the API endpoint is `https://api.fitbit.com/1/user/-/activities/heart/date/today/1d.json`. Paste this in, and you should see the command change. 

I'm not going to show you any commands I ran, that would give away my Auth token. Here's the results of the heartrate one though. Granularity goes down to 1 second, though I'm not sure why you would need that. 

&lt;p class="caption"&gt;Successful heartrate output from the terminal&lt;/p&gt;
![Successful heartrate output from the terminal]({{ url_for('static', filename = 'fitbit1_heartrateout.png')}})

This is a short post, but I wasn't sure what else needed to be done while I was navigating the process. Turns out it's just two things. Cya next time!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 26 Jul 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Sugar-Metering My First Watermelon</title>
      <link>blog/firstwatermelon</link>
      <description>Tastes like Capri-sun! Bonus: Lychee and Longan</description>
      <content:encoded>I recently [bought and tested](../refractometertesting) a refractometer's ability to determine the sugar content of various drinks to gain a better awareness of the stuff I eat. As promised, I have returned with more stats on fruit!

# Watermelon Sugar - high!

I bought a small watermelon and cut it up into bowls, which are by far the most fun way to eat a watermelon. I ate about half the bowl, then separately juiced up part of watermelon flesh in the very center and some near the wall (still red, but right beside the white flesh). 

The accumulated juice from each part of the watermelon was collected and measured visually in my refractometer. Since each watermelon gives two bowls, I tested both regions in both bowls to have more precise numbers. Here's the refractometer pics:

&lt;p class="caption"&gt;Watermelon center and edge sugar concentrations&lt;/p&gt;
![Watermelon center and edge sugar concentrations]({{ url_for('static', filename = 'wm1_watemelonrefract.jpg')}})

The juice near the center of the watermelon was around 7.9% and 8.1%, which is about the same as the Capri-sun I measured last time. The juice near the edges was 6.4% and 5.8%, so a few percentage points lower than the center. 

# Bonus, Lychee and Longan

I also had some Asian fruit on-hand, the juicy lychee and longan. I could taste that the Longan were a bit sweeter, but I wasn't sure how much. 

&lt;p class="caption"&gt;Tasty Lychee fruit&lt;/p&gt;
![Tasty Lychee fruit]({{ url_for('static', filename = 'wm1_lycheepic.jpg')}})


They turned out incredibly high, with the longan juice twice as sweet as Coke!

&lt;p class="caption"&gt;Lychee refractometer photo, 18.8% sugar&lt;/p&gt;
![Lychee refractometer photo, 18.8% sugar]({{ url_for('static', filename = 'wm1_lycheesugar.jpg')}})
&lt;p class="caption"&gt;Longan refractometer photo, 20.8% sugar&lt;/p&gt;
![Longan refractometer photo, 20.8% sugar]({{ url_for('static', filename = 'wm1_longan.jpg')}})

The Lychee are 18.8% sugar, and the Longan are 20.8%! Off by 2%, but on a whole different level. I'm sure I'm getting extra nutrients with these fruits, but that's a lot of sugar for a plant! How'd they figure out how to do that? Was it selective breeding? Incredible. We're lucky the Americans haven't found out about these *en masse*!

## Brief Discussion

This watermelon tasted regular sweet to me, maybe a bit on the light side. IMO, a good watermelon would taste like the center of this one, but throughout the whole fruit. I'm just spoiled by my ability to pick good watermelon. 

It's interesting that such a sugar concentration gradient (6% and 8%) can exist in a medium that is mostly water; maybe over time the concentration stabilizes from center to edge? Does the watermelon get sweeter too, as it ripens further? 

I forgot to pat the melon before cutting it up, so I get no data points about how the sound correlates to the watermelon's sweetness. I'll have to buy another and record it.

Amazing sugar levels in Longan and Lychee though. They are sure worth the price! I'm sure the actual sugar percentage including fruit solids is lower, but the juice is super high in sugar.

Cya with my next watermelon!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 27 Jul 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Supercharging Capacitors</title>
      <link>blog/superchargingcaps</link>
      <description>Who has time for 5RC?</description>
      <content:encoded>I recently thought about a way to charge capacitors really quickly. 

If you think about it, capacitors are sort of like buckets that you can fill with electrons. Their peak level is their voltage rating, and if you keep filling them they explode. Usually this means you only charge capacitors using a voltage less than or equal to their max voltage, to avoid the explosion.

But! This method sucks, and it's slow. As your cap fills up, the voltage difference between your source and the capacitor goes down, which makes your charging current go down too. Diminishing returns means that really big capacitors can take several seconds to charge up to a desired voltage. 

# Better method
Ideally, we'd use some massive voltage source for charging, and then turn it off immediately when our capacitor hits the final voltage. We only use the previous method because we humans have slow reaction times, and can't turn off the voltage source quickly enough to avoid a potential capacitor explosion.

But I'm *pretty sure* we've invented faster voltage switches than the human hand.

# Enter, the humble microcontroller
![Arduino-enabled fast off switch]({{ url_for('static', filename = 'sc1_setup.jpg')}})


All I did was connect an Arduino-controlled MOSFET to a 2200µF capacitor. The Arduino reads the voltage of the capacitor, and shuts off the MOSFET when the voltage gets past our set point (4V). I used 5V, 10V, 20V, and 30V for charging. Here's how long each one took:

&lt;p class="caption"&gt;Charging with 5V&lt;/p&gt;
![Charging with 5V]({{ url_for('static', filename = 'sc1_chargeto4with5.jpg')}})

&lt;p class="caption"&gt;Charging with 10V&lt;/p&gt;
![Charging with 10V]({{ url_for('static', filename = 'sc1_chargeto4with10.jpg')}})

&lt;p class="caption"&gt;Charging with 20V&lt;/p&gt;
![Charging with 20V]({{ url_for('static', filename = 'sc1_chargeto4with20.jpg')}})

&lt;p class="caption"&gt;Charging with 30V&lt;/p&gt;
![Charging with 30V]({{ url_for('static', filename = 'sc1_chargeto4with30.jpg')}})

In text form, charging a 2200µF capacitor to 4V took:

- 5V -&gt; 5.6ms

- 10V -&gt; 1.7ms

- 20V -&gt; 0.76ms

- **30V -&gt; 0.43ms**

My power supply caps out at 30V, and that gives you a **13x speedup** in capacitor charging time!

# Theoretical speedup
We've seen how the real world does it; how about comparing it to theory?

Here's the equation that governs how fast a capacitor charges. It calculates the cap's voltage given some time *t* connected to a voltage source Vs.

&lt;p class="caption"&gt;Capacitor voltage after some time t, charging with a voltage Vsource&lt;/p&gt;
![Capacitor voltage after some time t, charging with a voltage Vsource]({{ url_for('static', filename = 'sc1_chargeVeq.png')}})

Solving this backwards for *t*, we have some terrible looking equation for how long it would take to charge a capacitor given a source voltage.

![Above equation solved for time t]({{ url_for('static', filename = 'sc1_chargeTeq.png')}})

Plugging in our previous charging voltages and magically using R=1.5Ω, we get the following charge times:

- 5V -&gt; 5.3ms

- 10V -&gt; 1.7ms

- 20V -&gt; 0.73ms

- **30V -&gt; 0.47ms**


Greater than 10x theoretical speedup at 30V! And pretty good agreement with the numbers I measured. 

# Theoretical mismatch

Some of my times were faster than the theoretical times. What could cause these faster-than-theoretical real world charging times? 

- Volt drop across the MOSFET? The MOSFET is fairly low resistance (IRFZ44NPbF claims 0.0175Ω at max), so that can't be it. 

- Lower R_cap than 1.5Ω? Possibly, but I'd just be guessing.

- Perhaps the Arduino got a bit overzealous and cut off the capacitor before it truly charged to 4V? This seems like the case from some of the charging graphs &gt; 10V. For some reason, the reached voltage hit 4V, the Arduino shut off the MOSFET, and then the capacitor voltage dropped a bit. I'm not quite sure why this happened.

# Conclusions
I used an Arduino for reading voltage, meaning that I only have an effective range of 0-5V. I can get a higher effective voltage range if I buffer and divide the capacitor voltage down before `analogRead`ing it. Maybe even a resistor divider would work.

The only way my program was able to run fast enough on the Arduino is because there was nothing in the loop except the voltage checking. With that, it was able to stop the capacitor under a few hundred us. With a faster µC, we could use even higher charge voltages and still be sure we'd turn it off in time.

A friend suggested I use voltage prediction to get a more precise turn-off time for the capacitor, sorta like an instant-read thermometer. I forgot about that until after I finished, but it's probably a good idea.

# Head Fake

This post is actually about supercapacitors. Why go through the trouble to find a high current 3V source when you could just use any old voltage source and turn it off in time? 

Hell, for a large supercap, it'd charge slowly enough that you could probably disconnect it yourself. Just make sure you don't exceed your source or capacitor's current limit. I AM NOT RESPONSIBLE FOR BURNT CAPACITORS / FINGERS / HOUSES. 

Be safe. Cya around!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 09 Aug 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Measuring 150 Amps With A DIY Shunt</title>
      <link>blog/highcurrentsource</link>
      <description>Speccing a 150A@5V power supply, the NJE MK750</description>
      <content:encoded>Hello everyone! Today I'm going to show y'all my testing of a high current, low voltage DC power supply.

Typical bench power supplies will go up to about 30V, and output 3A at most. For more powerful power supplies, their focus will usually be on voltage, going up to kilovolts at only a few amps or milliamps. 

I recently had the need for the opposite kind of power supply, a high current and low voltage one, up to 200A at around 3V. I found this one on Ebay which seemed the fit the bill. Let me tell you all about it!

&lt;p class="caption"&gt;What could it be?&lt;/p&gt;
![What could it be?]({{ url_for('static', filename = 'mk750_backonly.jpg')}})

# How do you make 200A?
Before I bought this, I had juggled a few alternatives. There are many sketchy ways you can generate huge currents. A few I considered were: 

 - A) shorting out around 10 drone/RC car LiPos in parallel (burst numbers are higher, but about 20A sustained current per battery)

 - B) Rectifying and rewinding a microwave transformer to output 3V from the wall

 - C) Harvesting RAM power supplies from dead motherboards (Sketchy YouTube videos put this at 20A, 3V)

 - D) Buying a battery-tester power supply, which are at weird voltages (8V, 5V, 4V, 2V, etc.) but super high currents.

Option D beat out the others in terms of reliability, but those niche power supplies are quite expensive, even on eBay. Luckily, I found an older one that seemed to do exactly what I wanted. 

&lt;p class="MK750 eBay page. I may have bought the last one"&gt;&lt;/p&gt;
![MK750 eBay page. I may have bought the last one]({{ url_for('static', filename = 'mk750_ebaypage.png')}})


The NJE Corp. MK750, MK1000, and MK1500 were power supplies at their eponymous wattage, which output anything from 2V to 48V at massive currents. The only datasheet I could find was &lt;a href="{{ url_for('static', filename = 'mk750_datasheet.pdf')}}" target="_blank"&gt;this one&lt;/a&gt;, and there's not a ton of information on there (like what those 6 pins are for). But I needed this power supply, so I went ahead and ordered it. 

# Voltage Testing
The first thing I checked was the voltage. After wiring the AC, ACC, and GND pins (3 prong power), I plugged it into a wall meter and checked the output. It looked good, almost perfectly 5V. With no load, the power supply drew around 70W, or 0.5A from the wall. 

&lt;p class="caption"&gt;MK750 outputting 5V, as it should&lt;/p&gt;
![MK750 outputting 5V, as it should]({{ url_for('static', filename = 'mk750_5vconfirmed.jpg')}})

# Current Testing
What I *really* wanted to know was if the MK750 could output the massive currents its manual had promised. This was a bit trickier. I could have used a clamp, but I didn't think of that. Instead, I watched an ElectroBOOM [video](https://www.youtube.com/watch?v=j4u8fl31sgQ) about DIY current shunts and decided to use one of those.

Really, a DIY current shunt is just a wire. At 5V, to output 150A you need a load of 

$$ 5V/150A = 0.033Ω = 33mΩ $$

This is usually the resistance of a few feet of wire. Mehdi talks about how multimeters suck at measuring low resistances, but they are good at voltage. I cut a random length of a random wire off the wall, and passed 1A constant-current through it to find the resistance. The measured mV corresponded directly to the mΩ of the wire. 

&lt;p class="Thin wire of a good resistance, but not nearly enough current capacity"&gt;&lt;/p&gt;
![Thin wire of a good resistance, but not nearly enough current capacity]({{ url_for('static', filename = 'mk750_thinwire.jpg')}})

This bit of wire happened to be around 48mΩ, which was nearly perfect. However, just before I plugged it in, I realized that the entire output 750W of the MK750 would pass through this tiny 18 AWG  wire and vaporize it instantly. Immediately I threw the thin wire away, and went foraging for some thicker power wires. 

&lt;p class="Thicker wire of a perfect resistance"&gt;&lt;/p&gt;
![Thicker wire of a perfect resistance]({{ url_for('static', filename = 'mk750_thickwire.jpg')}})

We cleaned the club a few days ago, and had thrown out some chunky orange power cables. Each cable had 3 conductors, so I took a length of this and crimped alternating ends together to make a triply long cable in the length of one. I did the same test as before, passing 1A through the combined cable. This resistance turned out to be actually perfect, at 34mΩ. 


# Using the 3-wire shunt (34mΩ)
&lt;p class="3-strand shunt resistor all set up, multimeter right next to it"&gt;&lt;/p&gt;
![3-strand shunt resistor all set up, multimeter right next to it]({{ url_for('static', filename = 'mk750_threestrandbefore.jpg')}})

I connected our shunt to the power supply, then clipped my multimeter to the output. I'm measuring voltage to get the current. Since Ohm's law always holds, if we already calculated the resistance and then measure the voltage, we have everything we need. Time to plug it in!

&lt;p class="3-strand shunt resistor under test. Multimeter reads 5.14V"&gt;&lt;/p&gt;
![3-strand shunt resistor under test. Multimeter reads 5.14V]({{ url_for('static', filename = 'mk750_threestrandduring.jpg')}})

And the MK750 delivers! Across our 35mΩ wire we see a drop of 5.14V, which works out to
$$\frac{5.14V}{0.035Ω} = 147.7 \text{Amps}$$
Woohoo! Almost exactly 150 amps at 5V. The readout from the wall meter said around 900W, so approximately the baseline 50W + 150Ax5V. Not bad for efficiency.

Oh, and the wires got HOT. I only left it on for a few seconds for safety, but even then the thermometer said the wire got over 100F. Crazy!

&lt;p class="Our shunt temperature after only 10 seconds of 150A"&gt;&lt;/p&gt;
![Our shunt temperature after only 10 seconds of 150A]({{ url_for('static', filename = 'mk750_hotwire.jpg')}})

# Using a 2-wire shunt (23mΩ)
I wanted to see what this power supply was really capable of. I had avoided using a shunt lower than 33mΩ because I was afraid that shorting out the power supply would break it. Thinking further, I realized that 33mΩ is already considered a dead short in any other application, so it probably wasn't a concern. 

In our 3-stranded shunt, each wire was around 11mΩ. I moved the crimps so that it only used two wires, for a new shunt resistance of around 23.2mΩ. I reconnected everything to this smaller shunt, and switched it on. 

&lt;p class="A snapshot of the rising voltage of our 2-strand shunt"&gt;&lt;/p&gt;
![A snapshot of the rising voltage of our 2-strand shunt]({{ url_for('static', filename = 'mk750_twostrandduring.jpg')}})

As expected, the output voltage dropped a bit to compensate for the short. However, the voltage reading on my multimeter immediately started shooting up, beginning at 4.3V and climbing to 4.9V in the space of a few seconds. I assume this is from heat increasing the resistance of the wire, and shut it off quickly. 

Using the initial reading of 4.3V, Ohm's law says the MK750 output 185A at first, and a clamp meter agreed at around 170A. The wall meter read 1120W during this test.

&lt;video src="{{ url_for('static', filename = 'mk750_vid3.mov')}}" controls&gt;&lt;/video&gt;


# Closing thoughts
This power supply is amazing. Unfortunately, the NJE Corporation just filed for bankruptcy last year (2020), so I don't think we'll be seeing many more power supplies where that came from. 

It also smells weird, but that's completely fine given its the excellent performance otherwise. 

Cya next time!

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 06 Sep 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Real Time Fourier Transform Using Shaped Aperture</title>
      <link>blog/fourieraperture</link>
      <description>Light speed!</description>
      <content:encoded>Hello! I'm going to tell you something cool about light. Specifically, how to use apertures and lenses to perform the Fourier Transform of a 2D image at the speed of light.  

# Backstory
When I was still a wee boy, my computational photography professor mentioned that a lens performs a Fourier Transform on light that enters it. A Fourier transform of light is a frequency domain representation of the image, and captures the exact same information as the regular picture. We don't usually notice this property of lenses because it only happens to coherent light, like a laser. One example of this is how a bright image (the sun) becomes a single point at the focus of the lens, because there is a 0-frequency component to the brightness entering the lens. 

&lt;p class="caption"&gt;Example Fourier Transform of an image, specifically showing a text rotation application &lt;a href="https://homepages.inf.ed.ac.uk/rbf/HIPR2/fourier.htm"&gt;(source)&lt;/a&gt;&lt;/p&gt;
![Example Fourier Transform of an image, specifically showing a text rotation application]({{ url_for('static', filename = 'fa_textrotation.png')}})

Anyway, I noted this cool fact and carried on with my life

&lt;hr&gt;
# 1 Year Ago
About a year ago, I bought a coffee table book called "Laser Art and Optical Transforms", by a Mr. Thomas Kallard. The book contained a ton of photos of interesting laser effects that the author had created over his career as a lighting designer, and were catalogued down to the equipment he had used to make them. 

&lt;p class="caption"&gt;A copy of the book, mine is black&lt;/p&gt;
![A copy of the book, mine is black]({{ url_for('static', filename = 'fa_kallardbook.png')}})


I read the whole book (looked at pictures), and noticed that the back half of the book was entirely these aperture pictures. 

To make these aperture photos, Kallard took a laser beam that was spread out by a filter, then shot through a sheet of paper with a shape cut out. The resulting image on the far wall did not resemble the hole at all, due to the self-interference of the laser light shooting through the paper cut-out. If the light source were an incoherent light source like a lamp or projector, this resulting image would look very different, but we don't have to worry about that since Mr. Kallard did use a laser.

&lt;p class="caption"&gt;Mr. Kallard's setup&lt;/p&gt;
![Mr. Kallard's setup]({{ url_for('static', filename = 'fa_kallardapparatus.jpg')}})

Here's an example from the book, showing how changing the laser aperture shape changes the resulting image. The transform is slightly intuitive, but this reasoning breaks down with more complex apertures. 

&lt;p class="caption"&gt;Two dots vertically stacked make a circle with horizontal lines, two dots stacked top right and bottom left create a circle with diagonal lines.&lt;/p&gt;
![Two dots vertically stacked make a circle with horizontal lines, two dots stacked top right and bottom left create a circle with diagonal lines.]({{ url_for('static', filename = 'fa_kallardexample.png')}})

&lt;hr&gt;
# Last Week

I showed a friend my book, and he wondered aloud how the output images were being formed by the light's interference. Using what I learned from computational photography, I assumed that this was a similar principle, and that changing the aperture and changing the "image" coming into a lens were nearly the same thing. This would mean that the aperture pictures that Kallard had captured were probably just the Fourier Transforms of the apertures he was using. 

I had never tried it myself, but in that moment I remembered this web-based Fourier Transform [demo](https://homepages.inf.ed.ac.uk/rbf/HIPR2/fourier.htm) I had found that let you do your own images. In a flash, we had taken a few photos of the apertures and fed them into the web demo. Lo and behold, what should we find but near-matches to the actual output Kallard had recorded!


## 1. Circles
Here's some overlapping circles. The left image is the shape of the laser aperture, and the top-right image is the output that Kallard photographed. You can see the X and the overlapping arcs that face opposite directions on both, but the scale of the image is the only thing that changes.
&lt;p class="caption"&gt;Circle aperture + FFT&lt;/p&gt;
![Circle aperture + FFT]({{ url_for('static', filename = 'fa_circles.png')}})

## 2. Spiral-y thing
Similarly here, it's clear that the aperture on the left creates features like the 4 bright quadrant and the surrounding "fan" shapes in both the web and real world version, though they are a bit harder to see.

&lt;p class="caption"&gt;Spiral aperture + FFT&lt;/p&gt;
![Spiral aperture + FFT]({{ url_for('static', filename = 'fa_spiral.png')}})

&lt;hr&gt;

Kinda crazy! I'm surprised that taking the FFT of a photo worked so well, considering that the photo has all sorts of distortion and noise from the paper warp and real world effects.

I'm still not sure why the scale of the image was so zoomed for Kallard's images. The spread of the light depends on your angle, and Kallard had quite a large distance between his lens and the photographing wall. Perhaps if you were to recreate this using a closer camera, you'd get the same thing we got.

Either way, pretty dope. 



</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 20 Sep 2021 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>How Trees Fall Apart</title>
      <link>blog/howtreesfallapart</link>
      <description>The differing decay patterns of trees during fall</description>
      <content:encoded>You only get to experience 1 fall a year, and by the time you read this you're probably older than 15. This means you probably have only 65 falls left in you — remember to do your best to enjoy each one. 

Anyway, this particular fall I took more of a look at the trees around me. I noticed that the green-yellow-red transition of leaves followed different patterns for different trees, and took photos of all the ones I could. Here follows the decay patterns of 4 different trees.

&lt;hr&gt;

# 1. Top down
&lt;p class="caption"&gt;The decay here happens from the highest leaves to the lowest. &lt;/p&gt;
![The decay here happens from the highest leaves to the lowest. My favorite tree this year]({{ url_for('static', filename = 'topdown2.jpg')}})

# 2. Inside out
&lt;p class="caption"&gt;Core leaves brown completely while the outer ones are still green&lt;/p&gt;
![Core leaves brown completely while the outer ones are still green]({{ url_for('static', filename = 'insideout.jpg')}})

# 3. Outside in
&lt;p class="caption"&gt;My favorite tree of 2019 and 2020. The outermost leaves redden before the core&lt;/p&gt;
![My favorite tree of 2019 and 2020. The outermost leaves redden before the core]({{ url_for('static', filename = 'outside_in1.jpg')}})



# 4. All at once
&lt;p class="caption"&gt;No regard for order! All leaves turn red at the same time, perfect example of communism&lt;/p&gt;
![No regard for order! All leaves turn red at the same time, perfect example of communism]({{ url_for('static', filename = 'allatonce.jpg')}})


Evolutionarily, I'm not sure the decaying has a rhyme or reason. Maybe it makes sense to keep the outer ones green since they receive the most sunlight, but maybe it doesn't matter because red leaves can collect energy as well. Whatever the reason, it makes a beautiful sight. 


</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 13 Nov 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>How Good Are Stock Watchlists Anyway?</title>
      <link>blog/watchlists1</link>
      <description>Fact-checking Timothy Sykes</description>
      <content:encoded>Hello! Long time no see! In this post, I'm going to show y'all the behavior of stocks that were featured on a few of Timothy Sykes' watchlists earlier this year. 

## Background

I've been out for Winter Break for about 2 weeks now, and I've been thinking about a lot of stuff. I'm going to graduate from college after next semester (hopefully), and it's looking like I'll be attending grad school after taking a gap year. In my gap year, I'd like to learn how to make money without working for someone else (aside: if you have any tips for this please let me know). 

One way I'm considering is some algorithmic stock market trading, or at least computer-assisted. While researching this topic online, I stumbled upon several different traders publishing "watchlists" of stocks that they were considering for their own portfolios. These are not guaranteed, but rather stocks that they note as "potentially interesting" for pattern trading. 

Now personally, I think pattern trading is bull since all the graphs look the same to me. But I figured I should give these fellas the benefit of the doubt and at least try out their stock watchlists.

&lt;br&gt;

## Work
I signed up for [Alpaca Markets](https://alpaca.markets/), an algorithmic trading platform that also has a great Python API for real-time and historical stock data. I also browsed Mr. Sykes' old watchlists to get lists of stocks he had been promoting. Luckily, they had last-edited dates, so I could see the stock as it changed.

Using the API, I wrote a little scraper that takes a watchlist and startdate and shows the graphs of the stock prices after Mr. Sykes emailed them out to his eager audience. The results are disappointing. 

&lt;br&gt;

### 1. Regular September Watchlist
The graphs are all normalized price vs. time graphs, with the vertical red line indicating the date where Mr. Sykes shouted out this stock and the horizontal light blue line indicating a ratio of 1.0, or no price movement at all. 

Each stock's history is normalized to the opening price of the stock on the day Mr. Sykes published his watchlist, to simulate a naive investor buying each stock as soon as it gets endorsed.

![Regular September Watchlist Price Chart]({{ url_for('static', filename = 'wl1_sept.png')}})
We see that his "Top Stocks to Watch Today: Thursday, September 16" watchlist did not perform so well, only 1 of the stocks above the breakeven mark after 2 months. 

We also see the green and yellow spike that happened right before Mr. Sykes watchlisted this stock. While they were already spiking, his shoutout corresponded with even more spiking. 

&lt;br&gt;

### 2. Weed Stocks September Watchlist
If you think you're going to make money buying everything on this list, I wanna know what you're smoking and where you're getting it from. Only three stocks ever became profitable, and I'm not even listing the other 4 stocks from the canadian exchange.

![Weed Stocks September Watchlist Price Chart]({{ url_for('static', filename = 'wl1_weedsept.png')}})


&lt;br&gt;

### 3. Regular December Watchlist
Since only two of the stocks from this watchlist were in Alpaca's records, I made do. His watchlist was published about 3 weeks ago, and it seems that one has been doing very well!
![Regular December Watchlist Price Chart]({{ url_for('static', filename = 'wl1_dec.png')}})

&lt;br&gt;

## Open vs Close
I looked at only three of Tim's watchlists, chosen by order in which I saw them. However, I wasn't sure whether to post the open or closing prices. Here are the weed stocks, one normalized to open and one to close.

&lt;p class="caption"&gt;Price normed to close&lt;/p&gt;
![Price normed to close]({{ url_for('static', filename = 'wl1_weedclose.png')}})

&lt;p class="caption"&gt;Price normed to open&lt;/p&gt;
![Price normed to open]({{ url_for('static', filename = 'wl1_weedopen.png')}})

&lt;br&gt;
# Conclusion
No nilly-willy buys off some watchlist for me, get some insider trading if you can. And since all the stocks seem to go down, maybe Mr. Tim is just a great short salesman. 

Cya next time!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 25 Dec 2021 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Examples from the Taxonomy of Automation</title>
      <link>blog/taxonomyofautomation</link>
      <description>Is convenience pleasure?</description>
      <content:encoded>Hello y'all. I've been thinking about automation, specifically, intuitive interaction design's role in coddling people. 

My lab is called the Future Interfaces Group. We abuse hardware and software to make predictive interfaces and new interactions between users and technology. Every project needs a user study to verify usability or reliability — does the user like the interaction? How does the device compare to other ones like it? We think about these things to make devices that hopefully feel a bit magical, like they can tell what you're going to do before you do them.

I've been thinking about the usefulness of such research, especially since it doesn't feel like anyone's life is particularly impacted by the research we put out. While it's nice that the products we use are smooth and not frustrating, I sometimes feel that nobody would die or hate their lives if we took it all away. Do these things really build up into useful parts of someone's life? Or is it always just cruft, extra features and bloatware that nobody asked for? 

I'm getting off topic. Recently, I read a Tumblr blog called Crap Futures (I'm gonna abbreviate it as CF) which shared many of my thoughts about the apparent shittiness of the future we're creating. IoT devices everywhere that go down every week due to ransomware attacks or server outages does not sound like my idea of a good tech future. 

They wrote one particular post called [Scratch an itch: A taxonomy of automation](https://crapfutures.tumblr.com/post/180304398284/scratch-an-itch-a-taxonomy-of-automation) that really got me thinking about the degrees of automation in our lives. I'm going to summarize it here, but you should definitely give it a read too to see where I'm getting these ideas from. 

# Taxonomy of Automation
I see automation's actors as the human and the device/tech/automator, and I am going to explain CF's taxonomy of automation in terms of the actions: who does the sensing, and who does the action. And I liked CF's concrete example of scratching an itch, so I'll continue with that as my main focus

&lt;br&gt;

## Level 1: Human Sensing, Human Doing
On automation level 1, the human feels the itch and then reaches over to scratch it

## Level 2: Human Sensing, Device Doing
The human senses the itch, and uses a device to scratch it. This can be a stick or an ItchScratcher 3000.

## Level 3: Device Sensing, Device Doing
The device senses our itch (from imagery or something, use your imagination), then scratches it for us. 

## Level 4: Device Prediction
The device anticipates an itch, perhaps it occurs on a regular basis or shows a red spot before actually itching. The desire to scratch is circumvented entirely, through early anti-itch cream or pre-scratching. 

## Level 5: Device Omniscience
The device anticipates and even pre-supposes an itch. It can do this to sell repairs of itself, or to sell its own usefulness. Complete loss of desire control. 

&lt;br&gt;

# Examples
I've mentioned this to a few friends, and each one has thought of a few examples of devices at each level that we already use today. As we'll see, nearly everything sits on level 1 and 2.

At level 1, we do all the work that we've always done by hand. Scratching, massaging, feeding ourselves.

Level 2 contains all "dumb" tools - the shovel, the spatula, the TV remote. We sense a boring channel coming on and click the remote to change it. Most of our technology sits at level 2, including our phones and the Roomba.

Level 3 largely drops off and contains almost nothing because level 3 features a loss of autonomy. Most devices stop short of that. Most of these require user confirmation, but I'd say that's pretty close to just letting the device do it, we just don't trust them enough. 

- Our email client detects dates and offers to put them on a calendar, but it knows the limits of its own accuracy and doesn't create events on its own. 

- Auto-sharing wifi passwords and detecting lost devices are both features on this level. 

- GitHub Copilot also falls on this level, but it isn't quite good enough to be trusted.

And nothing falls beyond that. It's kind of sad that so few things live at Level 3, even given all our crazy machine learning advances in the past twenty years. Our devices stay tools, useful when we use them and not otherwise. 

&lt;br&gt;

# Cross-application to people
One thing that came up in my discussions is if anything sits past Level 4. I think our friends and family fall past level 3, since they're always looking out for us and can anticipate what we want pretty well (just think of your recent Christmas gifts!). And people besides them can sit anywhere on the spectrum. We can do a task ourselves (us sensing, us doing), tell someone what to do (us sensing, them doing), or tell someone what high-level thing to be working on (them sensing, them doing). Ideally they sit as high as possible on the levels. 

&lt;br&gt;

# Conclusion
I think we should be pushing robots as far up the automation hierarchy as possible. Each layer saves exponentially more time. The question is if the idea being automated is worth doing yourself, and if it is, why automate it? I close with a quote from the little prince

&lt;p class="caption"&gt;The Merchant passage from The Little Prince&lt;/p&gt;
![The Merchant passage from The Little Prince]({{ url_for('static', filename = 'tlpmerchant.png')}})

A question for later: Why do we value human level 3 much more than robot level 3? (I think it's related to the fact that we know humans have opportunity cost but don't really think about computers having the same). What if we reported robot operating cost whenever they did a task for us? 

Until then, cya around!




</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 03 Jan 2022 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Learning AppleScript</title>
      <link>blog/applescripts1</link>
      <description>Go Go Mac Powers</description>
      <content:encoded>Hello. I've always wanted to learn AppleScript in order to replace my Mac tracking apps and save a bit of money. Over this break, I had time to do that. And it seems to be about as hard as I expected it to be, which is to say it's simple to the point of being hard to learn. Here's an example

Applescripts is written in this natural language sort of format. You expect that makes it easier to learn and read and write, but it doesn't unless you know the syntax. Much like how a Python aficionado would find it difficult to explain to a newbie how they know of all the different built-in functions, I find it similarly hard to learn the types and values that variables can take on in an AppleScript. 

I've written a few atomic scripts and tips I wanted to share, and I'll show them in this post. Like usual, examples of other scripts helped me learn how to write my own. For Applescripts, I had an interesting problem that my example code wouldn't work when I began to switch in my own variables. This leads me to tip #1

# Tip #1: Do not name your variables "yes"
Everyone has a default name for trash variables that will be replaced by something more descriptive later, and mine happens to be "yes". However, if you name your variables yes in your Applescript, you are going to have a bad time. The script throws a funky error that doesn't mention that yes is a built-in word that cannot be assigned. 

# Tip #2: Applescript has no OOP, at least officially
This means when you try to write a more complicated Applescript and want to be a good little SWE who uses objects and functions, you will encounter various forum posts which tell you that AS is not meant for this stuff and you should give up. Maybe I just phrased my problem poorly, but my desire to make functions is solved perfectly by subroutines, which nobody mentioned until I saw it on an Applescripts blog. 

There are also not AppleScript blogs in the same way that there are Javascript/CSS blogs. Being Apple fans, each of the AS tutorial websites is written in a legible fashion. However they are nowhere as smooth as those CSS blogs showing live examples of X property. I understand that this is because scripts cannot be played online, but it still makes it hard when they show no pictures of what the output is supposed to look like. 

OK, onto the examples.

# Examples
### Spotify Context

&lt;!-- HTML generated using hilite.me --&gt;&lt;div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"&gt;&lt;pre style="margin: 0; line-height: 125%"&gt;&lt;span style="color: #888888"&gt;############ Get current spotify usage&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;on&lt;/span&gt; &lt;span style="color: #996633"&gt;getSpotify&lt;/span&gt;()
    &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;retList&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; {&lt;span style="background-color: #fff0f0"&gt;&amp;quot;&amp;quot;&lt;/span&gt;}
    &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Spotify&amp;quot;&lt;/span&gt; &lt;span style="color: #000000; font-weight: bold"&gt;is&lt;/span&gt; &lt;span style="color: #996633"&gt;running&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;then&lt;/span&gt;
        &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Spotify&amp;quot;&lt;/span&gt;
            &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; &lt;span style="color: #996633"&gt;player&lt;/span&gt; &lt;span style="color: #0000CC"&gt;state&lt;/span&gt; &lt;span style="color: #000000; font-weight: bold"&gt;is&lt;/span&gt; &lt;span style="color: #0000CC"&gt;playing&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;then&lt;/span&gt;
                &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;tr&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #996633"&gt;current&lt;/span&gt; &lt;span style="color: #996633"&gt;track&lt;/span&gt;
                &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;retList&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; {&lt;span style="color: #0000CC"&gt;name&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;tr&lt;/span&gt;, &lt;span style="color: #996633"&gt;artist&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;tr&lt;/span&gt;, &lt;span style="color: #996633"&gt;player&lt;/span&gt; &lt;span style="color: #0000CC"&gt;position&lt;/span&gt;, (&lt;span style="color: #996633"&gt;duration&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;tr&lt;/span&gt;) &lt;span style="color: #333333"&gt;/&lt;/span&gt; &lt;span style="color: #0000DD; font-weight: bold"&gt;1000&lt;/span&gt;, &lt;span style="color: #0000CC"&gt;id&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;tr&lt;/span&gt;}
            &lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt;
        &lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt;
    &lt;span style="color: #003366; font-weight: bold"&gt;return&lt;/span&gt; &lt;span style="color: #996633"&gt;retList&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #996633"&gt;getSpotify&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

My problem with learning from example Applescript I read online is that the examples are not clearly cross-applicable and the errors you get are not helpful. I can read this code perfectly fine, and you probably can too — but it's a trap to trick you into thinking you know how to write it! This took me like an hour because I kept naming my variable yes, but even when I stopped that it still took me a while to realize "application "x"" is not replaceable with a macro like it would be in C. 


### Current date and time, formatted as a timestamp or filename
&lt;!-- HTML generated using hilite.me --&gt;&lt;div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"&gt;&lt;pre style="margin: 0; line-height: 125%"&gt;&lt;span style="color: #888888"&gt;########## get current date/time formatted as a sortable string&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;on&lt;/span&gt; &lt;span style="color: #996633"&gt;date_format&lt;/span&gt;(&lt;span style="color: #996633"&gt;adate&lt;/span&gt;) &lt;span style="color: #888888"&gt;-- Old_date is text, not a date.&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; {&lt;span style="color: #007020"&gt;year&lt;/span&gt;:&lt;span style="color: #996633"&gt;y&lt;/span&gt;, &lt;span style="color: #007020"&gt;month&lt;/span&gt;:&lt;span style="color: #996633"&gt;m&lt;/span&gt;, &lt;span style="color: #007020"&gt;day&lt;/span&gt;:&lt;span style="color: #996633"&gt;d&lt;/span&gt;, &lt;span style="color: #996633"&gt;time&lt;/span&gt;:&lt;span style="color: #996633"&gt;t&lt;/span&gt;} &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #996633"&gt;adate&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;delim&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;.&amp;quot;&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;yada&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; (&lt;span style="color: #996633"&gt;y&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;as &lt;/span&gt;&lt;span style="color: #BB0066; font-weight: bold"&gt;string&lt;/span&gt;) &lt;span style="color: #333333"&gt;&amp;amp;&lt;/span&gt; &lt;span style="color: #996633"&gt;delim&lt;/span&gt; &lt;span style="color: #333333"&gt;&amp;amp;&lt;/span&gt; (&lt;span style="color: #996633"&gt;m&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;as&lt;/span&gt; &lt;span style="color: #996633"&gt;integer&lt;/span&gt;) &lt;span style="color: #333333"&gt;&amp;amp;&lt;/span&gt; &lt;span style="color: #996633"&gt;delim&lt;/span&gt; &lt;span style="color: #333333"&gt;&amp;amp;&lt;/span&gt; &lt;span style="color: #996633"&gt;d&lt;/span&gt; &lt;span style="color: #333333"&gt;&amp;amp;&lt;/span&gt; &lt;span style="color: #996633"&gt;delim&lt;/span&gt; &lt;span style="color: #333333"&gt;&amp;amp;&lt;/span&gt; &lt;span style="color: #996633"&gt;t&lt;/span&gt;
    &lt;span style="color: #003366; font-weight: bold"&gt;return&lt;/span&gt; &lt;span style="color: #996633"&gt;yada&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #996633"&gt;date_format&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

Here you'll note that I did not include the word "yes" as my variable, but notice how similar it is to the current variable "yada". Wonder how that happened...

### Get currently focused app and path to that app
Remixed from [Stack Overflow](https://stackoverflow.com/questions/5292204/macosx-get-foremost-window-title)
&lt;!-- HTML generated using hilite.me --&gt;&lt;div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"&gt;&lt;pre style="margin: 0; line-height: 125%"&gt;&lt;span style="color: #888888"&gt;############ Get URL and name of focused app&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;windowTitle&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;System Events&amp;quot;&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;frontApp&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #007020"&gt;first&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="color: #996633"&gt;process&lt;/span&gt; &lt;span style="color: #007020"&gt;whose&lt;/span&gt; &lt;span style="color: #0000CC"&gt;frontmost&lt;/span&gt; &lt;span style="color: #000000; font-weight: bold"&gt;is&lt;/span&gt; &lt;span style="color: #003366; font-weight: bold"&gt;true&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #0000CC"&gt;name&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;frontApp&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt; &lt;span style="color: #996633"&gt;process&lt;/span&gt; &lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt;
        &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt; (&lt;span style="color: #007020"&gt;1st&lt;/span&gt; &lt;span style="color: #0000CC"&gt;window&lt;/span&gt; &lt;span style="color: #007020"&gt;whose&lt;/span&gt; &lt;span style="color: #996633"&gt;value&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;attribute&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;AXMain&amp;quot;&lt;/span&gt; &lt;span style="color: #000000; font-weight: bold"&gt;is&lt;/span&gt; &lt;span style="color: #003366; font-weight: bold"&gt;true&lt;/span&gt;)
            &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;windowTitle&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #996633"&gt;value&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;attribute&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;AXTitle&amp;quot;&lt;/span&gt;
        &lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;appfilepath&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #996633"&gt;POSIX&lt;/span&gt; &lt;span style="color: #0000CC"&gt;path&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="color: #996633"&gt;file&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;frontApp&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


### Get the URL of any application that has a URL or path (Finder, Preview, Chrome, Safari)
Remixed from Stack Overflow, [[1]](https://gist.github.com/EvanLovely/cb01eafb0d61515c835ecd56f6ac199a) [[2]](https://stackoverflow.com/questions/12129989/getting-finders-current-directory-in-applescript-stored-as-application)
&lt;!-- HTML generated using hilite.me --&gt;&lt;div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"&gt;&lt;pre style="margin: 0; line-height: 125%"&gt;&lt;span style="color: #888888"&gt;############ Get URL of current chrome/safari/preview/finder tab&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;currentTabUrl&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; (&lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt; &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Safari&amp;quot;&lt;/span&gt;) &lt;span style="color: #000000; font-weight: bold"&gt;or&lt;/span&gt; (&lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt; &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Webkit&amp;quot;&lt;/span&gt;) &lt;span style="color: #008800; font-weight: bold"&gt;then&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Safari&amp;quot;&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;currentTabUrl&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #996633"&gt;URL&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #007020"&gt;front&lt;/span&gt; &lt;span style="color: #0000CC"&gt;document&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;else&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; (&lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt; &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Google Chrome&amp;quot;&lt;/span&gt;) &lt;span style="color: #000000; font-weight: bold"&gt;or&lt;/span&gt; (&lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt; &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Google Chrome Canary&amp;quot;&lt;/span&gt;) &lt;span style="color: #000000; font-weight: bold"&gt;or&lt;/span&gt; (&lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt; &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Chromium&amp;quot;&lt;/span&gt;) &lt;span style="color: #008800; font-weight: bold"&gt;then&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Google Chrome&amp;quot;&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;currentTabUrl&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #996633"&gt;URL&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #0000CC"&gt;active&lt;/span&gt; &lt;span style="color: #003366; font-weight: bold"&gt;tab&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #007020"&gt;front&lt;/span&gt; &lt;span style="color: #0000CC"&gt;window&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;else&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; (&lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt; &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Preview&amp;quot;&lt;/span&gt;) &lt;span style="color: #008800; font-weight: bold"&gt;then&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Preview&amp;quot;&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;currentTabUrl&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #0000CC"&gt;path&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #007020"&gt;front&lt;/span&gt; &lt;span style="color: #0000CC"&gt;document&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;else&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; (&lt;span style="color: #996633"&gt;frontAppName&lt;/span&gt; &lt;span style="color: #333333"&gt;=&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Finder&amp;quot;&lt;/span&gt;) &lt;span style="color: #008800; font-weight: bold"&gt;then&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt; &lt;span style="color: #007020"&gt;application&lt;/span&gt; &lt;span style="background-color: #fff0f0"&gt;&amp;quot;Finder&amp;quot;&lt;/span&gt;
        &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt; &lt;span style="color: #007020"&gt;exists&lt;/span&gt; &lt;span style="color: #996633"&gt;Finder&lt;/span&gt; &lt;span style="color: #0000CC"&gt;window&lt;/span&gt; &lt;span style="color: #0000DD; font-weight: bold"&gt;1&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;then&lt;/span&gt;
            &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;currentDir&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #0000CC"&gt;target&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;Finder&lt;/span&gt; &lt;span style="color: #0000CC"&gt;window&lt;/span&gt; &lt;span style="color: #0000DD; font-weight: bold"&gt;1&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;as&lt;/span&gt; &lt;span style="color: #996633"&gt;alias&lt;/span&gt;
        &lt;span style="color: #008800; font-weight: bold"&gt;else&lt;/span&gt;
            &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;currentDir&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #996633"&gt;desktop&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;as&lt;/span&gt; &lt;span style="color: #996633"&gt;alias&lt;/span&gt;
        &lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt;
        &lt;span style="color: #008800; font-weight: bold"&gt;set&lt;/span&gt; &lt;span style="color: #996633"&gt;currentTabUrl&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;to&lt;/span&gt; &lt;span style="color: #996633"&gt;POSIX&lt;/span&gt; &lt;span style="color: #0000CC"&gt;path&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;of&lt;/span&gt; &lt;span style="color: #996633"&gt;currentDir&lt;/span&gt;
    &lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;tell&lt;/span&gt;
&lt;span style="color: #008800; font-weight: bold"&gt;end&lt;/span&gt; &lt;span style="color: #008800; font-weight: bold"&gt;if&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


# Conclusion
That's all I got for now. I'm going to try to run a continuous log of my current laptop's "context", so I need file appending eventually. Until then, cya later!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 05 Jan 2022 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Replacing Timing</title>
      <link>blog/replacingtiming</link>
      <description>I promise I'm not trying to be more productive</description>
      <content:encoded>I have been working on replacing a piece of software I use on a weekly basis. The Mac app [Timing](https://timingapp.com/) is a computer context tracker, which is to say it logs what windows have been open as long as you're using it. It even offers you a beautiful interface to interact with your data through.

&lt;p class="caption"&gt;My Timing history for the past year&lt;/p&gt;
![Screencap of my Timing history for the past year]({{ url_for('static', filename = 'rt_timingdashboard.png')}})

Timing has cost me $8.50 a month for the past 6 months, and they primarily target freelancers who bill clients by the hour. Unfortunately, I am not a contractor, and Timing does not help me track hours. Timing just offers me a cool composite of time as I've spent it on my laptop in the past day/week/month. And though I can't justify its cost,  I still love the functionality. So I've been trying to replace it. 

# General plan
Timing calls itself a time tracker, but I don't think that's really what I want. I want to know how I'm using my laptop during the course of a day, and make adjustments based on whatever I happen to want to change.

Really, I just wanted to make a background-running shell or Python script that logs my context on the computer, then shows me the aggregated stats once in a while. 

To understand what I'm doing on my computer, I think you need to know A) the current time, B) what I'm listening to, and C) which application is open and for how long. I think these give you a pretty clear picture of my mental state. For example, if I have docs open, I'm probably writing. If I'm on Youtube, I'm watching videos. If I'm on preview, reading. Of course there are exceptions — I could be on YouTube watching gaming videos or electrical engineering tutorials.But it should be good enough. Logging my music will also help me conclusively decide if music helps or harms me when I'm working. 

## Improvements over Timing
My main complaint with Timing is that it didn't ever show me aggregated features pulled out of my data. I don't mean the "automagical insight extraction" that so many data analysis teams claim to implement for millions of dollars. The human brain can do all of that truly automatically, as long as the right data is shown routinely. I simply wanted something showing my usage to establish a feedback loop, and let myself decide what needs to change or not, and how that change has been going historically. 

# What've you done so far? 
So far, I've cancelled my Timing subscription and written a pretty crude day-by-day activity tracker using Applescripts, Bash, and Python. I'm gonna refer to it as Casey for now, just to have a name for it. 

&lt;br&gt;&lt;br&gt;

## As a logger, Casey uses only 15% of the storage of Timing while being 50% more efficient

&lt;br&gt;&lt;br&gt;&lt;hr&gt;

## Performance
Here's a photo of the two programs in the activity monitor, monitoring the same stretch of 16 minutes.  

&lt;p class="caption"&gt;CPU time for Timing&lt;/p&gt;
![CPU time for Timing]({{ url_for('static', filename = 'rt_cputesttiming.png')}})

vs.

&lt;p class="caption"&gt;CPU time for Casey&lt;/p&gt;
![CPU time for Casey]({{ url_for('static', filename = 'rt_cputestcasey.png')}})

We see the CPU time of Casey is 5.66 seconds compared to Timing's 7.68, around a 25% reduction. 

The memory footprint of Casey is also lower, sitting at 27.4 MB vs. 48.1 MB for Timing, representing a savings of 22.2%. 

&lt;hr&gt;

## Storage
Casey retrieves user context every second, and accumulated 1033 new lines and 179kB of extra storage during the test. Each record takes up around 173 bytes pre-compression. After compression, the total storage is only 6kB, putting each log entry at 5.8 bytes for Casey. 

![Compressed and uncompressed log sizes for Casey, Finder screenshot]({{ url_for('static', filename = 'rt_stotestcasey.png')}})

I planned on comparing this to Timing's 12kB + 235kB in the sync.db and wal.db respectively (they do not immediately store it into a sqlite database), assuming the same logging rate.  But I personally can't believe that Timing could be so terribly inefficient. At 239 bytes/log, it's worse than Casey when naively storing everything as text directly. I'll instead refer to the 2400 hours of data it has recorded into a storage folder only 320MB in size, including backups. This means it sits at 2346 hours/320.8 MB = 38 bytes/sec. A much more reasonable quantity, still beaten handily by Casey's 5.8 bytes/log.

&lt;p class="caption"&gt;My Timing records and how much space they take up&lt;/p&gt;
![My Timing records and how much space they take up]({{ url_for('static', filename = 'rt_stotimingrecords.png')}})

# Shortcomings
Casey does not yet have a GUI, or a plan for long-term backup storage. The lookup and indexing system is still not in place, nor is the aggregation of data. Casey also lacks the periodic reminder system that I want it to have through emails. But it's a WIP, so I'm happy to document its baby steps.


# Conclusion
I want to conclude by saying that Timing is a beautifully polished piece of software. I applaud the Timing team for making such a smooth application that does one job and does it well. But it isn't what I wanted, and it's a bit expensive for me. 

The storage and performance don't matter so much since we're talking about such small beans (7 secs on 16 minutes is around 0.7%, 320MB vs 3.2GB is not such a big swing for a year of data), but this is my current optimization. 

More progress tomorrow. 
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 09 Jan 2022 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Supercapacitor To Lithium Battery Converter</title>
      <link>blog/supercap2lipo</link>
      <description>To enable powering all lithium battery devices with chunky supercaps instead</description>
      <content:encoded>This past weekend, I attended my last Build18 as an undergrad. Build18 is a hardware hackathon at CMU which gives student teams a $300 budget to buy any parts they want, to build any project they'd like. This time around, I decided to convert a smartphone to use a supercapacitor instead of its lithium battery. The benefit of a supercapacitor is that it's easier to hold, and that it can charge around 100x faster than a lithium battery. The only downside is that it's massive. 

&lt;p class="caption"&gt;Size of the supercap, here charging my FitBit&lt;/p&gt;
![Size of the supercap, here charging my FitBit]({{ url_for('static', filename = 'cap2lipo_capsize.jpg')}})

To replace the lipo, I needed to convert supercapacitor voltages (0-3V) to lithium battery voltages (3.7-4.2V). The most common boost/buck-boost circuits online start working at 3V, which is just out of reach of all supercaps, so I made custom boost converter board based on an application note from the [LTC3124](https://www.analog.com/en/products/ltc3124.html). This post tests a few metrics of this board. 

Most consumer electronics use lipos, and by making this board, I hope to make any future device supercap conversions easy to do. The boost converter circuit keeps working down to 0.5V, and only kicks in around 1.7V. 
 
 &lt;p class="caption"&gt;Supercap board alone&lt;/p&gt;
![Supercap board alone]({{ url_for('static', filename = 'cap2lipo_board.jpg')}})


# Testing Metrics

The metrics I'm interested in are maximum continuous current output and efficiency.

The problem with powering a former-LiPo device with a supercap is that the boost converter bottlenecks the current. Even if the supercap can handle a device spike of 2 amps, the boost converter may not be able to, sending the voltage crashing. The maximum continuous current output lets me know which devices can use this converter board. 

The supercap's low energy density also means that the cap alone is only 2/3 or so of the total energy in a smartphone battery, and is only made worse by the inefficiency in the boost converter. The efficiency lets us get a measurement of how much current we can expect to use from the supercap.


# Setup

&lt;p class="caption"&gt;Setup for testing the board&lt;/p&gt;
![Setup for testing the board]({{ url_for('static', filename = 'cap2lipo_testsetup.jpg')}})

I used a DC  electronic load and a 3V/3A power supply for testing, and this board is set to output 4V. Here's the data:

&lt;p class="caption"&gt;Maximum continuous current out at 4V output&lt;/p&gt;
![Maximum continuous current out at 4V output]({{ url_for('static', filename = 'cap2lipo_currentout.png')}})

&lt;p class="caption"&gt;Efficiency at 4V for various input voltages&lt;/p&gt;
![Efficiency at 4V for various input voltages]({{ url_for('static', filename = 'cap2lipo_efficiency.png')}})

Considering the cap's total power scales with $V^2$, we are well above 70% efficiency for 90% of the energy of the cap. Also, the 0.6A current output at 1.5V is enough to power a smartphone, since most of the time they draw 200-500mA (except on startup). 

# Comparison
Crudely, the efficiency cuts down the capacity by 75%, and it only works (for smartphones, depends on current load) down to around 1.25V. At peak, the supercap is 2.85V, so we can use 1.25^2/2.85^2 = 80.7% of the power in the cap. This means when we calculate the equivalent lipo capacity of a supercap, we need to downrate it to 60% of total capacity. Ouch!

Could another boost converter be better? Well, the 80% downgrade is inherent to the supercapacitor at certain current output needs, and I don't think many boards can boost and output &gt;0.5A below 1.25V. I did see one more TI part which was out of stock for two years though, and it can do a bit higher current. The 75% efficiency can be better, but not at high current draws, and is capped at 90%. A realistically perfect converter would yield around 90%x90% = 80% of the supercapacitor power, which is a lot more than this board's 60%. However, this converter board works pretty well, and it's not like supercapacitors don't already have a supply issue already. 


# Conclusion

One day I will get my hands on that TPS chip, but until next time, cya!


</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Mon, 14 Feb 2022 00:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Setting up Electric Tables v0.2</title>
      <link>blog/settingupelectrictables</link>
      <description>Shoutout to Tom Critchlow</description>
      <content:encoded>Hello everyone! I've recently been thinking about how much time I spend on my computer, and I've gotten gotten into activity logging so I can reflect on the countless hours spent on here. There are other tools out there, but many are manual and many are paid, and I just can't be bothered with either. 

&lt;hr&gt;

## Browser as a Memex

{{ add_pic("rt_timingdashboard.png", "Screenshot from my Timing dashboard, back in my period of riches") }}

Over half of all my computer activity happens in the browser, so I've decided to start with indexing browser activity automatically. 

[Tom Critchlow](https://tomcritchlow.com/) recently released this great bookmarklet tool he calls [Electric Tables](https://tomcritchlow.com/2022/02/07/electric-tables-v2/), which template matches onto author names, titles, dates, and more to index a website as a row in a spreadsheet. Previously, this used the browser's `localStorage`, but it has since moved to Google Sheets. 

I think this is a great development, and wanted to set up Electric Tables myself. The [gist](https://gist.github.com/tomcritchlow/cbb06a9298fb6cc0804372552fda1f96) that Tom posted with the code had instructions for each step of the way, but silly coder I am, I ran into some trouble setting it up. This post is to clarify what exactly needs to be done so future Andy (and others!) can have less trouble in the future. 

If you're reading this, thanks Tom! 

# Let's begin!
So, theres three things to set up: the Google Script, the Google Sheet, and the Javascript bookmarklet. 

&lt;hr&gt;

## Google Script
Go to [script.google.com](https://script.google.com/home) and sign in, then click "New project" in the top left. It should take you to this blank page

{{ add_pic("et_googlescript1.png", "Blank Google Script") }} 

Name your project, then paste in the code from Tom's [gist](https://gist.github.com/tomcritchlow/cbb06a9298fb6cc0804372552fda1f96) under `electrict-tables-v0.2.gs`. Feel free to delete the starter `myFunction`. 

We need to add a dependency called Cheerio which scrapes the text from websites. Hit the plus icon next to "Libraries" on the left panel, and include [Cheerio](https://github.com/tani/cheeriogs) by looking up the Script ID: `1ReeQ6WO8kKNxoaA_O0XEQ589cIrRvEBA9qcWpNqdOP17i47u6N9M5Xh0`. Hit "Add" to include it.

{{ add_pic("et_googlescript2.png", "Google Script with code, looking up Cheerio successfully") }} 

Now we just gotta publish it to the web. Click "Deploy -&gt; New deployment", and select "Web app" as the type. I don't think the type matters for functionality, and you can leave the options as default. 

When you click deploy, you'll get a permissions popup screen. Since only you can run this script, it's safe to allow it to edit your spreadsheets, so go ahead and give it permission.

If you go to Deploy-&gt;Manage deployments, you can grab the Web app URL. We'll need this for our bookmarklet. 

{{ add_pic("et_googlescript3.png", "Our script's API endpoint URL") }} 


&lt;hr&gt;

## Google Sheet
Create a new Google Sheet, making sure it's under the same Google account as your script. We're going to add headers to the file so added rows are properly formatted. The column names are case-sensitive!

{{ add_pic("et_googlesheet.png", "A properly set up Google Sheet") }} 

I've also highlighted the Sheet ID in the URL. We only need this bit for the bookmarklet, not the whole thing. 

&lt;hr&gt;

## Bookmarklet
Last step, the actual trigger for adding pages to Electric Tables. Open your favorite text editor, and paste in `bookmarklet.js` from Tom's [gist](https://gist.github.com/tomcritchlow/cbb06a9298fb6cc0804372552fda1f96). 

You only need to add the macro URL from step 1 (should look like "[script.google.com/macros/s/....../exec]()") and the spreadsheet ID (should look like `AodN89ua98dWL12O1oidRTa...` and be really long)

Then head over to a [bookmarklet generator](https://caiorss.github.io/bookmarklet-maker/) and copy in your edited `bookmarklet.js`. Hit "Run Code" to test it out, and the Electric Tables menu should pop up in the top right of your screen. Add a note, then click "Submit". 

{{ add_pic("et_bmtesting.png", "Bookmarklet page getting indexed") }} 

Tab over to your Google Sheets page, and you should see a new row with the bookmarklet site and your note appear after a second.

{{ add_pic("et_newrow.png", "Successful Electric Tables entry!") }} 

Celebrate! Now you can just drag the bookmarklet button onto your bookmarks bar to make a button for it. You can also create a bookmark, then for the URL paste in the "Output" Javascript function. 

{{ add_pic("et_handbookmark.png", "Adding bookmarklet manually") }} 

&lt;hr&gt;

# Wrapping Up

So, you should have a nifty little bookmark sitting in your bookmarks bar that catalogues interesting pages you find for later, and it's all stored on Google's big boy servers! No need to remember it all yourself anymore (as if you were doing that before 😜)

{{ add_pic("et_bookmarklet.png", "Electric Tables bookmarklet") }} 

Happy searching!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Fri, 22 Apr 2022 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Infoglobe Tutorial Pt 1 — Hardware Prototyping</title>
      <link>blog/infoglobetutorial1</link>
      <description>Hacking a new brain into the Infoglobe</description>
      <content:encoded>&lt;style&gt;
    image{
        max-width: 100%;
    }
&lt;/style&gt;

Hello! I'm writing a tutorial on how to hack your own Olympia Infoglobe. This first part just tells you all the wires you'll need to cut or connect in order to make it display custom messages. 

{{ add_pic("igt1_hero.jpeg", "caption") }}


If you don't already know about the Infoglobe (surprising considering how I won't shut up about it), it's a caller ID system from the turn of the millenia which displayed who was calling your quaint little landline phone. It could also store messages that people left you, and display holiday messages. Overall, a cute little device!

Today, the humble Infoglobe has been rendered obsolete by the modern smartphone. With no landline resurgence in sight, we're going to hack the Infoglobe to show custom messages and give it a second life as a weather station or some other mundane job.

Let's start!

&lt;hr&gt;
# Tools
You'll need the following things


 - 1x Infoglobe
 - Philips head screwdriver
 - wire cutter
 - soldering iron + solder
 - 50-1kΩ resistor


{{ add_pic("igt1_tools.jpg", "Since I already did this hack, I have only photographed two of the tools. Please use your imagination for the rest") }}

## Step 1: Remove the screws from the base
Pretty straightforward. There's 4 Philips-head screws in these 4 dark holes. They're kinda deep in there, so get a long thing screwdriver. Nothing will happen when they come out, but it makes the next step easier. 

{{ add_pic("igt1_screws.jpg", "The four screw burrows, helpfully circled in red") }}



## Step 2: Remove the clear blue dome
Now we're taking off the top and getting to the insides. The top is held on by four latches you can see from the outside on the edges.

{{ add_pic("igt1_latches.jpeg", "Latches visible from the outside (pic from eBay)") }}


Inside, they have a little notch going from the wall radially inwards
{{ add_pic("igt1_bottomclasp.jpg", "Inside the base, there are notches that go towards the center") }}

The top part has these seatbelt-like shapes that line up with the notch on the inside. 

{{ add_pic("igt1_topclasp.jpg", "And on the top part, there are latches that extend into the body") }}

### Careful!!

I won't lie to you: getting this thing off is kinda hard. You feel like you're gonna break it on accident, and it's a sphere so it's hard to hold on to, and will definitely break when you drop it. I recommend sitting on the floor or hugging it when you're wrestling the top part off. 

You'll want to pick one latch to start with. Right above the latch on the blue part, pull inwards and pull up at the same time. Hold the base tightly with your other hand, or hug it with your arm. A wedge/screwdriver is unlikely to help. Here's a video of me getting the first one out.


&lt;video controls style="max-width: 100%" src={{ url_for("static", filename='igt1_firstlatch.mp4') }}&gt;&lt;/video&gt;
&lt;p style="font-size:.7em;"&gt;Funnily enough, I'm wearing a Dome shirt as I open this dome&lt;/p&gt;

The rest of the latches get easier after the first two. Same technique, just pull in and pull up at the same time. 

# Step 3: remove the propeller
Once you're in, the grey part will be loose, but still held in by the spinning arm of the Infoglobe. We'll have to take that off. Just use your screwdriver to remove those 3 small screws. The propeller is spring loaded, so maybe unscrew all of them a little first so the first one doesn't bounce out and get lost

{{ add_pic("igt1_propellerscrews.jpg", "Note the square center peg and the triangular screw arrangement - automatic alignment!") }}
Remove both the propeller and the grey plate and set them aside. 

# Step 4: Hijack the Infoglobe's data LED and add a current-limiting resistor
There's 3 obvious LEDs once you're looking at the circuit board. One is red, this is just a power indicator. Of the two other light-blue ones, the innermost one is the data LED. 

{{ add_pic("igt1_dataLED.jpg", "All LEDs inside the infoglobe, labeled") }}


Previous tutorials have made circuit boards that integrate into the infoglobe, allowing it to continue displaying the phone stuff you wanna see plus cool custom stuff. This is why past projects have had such complex setups. I say why bother? The data LED has two wires like any other LED, and if we cut them then we can write whatever we want. 

There's two ways about this. You can either cut both legs of the LED and solder on new wires that run to the outside of the infoglobe, or you can just cut the positive side of the LED and connect the grounds of the Infoglobe board to your microcontroller. 

Either way, two wires get added so it's really up to you. I started with the single wire and joined the grounds first because I knew I wanted to power my microcontroller off the infoglobe power eventually, and it made prototyping way faster. 

{{ add_pic("igt1_bothcut.jpg", "The two-cut approach hijacks the entire data LED. The LED's positive side is on the left") }}

{{ add_pic("igt1_onecut.jpg", "The one cut approach, hijacks only the power wire (left) and requires you find ground somewhere else (anywhere connected to the right wirse is fine)") }}

Run those two wires outside of the casing and connect a resistor in series so you don't blow out the Infoglobe's infrared LED. 

### PLEASE CONNECT A SERIES RESISTOR TO THE DATA LED LINE BEFORE USING IT, OTHERWISE YOU RISK BURNING OUT THE LED!

&lt;hr&gt;

# Testing the Infoglobe without putting it back together
If you wanted to plug in the infoglobe with all the plastic off, it'd probably be dangerous for both the board and you. It just takes a hair caught in the rotor to ruin your Infoglobe and hair and project all at once. But it sucks to put it all back together, and there is a way to be relatively safe without doing that. 

1) Put the grey piece back on. The alignment is a bit tricky but it should drop right in

{{ add_pic("igt1_reass1.jpg", "") }}

2) Then place the rotor back on the center section and screw it in. Make sure you screw them all in a bit simultaneously, since there is a spring under it. 

{{ add_pic("igt1_reass2.jpg", "") }}

3) Get something to fool the safety switch, preferably nonmetal
{{ add_pic("igt1_safeswitch.jpg", "This limit switch needs to be taped or pushed down") }}
{{ add_pic("igt1_reass3.jpg", "Chopstick works for depressing it") }}

The top dome can also be placed into the slots without clicking down, but still low enough to trigger the safety switch. This is probably the safest way to do it.
{{ add_pic("igt1_reass4.jpg", "The lid also works for triggering the safety switch, but this is the only method that is actually safe") }}
4) Plug in the infoglobe power, and the rotor should begin spinning with no words appearing

To change the insides requires at least unscrewing the rotor, but luckily you won't need to do it much at all. 

&lt;hr&gt;
# Final setup
At this point, you should have an Infoglobe with two wires coming out of it. I originally used F jumpers as the wires to the data LED, then plugged into them using an external Arduino. 

{{ add_pic("igt1_testingsetup2.jpg", "Picture of the Arduino setup I originally worked with, Arduino not in-frame") }}

Later when I was sure the code worked on Arduino, I moved to using an ESP8266 microcontroller. It's the little square with a blue light in the picture below. 


{{ add_pic("igt1_testingsetup.jpg", "") }}

Since the ESP has no headers, I'm using a breadboard to connect the ESP to the data wires. Again, the LED's wires you've added will just run out from under the shell of the Infoglobe, and they safety switch can be engaged using the top dome despite the wires preventing it from closing fully. 

There is some more hardware on the breadboard than I'm letting on, but that will be covered in Pt 2.


&lt;hr&gt;
# Conclusion
There's a few more steps if you want to integrate the infoglobe with your microcontroller, but that's for a later tutorial. Now you've gotten a connected LED. Time to write some messaging software!


If I haven't published it and you want me to get on with it, just shoot me an email and I'll add it to my very relaxed schedule. 


Update 10/13: There's a part two [here](../infoglobetutorial2)!
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 01 Oct 2022 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Making Clear Gelatin From Porcine Sheets</title>
      <link>blog/cleargelatin1</link>
      <description>From Jello you came, and to Jello you shall return</description>
      <content:encoded>Hello! I have recently moved to Switzerland. I'll be writing some smaller posts to encourage myself to get back into this blogging thing, and this is the first. We'll see if it works.

Before coming to Switzerland, I thought a bit about the things I'd miss about America. According to forum posts from other expats, the Swiss had no mint chocolate (still true), nor peanut butter (untrue as of now), nor chocolate chips (still true), nor Jell-O and some of the other brands of random snacks. That's all find and good since most snacks have substitutes, but Jello??? There's no equivalent for this clear, jiggly snack!

{{ add_pic("cg1_jellohero.jpg", "Look at that Joggly ") }}

Luckily, I was able to find some sheets of pure-ish gelatin in the supermarket baking section, which is (I think) the core ingredient of Jello. Since I've only ever had colored, sweetened Jello, I decided my first time making it I would see how gross the unsweetened version is, as well as checking the clarity of it without any coloring.
kes
{{ add_pic("cg1_jellolens.jpg", "Partially inspired by my discovery that science classes will use gelatin casts to teach optics") }}


&lt;hr&gt;
## In sheets???

The store sells gelatin in both powder and sheets form, and I chose the sheets cause I found the form factor interesting. Each sheet is good enough for around 100mL of liquid, and is totally clear with some bubbles at the seams.

{{ add_pic("cg1_composite.jpg", "caption") }}

The sheet is pretty cool. It's embossed in a diamond pattern (zoom in!) and is quite bendy (not brittle at all) and hard to the touch. And when I touched it, it didn't get sticky or anything, so my skin moisture did not sufficiently wet it. I put it in my mouth for 30 sec and it became only a bit softer, and had no flavor. I'm using a 500mL tupperware to hold my jello, so I used around 4 sheets and left some air in the top. 


First, you gotta soak the gelatin sheets in cold water for 5-10 minutes. This is said to activate or "bloom" it. Supposedly putting it straight into hot water will swell the outside without letting the inside activate, but that sounds like untested baloney. 


{{ add_pic("cg1_soaked.jpg", "Soaked gelatin sheet is like a clear tisseue") }}

While that was happening I went and microwaved my tupperware to get the water warm. I brought over the gelatin, which had sagged down in the cup of water and got ready to transfer it. I used a spoon because I didn't know how my hand oils would affect the setting. 

{{ add_pic("gc1_bothcups.jpg", "caption") }}

The package said to stir until totally dissolved and I had worried that it would take a long time, but it dissolved on impact into this slightly tan cloud. I stirred it for just a few seconds and it incorporated just fine. Here's my tupperware chilling in the fridge right after this


{{ add_pic("gc1_preset.jpg", "caption") }}


On my first attempt, it didn't set right because I misread the instructions. I thought each sheet could set 1L, but that was off by an order of magnitude (I never claimed to read French). I corrected that error, and after a night in the fridge, the properly-set gelatin turned cloudy.


{{ add_pic("gc1_postset.jpg", "caption") }}

The gelatin is super stiff after setting, and on measuring, I realized I used 4 sheets for around 300g of water. This is a good ratio for future gelatin — I hate squishy Jell-O.

{{ add_pic("gc1_meholding.jpg", "Me holding the gelatin. If it looks fun to hold, that's because it is") }}

&lt;hr&gt;
# Flavor

The taste is completely neutral. There is a tiny or no taste added from the gelatin, it just feels wrong to be eating water. The mouthfeel is that of Jell-O, but there is no flavor. It's really weird, and feels almost like you shouldn't be eating it. 

Since you can just reheat gelatin to make it liquid again, I made another batch with sugar. I heard that sucrose actually helps the gelatinization process, and wanted to check if that was true. I must say I didn't notice it being any stiffer, but it definitely tasted much better. 

# Laser?
{{ add_pic("cg1_laser.jpg", "Green laser experiment in gelatin") }}

Finally, the reason that one of the tags is "laser" is because I wanted to see if the sugar gradient experiment would still work if the sugar gradient were solidified in gelatin. Spoiler: It does! Still bends light as we expect it to. 

I think the diffusion of the sugar still happens, so eventually this block of gelatin won't bend lasers anymore. But I'll let you know how it looks later if I make a followup for this. 

{{ add_pic("cg1_laser2.jpg", "A red laser also bends, but much less visibly (look at the spot on the table on the right, this is the bent output of the red laser). I heard adding a small amount of creamer to the water helps the red show up, but I'll try that later.") }}

&lt;hr&gt;
# Future Work
1. Forgot to try the laser beam bending downwards, which is why I did this in the first place! Re-setting it now and seeing if it will bend down as well (probably will)
2. I wonder if the gelatin will set at room-temperature? I want to extend the time it takes to set, to allow a more impressive sugar gradient to form. Even if it doesn't set at room temperature, we can use use this for our purposes. Dissolve the gelatin and let the sugar gradient form outside the fridge, and only then put it in the fridge to set.
3. I wonder if the sugar gradient will slowly diffuse into the gelatin, negating the purpose of our gelatin in the first place? We can use saran wrap to avoid this though.

I'm trying to make a circular laser, and I fear this is easily done with some hose and gelatin instead of this complex ass setup i'm envisioning. Perhaps this will be cooler though.
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 04 Oct 2022 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Infoglobe Tutorial Pt. 2 — Hardware Integration</title>
      <link>blog/infoglobetutorial2</link>
      <description>Letting go of externalized electronics</description>
      <content:encoded>Hello! We're still hacking the Infoglobe. This post is about internalizing the external electronics we were using in the [first tutorial](../infoglobetutorial1), as well as powering our microcontroller using voltage from sources inside the infoglobe so we don't need two power lines. 

&lt;hr&gt;

As you may remember from [last time](../infoglobetutorial1), we wired up the data LED and ran the wires out of the infoglobe, where they were connected to an ESP8266 via breadboard. This is a great testing setup for the Infoglobe because we can fine-tune and measure the data we're sending to control the Infoglobe's display as we're figuring out the IR protocol. 

From here, I'd recommend fellow Infoglobers to first write the software to control the Infoglobe, just in case there are any repairable hardware problems that can be detected early. Assuming you've done that or are using known-working code, we'll proceed with internalizing the electronics

 
{{ add_pic("igt1_testingsetup.jpg", "Our testing setup from tutorial 1") }}

&lt;hr&gt;

# Tools
You'll need the following

 - 1x Infoglobe, opened
 - ESP8266/ES32 or other small microcontroller
 - Soldering iron + solder
     - Good air filter or fan
     - Dexterity
 - Various passives
     - Thin or flexible wire, preferably both
     - Power switch, small (optional)
     - Diode (optional)
     - Big capacitor (optional)


# Steps

1. Prep the microcontroller and its new home (µcompartment)
    * Prep compartment
    * Attach the flyback diode
    * Attach the filtering capacitor
2. Run power lines to our microcontroller
       * Find ~5V using a multimeter
       * Run wires to the µcompartment
       * (optional) Make an in-line power switch for the microcontroller
3. Run data lines to our microcontroller
       * Connect LED to ground with a resistor
       * Solder new wire to the LED's high pin
       * Run data line to µcompartment
4. Wire up the ESP
       * Melt a hole in the battery compartment
       * Feed wires through the hole

&lt;hr&gt;

# Step 1: Prepping the microcontroller
First we're gonna prepare the microcontroller for implanting. The main thing we're worried about is power — it sucks to have to plug two power cables in to use a consumer device, so we're hijacking the 5.6V line from within the Infoglobe itself. The only problem is that it's the motor power, so it's noisy and has kickback. Feel free to skip ahead if you know all this already. 


The motor's source is not the best. Starting up the spinning rotor requires a few kicks sometimes, evidenced in these sharp voltage drops on the scope. And even after the motor begins, there's noise on the voltage spanning almost 1Vpp. However these are tameable.
{{ add_pic("igt2_motorstartupnoise.jpg", "Motor noise and startup behavior") }}



### Motor protections
A motor is a set of coils that magnetize when current goes through them, allowing them to push off a set of permanent magnets in their center and generate the torque that they're known for. Whenever we turn a motor off suddenly, we essentially stop pushing current through a large inductor, their coils. This inductor then tries to preserve the current going through it by flipping the voltage across it. In practice, this means turning off a motor briefly creates a massive spike of both positive and negative voltage. 


{{ add_pic("igt2_motorkickback.jpg", "Motor turn off creates a big spike of voltage") }}

This is called kickback, and it can damage sensitive electronics like your microcontroller. To protect our ESP against kickback voltage, we connect a diode going from GND to V+. In case GND ever exceeds V+, it'll leak through the diode before it has a chance to go through our microcontroller and destroy it. 

### Smoothing motor voltage
Besides quickly fatal problems like kickback, I was also concerned about slowly fatal problems. Like that the noisy voltage of the motor would mess with the internal timings of the ESP and prevent WiFi from working properly. 

We are taking the stardard solution to this, which is to use decoupling or filter capactiors across V+ to GND. Due to the size of the noise, nothing worked until I got to massive values of C. Here I've placed a 220uF tantalum capacitor across the V+ line and GND halfway through the oscilloscope trace. 

{{ add_pic("igt2_filtercapworks.jpg", "Capacitor filtering motor noise down to half the amplitude") }}

You can see the noise drop from a 200mV brick line down to ~100mV. Although it's not super significant, I left it in because it didn't do any harm and definitely does help smooth the noise. 

## Done setting up the microcontroller?

It should look something like this:

{{ add_pic("igt2_microwiringdiagram.jpg", "schematic of microcontroller for the infoglobe.") }}

{{ add_pic("igt2_equippedesp.jpg", "View of the bottom of the infoglobe's new brain, the ESP8266") }}

## Finishing touches
Once you've wired everything, tape up the inside of the AAA backup power supply so we can put our ESP in there without shorting it out.

Why are all those funniny blue wires going though the hole for? Patience, my dear friend. We're getting there.

&lt;hr&gt;

# Step 2: Finding power on the Infoglobe board
When you first open the infoglobe, you get to see its beautiful, unmarred circuit board. We're gonna mess with it immediately. 

{{ add_pic("igt2_circuitoverhead.jpg", "The power wires are the red ones. Yes there's 3 sets of red wires wrapped together. No I don't remember which is which.") }}

We know that the motor probably gets its power from one of the red wires. The only problem is, there's three of them and they're all super tangled up.
If we look closely, we notice that two of the red wires go to the bottom of the limit switch, the very piece that controls the motor turning on! As it turns out, those ARE the power wires, you just need to figure out which one is "after" the switch (off when switch off, etc.), then connect our ESP's power wire to it somehow. 

Stripping out a section and soldering works, but alternatively, you can use the power wire to do a continuity test and find regions on the board that are connected to it. One probe on the power wire, and with the other probe you just start testing random pads on the infoglobe board. 

Here's where I broke out the voltage. Under the safety switch you can find the V+ connections, both before and after the switch. 

{{ add_pic("igt2_powerlabeled.jpg", "Location of the 5.6V line we'll be using. \"After switch\" comes on when the safety is down.") }}

If you want to check, you can plug the infoglobe in and check if the motor supply voltage is what you want. It should sit at ~6-6.3V when the safety switch is off, and drop to ~5.6V with the rotor on.

{{ add_pic("igt2_motoronvoltage.jpg", "Supply voltage at 5.6V, rotor visibly spinning") }}

{{ add_pic("igt2_motoroffvoltage.jpg", "With the safety switch untriggered, supply voltage is over 6V") }}

Once you've confirmed that the power lines are giving you what you want, go ahead and solder a long wire to the V+ point. Also solder a similarly long wire to a GND point of the board. You can find these all over, here's the two I used. 

{{ add_pic("igt2_circuitoverhead_gndlabeled.jpg", "I used the left one for power ground, and the right one to ground my data LED") }}


At this point you should have 2 wires coming from the left side of the board connected to power and ground, with long enough wires to push them into the microcontroller's compartment. 

&lt;hr&gt;

# Step 3: Data routing
All that's left is to reroute the data wires that we made earlier. 

Earlier when our computer powered the microcontroller, the microcontroller didn't share a ground with the infoglobe so we had to cut both power and GND on the infoglobe's data LED. Now that the microcontroller is sharing the Infoglobe ground, we can ground the LED near where it stands and just run one wire to the microcontroller, avoiding another messy wire.

If you look in the ground pic above, you'll see the right connection point is right beside the data LED. I soldered the right leg of the LED to that spot on the board through the current-limiting resistor we had on it before.


{{ add_pic("igt2_ledgrounded1.jpg", "Picture of the right leg of the LED (ground side) connected via resistor to ground") }}
{{ add_pic("igt2_ledgrounded2.jpg", "Alternate view of the LED's ground side") }}


Now we have the LED grounded, we want to connect a longer wire to the LED's positive side and run that wire towards our microcontroller compartment. I just cut my old wire and soldered on a longer bit, which can be seen in the pictures above.

After you've done this, the data LED should have one long wire coming out of it and the power area should have two. Picture below, but the wires need to be elongated to reach the compartment comfortably.
{{ add_pic("igt2_completewiring.jpg", "Completed wiring overhead shot, but the wires are all too short") }}


# &lt;s&gt;Step ? (optional): Add a power switch&lt;/s&gt;
&lt;b&gt;EDIT - I don't think this mod is necessary anymore. I was worried that powering the ESP with my computer while it was still connected to the motor's power would run 5V to the motor and and make it spin, or else damage the motor by underpowering it in some way. I checked the resistance across the motor and it seemed pretty high, so I'm no longer worrying about this. &lt;/b&gt;

&lt;s&gt;Since I wanted to be able to change the code on my ESP after putting it all back together, I connected the microcontroller to power through a little click switch that I left in the bottom. I had to burn a second hole through the casing to do this, but I think it's necessary &lt;/s&gt;

{{ add_pic("igt2_powerswitch.jpg", "Power switch for ESP") }}


&lt;!-- - explain flyback - online graphic --&gt;
&lt;!-- - explain capacitor - 2x images of noise before and after if we have them, otws dwai --&gt;
&lt;!-- - explain compartment photo --&gt;
&lt;!-- - circuit diagram?  --&gt;

&lt;!-- - icture for searching for power --&gt;
&lt;!-- - picture of voltage on the lines --&gt;
&lt;!-- - picture of power before and after the switch --&gt;
&lt;!-- - picture of startup kicking --&gt;
&lt;!-- - overhead shot of the power connections and where the wires are going and overall --&gt;

&lt;!-- # Step 2: Data --&gt;
&lt;!-- - picture for LED ground connection --&gt;
&lt;!-- - picture for wiring high line, leave it long --&gt;
&lt;!-- - overhead shot of the data connections --&gt;
&lt;!-- - burn hole for the data wires --&gt;




# Step 4: Finish routing the wires to the compartment
You'll need your soldering iron again for this part. The battery compartment at the bottom of the infoglobe does not have a hole to the inside, so we're going to add one by melting the plastic with our soldering iron. If you have a drill, it'll work but probably not well. 

The spot I chose is next to the safety switch on the left side of the Infoglobe, and is the only side of the battery compartment visible from the top without removing the PCB. Clear the area of wires and make sure the other side is clean, then get to a soldering filter fan. You'll just push the soldering iron tip into the plastic casing slowly, and voila! A hole for our wires!

{{ add_pic("igt2_burnhole_inside.jpg", "Internal hole for wires going to the microcontroller") }}

{{ add_pic("igt2_burnhole_outside.jpg", "Corresponding external hole") }}

We are gonna push three wires through this hole, mine happened to all look the same so I had to mark each one so I could tell them apart. On the other side, you'll solder the power to 5V, ground to GND, and the data line to D2 or whatever pin you're using in your code to control the infoglobe.

Your microcontroller should sit nicely if it's got the Wemos D1 Mini shape. 

# Step 7: Put everything back together.
Secure the wires internally, then put the grey shell back on, then screw the rotor back on, then put the dome back on. 

{{ add_pic("igt2_tolife.jpg", "") }}

And we're done! You are now the proud owner of a modded Infoglobe. Now get out there and make it say some cool stuff!


# Cya later!

Let me know if I've left out details or you need any clarification, my email is listed above.

</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sat, 15 Oct 2022 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Infoglobe Tutorial Pt. 3 — Software</title>
      <link>blog/infoglobetutorial3</link>
      <description>Adding bits to our atoms</description>
      <content:encoded>&lt;style&gt;
    /* Limit height. Show scrollbars when exceeding height */
.gist .blob-wrapper.data {
   max-height:40vh;
   overflow:auto;
}
    &lt;/style&gt;

Ok, welcome back for our final installment of the infoglobe tutorial series. If you've not been following along, here's links to the hardware mods that you'll need to have done before this post will be useful to you — [part 1](../infoglobetutorial1) and [part 2](../infoglobetutorial2).

{{ add_pic("igt1_hero.jpeg", "") }}

If you've forgotten, here's the device we're hacking - the Olympia Infoglobe. I have described it too often, so we're gonna get right into the meat of this post. 

&lt;hr&gt;

# Code
We're using the Wemos D1 mini ESP8266/ESP32 breakout board to control our globe, it looks like this.
{{ add_pic("igt3_wemos.png", "") }}

Boot up your Arduino IDE and download the "IRremoteESP8266" library from the Tools-&gt;Manage Libraries-&gt; then search for IRremoteESP8266. This library should be supported for ESP32s as well. Hit install, and twiddle your thumbs for a bit as it installs.

{{ add_pic("igt3_irlib.png", "") }}

Now, copy past [this gist](https://gist.github.com/kongmunist/a8bdadbacda4bcb129cd183f2f0fffc5) into a new Arduino file and upload it to your ESP board. 
&lt;script src="https://gist.github.com/kongmunist/a8bdadbacda4bcb129cd183f2f0fffc5.js"&gt;&lt;/script&gt;

This script is a demo showing the usage of all the functions we have to make controlling the Infoglobe easier. If you read through the `loop()` function, then you'll begin to understand the code and become able to extend it to your own beneficial/nefarious purposes.

If all went well and your ESP and Infoglobe are both powered up, you should see a "Hello World" message swirling around. You can change the default message on line 47, or you can open the Serial Monitor to upload a message to the Infoglobe immediately. 

## Example use case

I've personally written some code which lets the Infoglobe connect to my wifi and access [this website](https://aksuper7.pythonanywhere.com/) I made for receiving notes from my friends. If you type a message and my globe is plugged in, I'll be able to see the message right on my Infoglobe. Pretty cool, right? 

{{ add_pic("igt3_example1.png", "Website to connect digital friends to my physical environment") }}

I know of other people using their infoglobes as a weather/disaster reporting station, or a way to visualize Alexa messages. 


## Alternatives to ESP
It should be possible to substitute in any microcontroller which is supported by one of the IRremote libraries, since it should work as long as the function names are identical.

## More code?
There are some cool folks working on a more sophisticated platform for interacting with the Infoglobe, but I don't currently have the link for that. Hopefully I can update this soon with the link.

# That's all folks!
What will you do with your Infoglobe? I'd love to see it! Please feel free to email me if you get it working :)

Happy hacking!




</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 09 Apr 2023 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Calendar Memories</title>
      <link>blog/calendarmemories</link>
      <description>Personalized holidays in my calendar</description>
      <content:encoded>When I first began my life, time was nothing to me. I did not notice it passing, nor did I care. I lived in the world of events, and as stuff happened, I noticed the stuff without caring what time it happened. I was incredible at reading analog clocks when I was in preschool, but my parents worried about the time on my behalf, delivering me to soccer practice or school whenever they started. 

As I got older, specific times took on meaning. Classes starting and ending times, lunchtime, and school letting out were all hard-coded in my head, and I would think about them every time I glanced at the time. Over time, it only got worse. In college, I started keeping a calendar to track my courses and their locations. If a clock is a machine that produces time, a calendar is a factory. It is the ultimate time-keeping tool, containing every minute of your life and your past, theoretically forever in both directions if its electronic. Isn't that incredible? 

You may not think so, but try this. One day while working on [Carnegie Calendar](../projects/carnegiecalendar), I realized that you could put in ANY date you wanted for an event. 

I scheduled a meeting for April 9th, 2150 that I probably won't be attending. I scrolled on my calendar to make sure the event showed up, and it was there, sitting as innocently as any other calendar event. Then I navigated to my 150th birthday, and that event was there too. I probably won't make it to that either. 

{{ add_pic("cm_150th.png", "") }}

If that's not insane to you, just think about it for a bit. To me, it felt like knowing exactly where I will be buried and visiting the plot of land 50 years in advance.

&lt;hr&gt; 

# Sorry, that was a tangent
I'm actually here to talk to you about a way to create your own holidays.

You know how there are these preset holidays in your calendar that you didn't add, like "Presidents' Day" or "Thanksgiving Day"? We celebrate these events on certain days because these days were instrumental to the country's development, and that's cool. 

But to me, these events are kinda just taking up space — I didn't make them, and I might not celebrate them. What I find much more interesting to commemorate are the events that were instrumental to my life and my development. Stuff like "First time I got stitches" or "Failed midterm" that I can look back on and smile because they happened. 

{{ add_pic("cm_firststiches.png", "") }}

Now, whenever I feel like I've experienced something life-changing, I put it on my calendar and set it to repeat annually. I've started added the year too, just so I can gauge how long it's been. 

## In the far-future...
I know that if I continue this, my calendar will eventually be filled with ghosts — past iterations of myself experiencing my own history. My routine weekly work meetings will clamor with "First stiches" for space on my screen. Birthdays of people I don't remember will populate the top bar. But these are the holidays I have chosen for myself, and to me, they are worth remembering.

Give it a go!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 09 Apr 2023 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Noisy NeoPixels</title>
      <link>blog/noisyneopixels</link>
      <description>Audible annoyance from WS2812B LEDs</description>
      <content:encoded>_When I was small, I visited my friend whose dad was an electrical engineer. I got to talking with him, and asked "Hey, what's the point of that thick bit on my cables?"_

{{ add_pic("nn_ferritebead.jpg", "Big chunky cylinder on a charging cable") }}

_He said "that's a ferrite bead, it reduces noise," and I nodded in understanding — that explained why my cables never made noise._

_About a decade later, I learned about electrical noise._

&lt;hr&gt;&lt;br&gt;

Hello!

Recently, I've been playing with the Adafruit Circuit Playground Express. This is an incredibly nifty SAMD21 microcontroller board which I have been using exclusively for its built-in ring of NeoPixels.

NeoPixels are like multi-colored RGB LEDs but smarter, and it's easy to control a whole strip of 'em from just a few digital pins. But like all LEDs, the brightness control is implemented via PWM — in this particular case, at a frequency of 400 kHz or so. And while it should be out of the human hearing range, _I can hear it!_

# Electro-audible noise
As it turns out, electrical oscillation often creates audible noise. Most people are probably familiar with electronics or wall warts that start whining or [singing](https://product.tdk.com/system/files/contents/faq/capacitors-0031/singing_capacitors_piezoelectric_effect.pdf) when plugged into the wall. Usually it comes from a high frequency physical oscillation caused by the electrical oscillation. 

I'm quite sensitive to noise, and I noticed a slight buzzing coming from my Neopixels when setting the brightness higher than 5/255. Since I'm making a desktop doohickey, I wanted to learn more about the noise, particularly how it changes with brightness. 

In a stroke of luck, the Circuit Playground board comes with a microphone! I wrote a small script to vary the brightness and recorded sound pressure levels (SPL) for a 0.5 second window at each level. My board has a small cover which probably amplified the measured noise. 


{{ add_pic("nn_dbvslight.png", "Graph of NeoPixel noise in dB as brightness increases") }}

Each line is represented by the LED color that produced it. Since white is produced by keeping all three LEDs on, it makes sense for it to be higher than all the others. 

Now I could target low and high brightness to cause the least amount of noise.


# Future Work
One factor I forgot to test is the pitch of the whining at each brightness. Some brightness levels sound more annoying than other levels, even with a lower SPL. This is probably related to the nonlinearity of human hearing. 

With my NeoPixels on, I recorded the audio spectra to see where the whining lay. It turns out to be much lower than the PWM frequency of the Neopixels, which means I do not understand the root causes of the noise very well. 

{{ add_pic("nn_points.png", "The three noisiest frequencies are all much lower than the PWM frequency of the NeoPixels. Vertical lines are the whining, and horizontal ones are me dropping my phone") }}

# Conclusion
The Circuit Playground mic also has an FFT function, and someday I will get around to recording the spectrum along with the SPL. For now, I have minimized the overall sound, and that is enough. 

Cya later!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Wed, 31 May 2023 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>iCloud Cleanup</title>
      <link>blog/icloudconfusion</link>
      <description>Script for highlighting large videos in iCloud and a storage discrepancy</description>
      <content:encoded>Hello!

Recently I received an email from Apple letting me know that my iCloud storage was full. 

{{ add_pic("ic_appleemail.jpg", "Kind letter from Apple requesting I purchase more storage") }}

Since the cost of 200GB vs. 1TB jumps more than threefold, I embarked on a great exploration of alternatives. Since photos made up the majority of my storage (~127GB), I figured I could just go into my Photos album and click Sort By File Size.

...

Except this is not a feature offered by Apple. It's not even a feature in the iCloud browser. I guess it's a bit too technical for Apple Photos, and unfortunately it would also make it too easy to avoid paying for more iCloud storage. 

Whatever the reason, I still wanted to go through my photos by file size. Several apps exist for going through your photos and highlighting duplicates, or into your videos and showing you the file size. The only problem — photos that have been offloaded to iCloud do not show up in these apps, so they're not actually super useful. Also, would you really want random apps to scan through all your photos? 


{{ add_pic("ic_photosbysize.png", "Screenshot from a photo file size viewing app, with non-downloaded photos represented as 0B") }}

## Janky JS solution

Instead, I went to the iCloud website and thought about what was possible. I navigated to Photos-&gt;Media Types-&gt;Videos, and zoomed out as far as I could. 

{{ add_pic("ic_icloudphotos.png", "Videos anonymized by slow internet loading") }}

We can see that each video has an accompanying duration box — as long as it's an HTML element, we can use JS to search and filter them. I found that every runtime box has the class `video-text-badge`. From there is was a simple matter to find all of them in the page, sort by their duration, and highlight the ones that were past some threshold. Here's the code:

&lt;script src="https://gist.github.com/kongmunist/a598bcdd8c226c3a3159b1a918344977.js"&gt;&lt;/script&gt;

Because iCloud only loads the elements that are on the page, I've made this into a function that runs on a timer so new elements get highlighted as they get scrolled into. Here's what it looks like:

{{ add_pic("ic_icloudphotoshighlighted.png", "Videos bigger than 20s are surrounded by a red box, making them much easy to pick out") }}

To use it, just open the Javascript console (right click page -&gt; Inspect Element) and paste in the entire gist. Now you can easily select multiple big videos from iCloud and download them before deleting, moving them into longer-term storage: secret HDD under your mattress, other cloud storage, etc etc. 

&lt;hr&gt;

# The Mystery
So, I used this script to remove all my iCloud videos &gt;30s. The interesting thing is, after I had removed all the "big videos" and downloaded them, it cleared ~55GB from my iCloud *despite only downloading 7GB of videos.* Herein lies the mystery.

{{ add_pic("ic_dl1info.png", "All downloaded videos take up 8GB of disk space") }}

{{ add_pic("ic_dl1afterdelete.png", "iCloud storage reduces from 199GB to 143GB after downloading 7GB of videos") }}

Somehow, those 7GB of video took up way more space in the cloud than on my hard drive. Interesting...

# Experiment 1

I wanted to test this further. First, I uploaded a 4K video with a lot of motion. This took up 281 MB. My storage looked like this after uploading it:

{{ add_pic("ic_dl2storagebefore.png", "4K video uploaded, iCloud says 145.33 GB used") }}

Then I downloaded it and deleted it. The file was still 281 MB. Here is the storage afterwards:

{{ add_pic("ic_dl2storageafter.png", "4K video deleted, iCloud says 145.6 GB used") }}

Removing a 281MB video frees up ~270MB. This adds up, which is puzzling. What about the other, older videos? 

# Experiment 2
I thought that maybe older videos could have multiple copies saved in iCloud, so I searched through my videos to see if I could find a shorter one that takes up a lot of storage space. I found one with a lot of graphs, iCloud said it took up 128 MB.

{{ add_pic("ic_dl3icloudinfo.png", "Older big video that takes up 128 MB") }}

When I downloaded it, the file was only 47 MB!

{{ add_pic("ic_dl3download.png", "Downloaded video file is 47 MB") }}

And here is my iCloud storage before and after

{{ add_pic("ic_dl3storagebefore.png", "iCloud storage before deleting the old video, 145.29 GB used") }}
{{ add_pic("ic_dl3storageafter.png", "iCloud storage before deleting the old video, 145.12 GB used, reduction of 170MB") }}

So iCloud says the video is 128MB, I download it and the video is actually 48MB, and my free storage increases by ~170MB when I deleted it. Interesting!

# Conclusion
It's weird that my storage freed up more than 7x the removed files size, and weirder still that old, big videos appear to have a much larger storage footprint in iCloud than in real life. 

I am mildly interested in finding out why this happens, but I am not interested/bored enough to do it myself. If one of you fine people figure it out, please let me know by emailing me.

Anyway, I have freed up &gt;50GB to fill with more inane videos, and written a small script that allows me to do it again in the future. Hope this proves helpful to you, dear reader.

Cya later!</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Tue, 06 Jun 2023 00:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Average Clip Duration In Milf Manor</title>
      <link>blog/milfpy</link>
      <description>Gauging the preferred attention span on TLC dating shows</description>
      <content:encoded>Whenever I'm watching unpretentious TV like sports or reality TV, I calculate the average clip length. I get my phone stopwatch out, and enter a lap whenever a cut happens. 

Do this maybe 20 times, and you usually get some idea of the distribution. Usually the cuts happen so quickly and fluidly that you may notice yourself _missing_ a few cuts, even though every pixel on the screen dramatically changes and it seems like something you'd notice. 

Recently I watched Milf Manor, a "reality" dating show where older women seeking younger men are paired with younger men seeking older women on a fancy island. The twist is that every young man is the child of one of the older women. Hilarity ensues (?). 

{{ add_pic("milf_cutfeature.jpg", "The complicated feature I used for detecting jump cuts") }}

Common decency aside, this show jump-cuts like nothing else. I did my stopwatch thing but didn't want to stop there — I really wanted to know all about the cut length distribution. I downloaded an episode and used the change in standard deviation of the absolute color difference of two adjacent frames. When this number crossed ~30, I called it a cut and recorded the frame number.

Then I calculated a few stats about it. Here they are:

{{ add_pic("milf_stats.png", "Avg clip length is 2.48 sec, with the median at 1.92 sec") }}

While the average clip length is 2.5 seconds, the median is only 2 seconds. I also find it interesting that the average human blink rate is 1 blink per 4-5 seconds, which means we can see around two clips between blinks. Since the average blink takes ~0.3 seconds, we also won't miss much of a clip while blinking.

{{ add_pic("milf_cutdistro.jpg", "Milf Manor clip duration histogram") }}

# Conclusion
The difficulty of identifying a cut in a video surprised me, but it makes sense if you consider all the possible transitions (fade to black, fade to another clip, slo-mo transition to real-time, etc.). There is even a library in Python called [scenedetect](https://pypi.org/project/scenedetect/) which identifies these for you. Next time I will definitely use that for cut detection instead of making my own feature. 

I have completely satisfied my curiousity about Milf Manor at this time. 
</content:encoded>
      <author>andyking99@gmail.com (Andy Kong)</author>
      <pubDate>Sun, 11 Jun 2023 00:00:00 -0400</pubDate>
    </item>
  </channel>
</rss>
